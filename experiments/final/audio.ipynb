{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b5d9a",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25780097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: groq in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.29.0)\n",
      "Requirement already satisfied: load_dotenv in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (4.14.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from load_dotenv) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas librosa groq load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771b07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nltk in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.9.0)\n",
      "Collecting parselmouth\n",
      "  Downloading parselmouth-1.1.1.tar.gz (33 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pydub in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (7.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Collecting googleads==3.8.0 (from parselmouth)\n",
      "  Downloading googleads-3.8.0.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [1 lines of output]\n",
      "      error in googleads setup command: use_2to3 is invalid.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk tiktoken parselmouth pydub psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91bfa84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import asyncio\n",
    "import librosa\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from nltk.corpus import cmudict\n",
    "from parselmouth.praat import call\n",
    "from groq import Groq, AsyncClient\n",
    "from groq.types.audio import Transcription\n",
    "\n",
    "# Load environment file\n",
    "from load_dotenv import load_dotenv\n",
    "print(load_dotenv('../../.env.local'))\n",
    "\n",
    "assert os.environ.get('GROQ_API_KEY'), \"Groq API key not found in .env file, please set the key before starting this notebook\"\n",
    "\n",
    "# Global variables\n",
    "client = AsyncClient()\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "fluency_model = joblib.load('../fluency/weights/xgboost_model.pkl')\n",
    "\n",
    "try:\n",
    "    cmu_dict = cmudict.dict()\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('cmudict')\n",
    "    cmu_dict = cmudict.dict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c16201",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Features extracted:\n",
    "* ZCR\n",
    "* Pitch\n",
    "* Jitter\n",
    "* Shimmer\n",
    "* Harmonic-to-Noise ratio\n",
    "* RMS\n",
    "* MFCC\n",
    "* DeltaMFCC\n",
    "* SpeakingRate\n",
    "* PauseCount\n",
    "* PauseDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4b64c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async Transcription\n",
    "def split_audio_in_memory(audio_data, max_mb=24):\n",
    "    if isinstance(audio_data, io.BytesIO):\n",
    "        audio_data.seek(0)\n",
    "        audio = AudioSegment.from_file(audio_data, format='wav')\n",
    "    else:\n",
    "        audio = AudioSegment.from_wav(audio_data)\n",
    "    \n",
    "    bytes_per_ms = (audio.frame_rate * audio.frame_width * audio.channels) / 1000\n",
    "    max_bytes = max_mb * 1024 * 1024\n",
    "    chunk_duration_ms = int(max_bytes / bytes_per_ms)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(audio), chunk_duration_ms):\n",
    "        chunk = audio[i:i+chunk_duration_ms]\n",
    "        buffer = io.BytesIO()\n",
    "        chunk.export(buffer, format=\"wav\")\n",
    "        buffer.seek(0)\n",
    "        chunks.append((f\"chunk_{i//chunk_duration_ms}.wav\", buffer))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "async def transcribe_chunk(filename, audio_buffer):\n",
    "    return await client.audio.transcriptions.create(\n",
    "        file=(filename, audio_buffer),\n",
    "        model=\"whisper-large-v3-turbo\",\n",
    "        response_format=\"verbose_json\",\n",
    "        timestamp_granularities=[\"word\"]\n",
    "    )\n",
    "\n",
    "\n",
    "async def transcribe_audio(audio):\n",
    "    chunks = split_audio_in_memory(audio)\n",
    "    tasks = [transcribe_chunk(name, buffer) for name, buffer in chunks]\n",
    "    all_transcripts = await asyncio.gather(*tasks)\n",
    "\n",
    "    full_transcript = \"\"\n",
    "    all_words = []\n",
    "    total_duration = 0.0\n",
    "\n",
    "    for chunk in all_transcripts:\n",
    "        full_transcript += chunk.text\n",
    "        all_words.extend(getattr(chunk, \"words\", []))\n",
    "        total_duration += chunk.duration          # type: ignore\n",
    "    \n",
    "    return Transcription(text=full_transcript, words=all_words, duration=total_duration)   # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1898093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for calculating syllables speaking rate\n",
    "def get_word_syllable_count(word):\n",
    "    word = word.lower().strip(\".,?!;:\")\n",
    "    if word in cmu_dict:\n",
    "        return len([p for p in cmu_dict[word][0] if p[-1].isdigit()])\n",
    "    return max(1, len(re.findall(r'[aeiouy]+', word)))\n",
    "\n",
    "\n",
    "def estimate_syllable_rate(transcript, duration_sec):\n",
    "    words = transcript.split()\n",
    "    total_syllables = sum(get_word_syllable_count(word) for word in words)\n",
    "    return total_syllables / duration_sec if duration_sec > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "958711f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pitch statistics, Jitter, Shimmer, and HNR ratio through Parselmouth\n",
    "def extract_parselmouth_features(data, sr):\n",
    "    snd = parselmouth.Sound(values=data, sampling_frequency=sr)\n",
    "\n",
    "    pitch_obj = snd.to_pitch()\n",
    "    pitch_mean = call(pitch_obj, \"Get mean\", 0, 0, \"Hertz\")\n",
    "    pitch_std = call(pitch_obj, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "\n",
    "    point_process = call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "    jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    shimmer = call([snd, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "    harmonicity = call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "\n",
    "    return {\n",
    "        \"pitch_mean\": pitch_mean,\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"pitch_var\": pitch_std**2,\n",
    "        \"jitter_local\": jitter,\n",
    "        \"shimmer_local\": shimmer,\n",
    "        \"hnr\": hnr\n",
    "    }\n",
    "\n",
    "async def async_extract_parselmouth_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_parselmouth_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6055eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RMS Energy, ZCR, MFCC and Deltas using librosa\n",
    "def extract_librosa_features(data, sr):\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(data))\n",
    "    \n",
    "    rms = librosa.feature.rms(y=data)[0]\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    rms_var = np.var(rms)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_mean = np.mean(mfcc)\n",
    "    delta_mean = np.mean(delta)\n",
    "\n",
    "    return {\n",
    "        \"zcr\": zcr,\n",
    "        \"rms_mean\": rms_mean,\n",
    "        \"rms_std\": rms_std,\n",
    "        \"rms_var\": rms_var,\n",
    "        \"mfcc_mean\": mfcc_mean,\n",
    "        \"delta_mean\": delta_mean\n",
    "    }\n",
    "\n",
    "    \n",
    "async def async_extract_librosa_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_librosa_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a970b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_wave(data, sr):\n",
    "    return {\n",
    "        **extract_librosa_features(data, sr),\n",
    "        **extract_parselmouth_features(data, sr)\n",
    "    }\n",
    " \n",
    "    \n",
    "async def async_extract_features_from_wave(data, sr, executor):\n",
    "    # Start both tasks concurrently\n",
    "    librosa_task = asyncio.create_task(async_extract_librosa_features(data, sr, executor))\n",
    "    parselmouth_task = asyncio.create_task(async_extract_parselmouth_features(data, sr, executor))\n",
    "\n",
    "    # Wait for both\n",
    "    librosa_feats, parselmouth_feats = await asyncio.gather(librosa_task, parselmouth_task)\n",
    "\n",
    "    return {**librosa_feats, **parselmouth_feats}\n",
    "\n",
    "\n",
    "# Full function to extract all the features of the audio file\n",
    "async def extract_features(audio_data):\n",
    "    # -------------- Load the audio file --------------\n",
    "    if isinstance(audio_data, io.BytesIO):\n",
    "        data, sr = sf.read(audio_data)\n",
    "    else:\n",
    "        data, sr = sf.read(audio_data)\n",
    "    \n",
    "    assert len(data) > 160, \"Your audio file appears to contain no content. Please input a valid file\"\n",
    "    \n",
    "    # Convert to mono channel and resample\n",
    "    if data.ndim > 1:\n",
    "        data = data.mean(axis=1)\n",
    "    data = librosa.resample(data, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    duration_in_secs = len(data) / 16000\n",
    "    baseline_duration = max(10.0, duration_in_secs * 0.05)      # Minimum duration for baseline is 10 seconds\n",
    "\n",
    "    # Start the transcription job\n",
    "    print(\"Started transcription job\")\n",
    "    transcription_task = asyncio.create_task(transcribe_audio(audio_data))\n",
    "\n",
    "    # -------------- Get features of baseline and full wave --------------\n",
    "    baseline_data = data[:min(len(data), int(16000 * baseline_duration))]\n",
    "    baseline_feats = extract_features_from_wave(baseline_data, sr)\n",
    "    print(\"Gotten full wave features\")\n",
    "    full_feats = extract_features_from_wave(data, sr)\n",
    "    print(\"Gotten full wave features\")\n",
    "\n",
    "    # Get fluency ratings\n",
    "    features = ['zcr', 'pitch_mean', 'pitch_std', 'rms_mean', 'rms_std', 'rms_var', 'mfcc_mean', 'delta_mean']\n",
    "    rating_map = ['Low', 'Medium', 'High']\n",
    "        \n",
    "    baseline_fluency_features = np.array([baseline_feats[key] for key in baseline_feats if key in features])\n",
    "    full_fluency_features = np.array([full_feats[key] for key in full_feats if key in features])\n",
    "\n",
    "    res = fluency_model.predict(np.vstack((baseline_fluency_features, full_fluency_features)))\n",
    "    baseline_fluency = rating_map[res[0].argmax()]\n",
    "    full_fluency = rating_map[res[1].argmax()]\n",
    "    print(\"Get fluency ratings\")\n",
    "    \n",
    "    # Get Relative features\n",
    "    relative_feats = {}\n",
    "    for key in full_feats:\n",
    "        if key not in ['mfcc', 'delta_mfcc']:\n",
    "            base = baseline_feats.get(key, 0.0)\n",
    "            full = full_feats[key]\n",
    "            relative_feats[f'{key}_delta'] = full - base\n",
    "\n",
    "    # -------------- Get transcription and speaking rates --------------\n",
    "    transcription_json = await transcription_task\n",
    "    print(\"Gotten transcriptions\")\n",
    "    \n",
    "    # Baseline speaking rate\n",
    "    baseline_transcript = [word_segment['word'] for word_segment in transcription_json.words if word_segment['start'] <= baseline_duration]  # type: ignore\n",
    "    baseline_word_count = len(baseline_transcript)\n",
    "    baseline_transcript = \" \".join(baseline_transcript)\n",
    "    baseline_speaking_rate = baseline_word_count / baseline_duration\n",
    "    baseline_syllables_rate = estimate_syllable_rate(baseline_transcript, baseline_duration)\n",
    "\n",
    "    # Full data speaking rate\n",
    "    transcript = transcription_json.text\n",
    "    word_count = len(transcript.split())\n",
    "    speaking_rate = word_count / duration_in_secs\n",
    "    syllables_rate = estimate_syllable_rate(transcript, duration_in_secs)\n",
    "        \n",
    "    # -------------- Pause detection --------------\n",
    "    intervals = librosa.effects.split(data, top_db=30)\n",
    "    pauses = [(intervals[i][0] - intervals[i - 1][1]) / sr\n",
    "              for i in range(1, len(intervals))\n",
    "              if (intervals[i][0] - intervals[i - 1][1]) / sr > 1.0]\n",
    "    \n",
    "    long_pause_count = len(pauses)\n",
    "    long_pause_total = sum(pauses)\n",
    "\n",
    "    # -------------- Return full feedback for prompt generation --------------\n",
    "    return {\n",
    "        \"transcript\": transcript,\n",
    "        \"duration\": duration_in_secs,\n",
    "        \"baseline_duration\": int(baseline_duration),\n",
    "        \"speaking_rate\": speaking_rate,\n",
    "        \"syllables_rate\": syllables_rate,\n",
    "        \"baseline_speaking_rate\": baseline_speaking_rate,\n",
    "        \"baseline_syllables_rate\": baseline_syllables_rate,\n",
    "        \"long_pause_count\": long_pause_count,\n",
    "        \"long_pause_duration\": long_pause_total,\n",
    "        \"fluency_rating\": full_fluency,\n",
    "        \"baseline_fluency_rating\": baseline_fluency,\n",
    "        **full_feats,\n",
    "        **{f'baseline_{k}': v for k, v in baseline_feats.items()},\n",
    "        **relative_feats,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc49174",
   "metadata": {},
   "source": [
    "# Send to GPT for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f8a0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(audio_features, posture_features = None):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is a detailed snapshot of the speaker’s delivery — both baseline and full-clip — along with their changes. Use this to deliver personalized, context-aware feedback.\n",
    "\n",
    "## NOTE:\n",
    "- The **first {int(audio_features['baseline_duration'])} seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Interpret *changes* from baseline as signs of adaptation or stress — not necessarily flaws.\n",
    "- **Avoid quoting any raw values** in your response. Use intuitive, narrative insights only.\n",
    "- An 86% accurate ML model was used to rate the fluency of the speech, and that rating has also been provided to you.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📝 TRANSCRIPT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "<transcript>\n",
    "{audio_features['transcript']}\n",
    "</transcript>\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📏 BASELINE METRICS (First {int(audio_features['baseline_duration'])} seconds)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['baseline_fluency_rating']}\n",
    "- Words/sec: {audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['baseline_pitch_mean']:.2f} / {audio_features['baseline_pitch_std']:.2f} / {audio_features['baseline_pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['baseline_jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['baseline_shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['baseline_hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['baseline_rms_mean']:.2f} / {audio_features['baseline_rms_std']:.2f} / {audio_features['baseline_rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['baseline_zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['baseline_mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['baseline_delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📊 FULL CLIP METRICS\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['fluency_rating']}\n",
    "- Words/sec: {audio_features['speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['syllables_rate']:.2f}\n",
    "- Long pauses (>1s): {audio_features['long_pause_count']}\n",
    "- Total pause duration: {audio_features['long_pause_duration']:.2f} sec\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['pitch_mean']:.2f} / {audio_features['pitch_std']:.2f} / {audio_features['pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['rms_mean']:.2f} / {audio_features['rms_std']:.2f} / {audio_features['rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 RELATIVE CHANGES FROM BASELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Tempo & Fluency\n",
    "- Speaking rate ratio: {audio_features['speaking_rate'] / audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllable rate ratio: {audio_features['syllables_rate'] / audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Modulation\n",
    "- Pitch std delta: {audio_features['pitch_std_delta']:+.2f}\n",
    "- Jitter delta: {audio_features['jitter_local_delta']:+.3f}\n",
    "- Shimmer delta: {audio_features['shimmer_local_delta']:+.3f}\n",
    "- HNR delta: {audio_features['hnr_delta']:+.2f}\n",
    "\n",
    "## Energy\n",
    "- RMS mean delta: {audio_features['rms_mean_delta']:+.2f}\n",
    "- RMS std delta: {audio_features['rms_std_delta']:+.2f}\n",
    "- ZCR delta: {audio_features['zcr_delta']:+.3f}\n",
    "\n",
    "## Timbre\n",
    "- MFCC mean delta: {audio_features['mfcc_mean_delta']:+.2f}\n",
    "- Delta MFCC mean delta: {audio_features['delta_mean_delta']:+.6f}\n",
    "\n",
    "🧠 **Interpretation Tips** (for internal use only):\n",
    "- A **negative pitch_std_delta** might suggest monotony or nervousness; a positive value implies expressive modulation.\n",
    "- **Decreased RMS or HNR** may imply loss of vocal energy or confidence.\n",
    "- **Increased jitter/shimmer** may reflect stress or instability.\n",
    "- A **low syllable rate ratio** suggests slowing down relative to their natural pace, which may imply hesitation or deliberate pacing.\n",
    "- **ZCR changes** may reflect articulation style or clarity.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🧭 INSTRUCTIONS FOR FEEDBACK GENERATION\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Using the data above, write a highly personalized and supportive **narrative-style voice coaching paragraph**. Do not cite any specific numerical values. Your tone should be professional, encouraging, and practical.\n",
    "\n",
    "Structure your feedback in **three sections**:\n",
    "\n",
    "1. ✅ **What the speaker did well** — Highlight strengths or improvements in vocal control, energy, fluency, or confidence.\n",
    "2. 🛠️ **What they can improve** — Tactfully mention areas that deviated from their baseline and might affect clarity or delivery.\n",
    "3. 📊 **Confidence & fluency rating** — Conclude with your overall impression of their vocal confidence and fluency (e.g., low, moderate, high), based on relative metrics.\n",
    "\n",
    "DO NOT compare to average speakers. DO NOT be generic. Focus only on deviations from this speaker's own baseline and the emotional/functional impact of those changes.\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92efc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class FluencyEvaluator(BaseModel):\n",
    "    comment: str = Field(..., description=\"3-5 sentence feedback on fluency, including pace, fillers, pauses, flow.\")\n",
    "    score: int = Field(..., description=\"Fluency score (0-100).\", ge=1, le=100)\n",
    "\n",
    "class ContentEvaluator(BaseModel):\n",
    "    strengths: List[str] = Field(..., description=\"Strengths in content, structure, language, and grammar.\", min_length=2, max_length=5)\n",
    "    improvements: List[str] = Field(..., description=\"Suggestions for improvement in content, structure, language, grammar.\", min_length=2, max_length=5)\n",
    "    structure_score: int = Field(..., description=\"Score (0-100) for logical organization and transitions.\", ge=1, le=100)\n",
    "    grammar_score: int = Field(..., description=\"Score (0-100) for correctness of language use.\", ge=1, le=100)\n",
    "\n",
    "class SpeechEvaluator(BaseModel):\n",
    "    strengths: List[str] = Field(..., description=\"Strengths in clarity, delivery, and perceived confidence.\", min_length=2, max_length=5)\n",
    "    improvements: List[str] = Field(..., description=\"Suggestions for improvement in clarity, delivery, perceived confidence.\", min_length=2, max_length=5)\n",
    "    clarity_score: int = Field(..., description=\"Score (0-100) for clarity of speech.\", ge=1, le=100)\n",
    "    confidence_score: int = Field(..., description=\"Score (0-100) for perceived speaker confidence.\", ge=1, le=100)\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    fluency_evaluator: FluencyEvaluator\n",
    "    language_evaluator: ContentEvaluator\n",
    "    speech_evaluator: SpeechEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7923de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_prompt_with_schema(audio_features, response_schema = Feedback):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating the user's performance based on a variety of acoustic and prosodic features. Below is a detailed snapshot of the speaker's delivery — both baseline and full-clip — along with their changes. Use this to deliver personalized, context-aware feedback.\n",
    "\n",
    "## NOTE:\n",
    "- The **first {audio_features['baseline_duration']} seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Interpret *changes* from baseline as signs of adaptation or stress — not necessarily flaws.\n",
    "- **Avoid quoting any raw values** in your response. Use intuitive, narrative insights only.\n",
    "- An 86% accurate ML model was used to rate the fluency of the speech, and that rating has also been provided to you.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📝 TRANSCRIPT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "<transcript>\n",
    "{audio_features['transcript']}\n",
    "</transcript>\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📏 BASELINE METRICS (First {audio_features['baseline_duration']} seconds)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['baseline_fluency_rating']}\n",
    "- Words/sec: {audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['baseline_pitch_mean']:.2f} / {audio_features['baseline_pitch_std']:.2f} / {audio_features['baseline_pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['baseline_jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['baseline_shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['baseline_hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['baseline_rms_mean']:.2f} / {audio_features['baseline_rms_std']:.2f} / {audio_features['baseline_rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['baseline_zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['baseline_mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['baseline_delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📊 FULL CLIP METRICS\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['fluency_rating']}\n",
    "- Words/sec: {audio_features['speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['syllables_rate']:.2f}\n",
    "- Long pauses (>1s): {audio_features['long_pause_count']}\n",
    "- Total pause duration: {audio_features['long_pause_duration']:.2f} sec\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['pitch_mean']:.2f} / {audio_features['pitch_std']:.2f} / {audio_features['pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['rms_mean']:.2f} / {audio_features['rms_std']:.2f} / {audio_features['rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 RELATIVE CHANGES FROM BASELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Tempo & Fluency\n",
    "- Speaking rate ratio: {audio_features['speaking_rate'] / audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllable rate ratio: {audio_features['syllables_rate'] / audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Modulation\n",
    "- Pitch std delta: {audio_features['pitch_std_delta']:+.2f}\n",
    "- Jitter delta: {audio_features['jitter_local_delta']:+.3f}\n",
    "- Shimmer delta: {audio_features['shimmer_local_delta']:+.3f}\n",
    "- HNR delta: {audio_features['hnr_delta']:+.2f}\n",
    "\n",
    "## Energy\n",
    "- RMS mean delta: {audio_features['rms_mean_delta']:+.2f}\n",
    "- RMS std delta: {audio_features['rms_std_delta']:+.2f}\n",
    "- ZCR delta: {audio_features['zcr_delta']:+.3f}\n",
    "\n",
    "## Timbre\n",
    "- MFCC mean delta: {audio_features['mfcc_mean_delta']:+.2f}\n",
    "- Delta MFCC mean delta: {audio_features['delta_mean_delta']:+.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🧭 INSTRUCTIONS FOR FEEDBACK GENERATION\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "You are expected to evaluate the user's delivery **by taking on four different professional roles, one at a time**, and provide clear, structured feedback and scoring for each role.\n",
    "\n",
    "Below are your personas and what you focus on in each.\n",
    "\n",
    "---\n",
    "\n",
    "🎩 **1️⃣ Fluency Coach**\n",
    "You are a delivery specialist who analyzes the *flow* of speech.\n",
    "\n",
    "What to focus on:\n",
    "- Speaking pace and rhythm\n",
    "- Pauses and hesitations\n",
    "- Use of filler words (e.g., “um,” “uh,” “like”)\n",
    "- Smoothness and flow between sentences\n",
    "\n",
    "Your goals:\n",
    "- Provide a **concise, professional comment** on fluency in 3-5 sentences\n",
    "- Suggest whether the delivery felt smooth, hesitant, rushed, or confident\n",
    "- Assign a **fluency_score** (0-100), reflecting overall fluency\n",
    "\n",
    "---\n",
    "\n",
    "🎩 **2️⃣ Language Coach**\n",
    "You are an expert in the *content and language* of speaking.\n",
    "\n",
    "What to focus on:\n",
    "- Quality, relevance, and organization of ideas (content)\n",
    "- Logical structure and transitions between points\n",
    "- Accuracy and appropriateness of grammar\n",
    "- Vocabulary choice and variation\n",
    "- Sentence structure and clarity\n",
    "\n",
    "Your goals:\n",
    "- List 2-5 **strengths** in content, structure, language, and grammar\n",
    "- List 2-5 **areas for improvement** in those same areas\n",
    "- Assign:\n",
    "  - **structure_score** (0-100): Logical organization and flow\n",
    "  - **grammar_score** (0-100): Correctness of language use\n",
    "\n",
    "---\n",
    "\n",
    "🎩 **3️⃣ Speech Evaluator**\n",
    "You are a holistic evaluator of *communication impact*.\n",
    "\n",
    "What to focus on:\n",
    "- Clarity of pronunciation and articulation\n",
    "- Ease of understanding for the listener\n",
    "- Delivery style (tone, energy, vocal modulation)\n",
    "- Signs of confidence or nervousness\n",
    "- Audience engagement and persuasive power\n",
    "\n",
    "Your goals:\n",
    "- List 2-5 **strengths** in clarity, delivery, and perceived confidence\n",
    "- List 2-5 **areas for improvement** in those same areas\n",
    "- Assign:\n",
    "  - **clarity_score** (0-100): How clear and understandable the speech is\n",
    "  - **confidence_score** (0-100): How confident, convincing, and assured the speaker seems\n",
    "\n",
    "---\n",
    "\n",
    "✅ **IMPORTANT OUTPUT RULES**\n",
    "- Do not necessarily interpret *relative changes from baseline* as flaws.\n",
    "- Be supportive, specific, and context-aware.\n",
    "- Avoid quoting or mentioning any raw numerical feature values.\n",
    "- Avoid mentioning baseline changes or the baseline.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📄 OUTPUT FORMAT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "Strictly follow this JSON format:\n",
    "{{\n",
    "  \"fluency_evaluator\": {{\n",
    "    \"comment\": str,\n",
    "    \"fluency_score\": int\n",
    "  }},\n",
    "  \"language_evaluator\": {{\n",
    "    \"strengths\": [str],\n",
    "    \"improvements\": [str],\n",
    "    \"structure_score\": int,\n",
    "    \"grammar_score\": int\n",
    "  }},\n",
    "  \"speech_evaluator\": {{\n",
    "    \"strengths\": [str],\n",
    "    \"improvements\": [str],\n",
    "    \"clarity_score\": int,\n",
    "    \"confidence_score\": int\n",
    "  }},\n",
    "}}\n",
    "\n",
    "This is the schema you must follow:\n",
    "{json.dumps(response_schema.model_json_schema(), indent=2)}\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "751d8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feedback(audio_features, posture_features = None, response_schema = None, llm_model : str = \"llama-3.3-70b-versatile\"):\n",
    "    prompt = get_prompt_with_schema(audio_features)\n",
    "\n",
    "    client = Groq()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_completion_tokens=32768,\n",
    "        top_p=1,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211bdbf",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0365e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started transcription job\n",
      "Gotten full wave features\n",
      "Gotten full wave features\n",
      "Get fluency ratings\n",
      "Gotten transcriptions\n"
     ]
    }
   ],
   "source": [
    "# Test on your speech sample\n",
    "path = \"../../samples/tim-urban.wav\"                                   # ENTER PATH TO YOUR AUDIO FILE HERE\n",
    "\n",
    "if path:\n",
    "    features = await extract_features(path)\n",
    "    feedback = generate_feedback(features)\n",
    "else:\n",
    "    feedback = \"No audio path provided, feedback cannot be given\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e792308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"fluency_evaluator\": {\n",
      "        \"comment\": \"The speaker's delivery was marked by a notable decrease in speaking rate, suggesting a more deliberate and thoughtful pace. However, this change also introduced a sense of hesitancy, with the speaker seeming to choose words more carefully. The flow between sentences was generally smooth, but the overall impression was one of caution rather than confidence. The use of pauses was minimal, which helped maintain a sense of continuity.\",\n",
      "        \"fluency_score\": 70\n",
      "    },\n",
      "    \"language_evaluator\": {\n",
      "        \"strengths\": [\n",
      "            \"The speaker demonstrated a strong ability to organize ideas in a logical and coherent manner.\",\n",
      "            \"The use of anecdotes and personal experiences added depth and engagement to the content.\",\n",
      "            \"Vocabulary choice was varied and appropriate, contributing to the overall clarity of the message.\"\n",
      "        ],\n",
      "        \"improvements\": [\n",
      "            \"Transitions between ideas could be smoother, with more explicit connections made between different points.\",\n",
      "            \"Some sentences were lengthy and convoluted, which could be simplified for better comprehension.\",\n",
      "            \"The speaker could benefit from practicing more concise summaries of main points to reinforce key takeaways.\"\n",
      "        ],\n",
      "        \"structure_score\": 85,\n",
      "        \"grammar_score\": 90\n",
      "    },\n",
      "    \"speech_evaluator\": {\n",
      "        \"strengths\": [\n",
      "            \"The speaker showed a good range of vocal modulation, which helped maintain audience interest.\",\n",
      "            \"The use of humor and self-deprecation was effective in engaging the audience and making the content more relatable.\",\n",
      "            \"Despite moments of hesitation, the speaker's enthusiasm for the topic was evident and infectious.\"\n",
      "        ],\n",
      "        \"improvements\": [\n",
      "            \"To enhance clarity, the speaker could work on enunciating more clearly, especially at the beginnings and ends of sentences.\",\n",
      "            \"Confidence could be boosted by maintaining eye contact with the audience and using more assertive body language.\",\n",
      "            \"Practicing the delivery to reduce hesitations and fillers would improve the overall flow and professionalism of the speech.\"\n",
      "        ],\n",
      "        \"clarity_score\": 80,\n",
      "        \"confidence_score\": 75\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "if isinstance(feedback, str):\n",
    "    ipd.display(ipd.Markdown(feedback))\n",
    "else:\n",
    "    print(json.dumps(json.loads(feedback.content), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4fdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
