{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b5d9a",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25780097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: groq in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.29.0)\n",
      "Collecting load_dotenv\n",
      "  Downloading load_dotenv-0.1.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (4.14.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Collecting python-dotenv (from load_dotenv)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Downloading load_dotenv-0.1.0-py3-none-any.whl (7.2 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, load_dotenv\n",
      "\n",
      "   ---------------------------------------- 0/2 [python-dotenv]\n",
      "   ---------------------------------------- 0/2 [python-dotenv]\n",
      "   ---------------------------------------- 2/2 [load_dotenv]\n",
      "\n",
      "Successfully installed load_dotenv-0.1.0 python-dotenv-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas librosa groq load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771b07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nltk in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (0.9.0)\n",
      "Collecting parselmouth\n",
      "  Downloading parselmouth-1.1.1.tar.gz (33 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (7.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\.venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Collecting googleads==3.8.0 (from parselmouth)\n",
      "  Downloading googleads-3.8.0.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [1 lines of output]\n",
      "      error in googleads setup command: use_2to3 is invalid.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk tiktoken parselmouth pydub psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91bfa84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import asyncio\n",
    "import librosa\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "from pydub import AudioSegment\n",
    "from nltk.corpus import cmudict\n",
    "from parselmouth.praat import call\n",
    "from groq import Groq, AsyncClient\n",
    "from groq.types.audio import Transcription\n",
    "\n",
    "# Load environment file\n",
    "from load_dotenv import load_dotenv\n",
    "print(load_dotenv('.env.local'))\n",
    "\n",
    "assert os.environ.get('GROQ_API_KEY'), \"Groq API key not found in .env file, please set the key before starting this notebook\"\n",
    "\n",
    "# Global variables\n",
    "client = AsyncClient()\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "fluency_model = joblib.load('fluency/models/weights/xgboost_model.pkl')\n",
    "\n",
    "try:\n",
    "    cmu_dict = cmudict.dict()\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('cmudict')\n",
    "    cmu_dict = cmudict.dict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f222a",
   "metadata": {},
   "source": [
    "## Monitor CPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d9afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import functools\n",
    "\n",
    "def monitor_resources(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process = psutil.Process(os.getpid())\n",
    "\n",
    "        # Get memory and CPU before\n",
    "        mem_before = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "        cpu_before = process.cpu_percent(interval=None)\n",
    "\n",
    "        # Start time and CPU\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get memory and CPU after\n",
    "        mem_after = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "        cpu_after = process.cpu_percent(interval=0.1)\n",
    "\n",
    "        # Get number of CPUs used\n",
    "        cpu_affinity = process.cpu_affinity()\n",
    "        \n",
    "        print(f\"Function: {func.__name__}\")\n",
    "        print(f\"Execution Time: {end_time - start_time:.2f} sec\")\n",
    "        print(f\"Memory Usage: {mem_after - mem_before:.2f} MB\")\n",
    "        print(f\"CPU Usage: {cpu_after:.2f}%\")\n",
    "        print(f\"CPU Cores Used: {cpu_affinity}\")\n",
    "\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def limit_to_one_core(core_id=0):\n",
    "    \"\"\"\n",
    "    Set process to run only on one CPU core (default: core 0).\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            process = psutil.Process(os.getpid())\n",
    "\n",
    "            # Store the current affinity to restore later\n",
    "            original_affinity = process.cpu_affinity()\n",
    "            \n",
    "            try:\n",
    "                # Set affinity to a single core\n",
    "                process.cpu_affinity([core_id])\n",
    "                print(f\"Running {func.__name__} on CPU core {core_id}\")\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                # Restore original affinity\n",
    "                process.cpu_affinity(original_affinity)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# # Limit NumPy, OpenBLAS etc to use only one CPU core\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c16201",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Features extracted:\n",
    "* ZCR\n",
    "* Pitch\n",
    "* Jitter\n",
    "* Shimmer\n",
    "* Harmonic-to-Noise ratio\n",
    "* RMS\n",
    "* MFCC\n",
    "* DeltaMFCC\n",
    "* SpeakingRate\n",
    "* PauseCount\n",
    "* PauseDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b64c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transcription(text=\" Hi, my name is Adkarsh Malaya. I'm a student and right now what I'm trying to do is I'm trying to get a model and I'm trying to use it to transcribe some filler words. I am very scared. I don't know what will happen and I really really hope this works.\", words=[{'word': 'Hi,', 'start': 0.78, 'end': 1.18}, {'word': 'my', 'start': 1.18, 'end': 1.52}, {'word': 'name', 'start': 1.52, 'end': 1.66}, {'word': 'is', 'start': 1.66, 'end': 1.82}, {'word': 'Adkarsh', 'start': 1.82, 'end': 2.04}, {'word': 'Malaya.', 'start': 2.04, 'end': 2.52}, {'word': \"I'm\", 'start': 2.52, 'end': 3.12}, {'word': 'a', 'start': 3.12, 'end': 3.26}, {'word': 'student', 'start': 3.26, 'end': 3.76}, {'word': 'and', 'start': 3.76, 'end': 4.22}, {'word': 'right', 'start': 4.22, 'end': 4.62}, {'word': 'now', 'start': 4.62, 'end': 4.84}, {'word': 'what', 'start': 4.84, 'end': 5.1}, {'word': \"I'm\", 'start': 5.1, 'end': 5.26}, {'word': 'trying', 'start': 5.26, 'end': 5.38}, {'word': 'to', 'start': 5.38, 'end': 5.5}, {'word': 'do', 'start': 5.5, 'end': 5.64}, {'word': 'is', 'start': 5.64, 'end': 5.88}, {'word': \"I'm\", 'start': 5.88, 'end': 6.02}, {'word': 'trying', 'start': 6.02, 'end': 6.2}, {'word': 'to', 'start': 6.2, 'end': 6.34}, {'word': 'get', 'start': 6.34, 'end': 6.54}, {'word': 'a', 'start': 6.54, 'end': 6.84}, {'word': 'model', 'start': 6.84, 'end': 7.08}, {'word': 'and', 'start': 7.08, 'end': 7.7}, {'word': \"I'm\", 'start': 7.7, 'end': 7.88}, {'word': 'trying', 'start': 7.88, 'end': 8.02}, {'word': 'to', 'start': 8.02, 'end': 8.18}, {'word': 'use', 'start': 8.18, 'end': 8.76}, {'word': 'it', 'start': 8.76, 'end': 8.98}, {'word': 'to', 'start': 8.98, 'end': 9.18}, {'word': 'transcribe', 'start': 9.18, 'end': 9.82}, {'word': 'some', 'start': 9.82, 'end': 10.06}, {'word': 'filler', 'start': 10.06, 'end': 10.34}, {'word': 'words.', 'start': 10.34, 'end': 10.68}, {'word': 'I', 'start': 10.68, 'end': 11.46}, {'word': 'am', 'start': 11.46, 'end': 11.56}, {'word': 'very', 'start': 11.56, 'end': 11.76}, {'word': 'scared.', 'start': 11.76, 'end': 12.22}, {'word': 'I', 'start': 12.22, 'end': 12.54}, {'word': \"don't\", 'start': 12.54, 'end': 12.7}, {'word': 'know', 'start': 12.7, 'end': 12.78}, {'word': 'what', 'start': 12.78, 'end': 12.92}, {'word': 'will', 'start': 12.92, 'end': 13.04}, {'word': 'happen', 'start': 13.04, 'end': 13.22}, {'word': 'and', 'start': 13.22, 'end': 13.96}, {'word': 'I', 'start': 13.96, 'end': 14.16}, {'word': 'really', 'start': 14.16, 'end': 14.42}, {'word': 'really', 'start': 14.42, 'end': 14.58}, {'word': 'hope', 'start': 14.58, 'end': 14.78}, {'word': 'this', 'start': 14.78, 'end': 14.96}, {'word': 'works.', 'start': 14.96, 'end': 15.26}], duration=15.72)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async Transcription\n",
    "def split_audio_in_memory(audio_path, max_mb=24):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    bytes_per_ms = (audio.frame_rate * audio.frame_width * audio.channels) / 1000\n",
    "    max_bytes = max_mb * 1024 * 1024\n",
    "    chunk_duration_ms = int(max_bytes / bytes_per_ms)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(audio), chunk_duration_ms):\n",
    "        chunk = audio[i:i+chunk_duration_ms]\n",
    "        buffer = io.BytesIO()\n",
    "        chunk.export(buffer, format=\"wav\")\n",
    "        buffer.seek(0)\n",
    "        chunks.append((f\"chunk_{i//chunk_duration_ms}.wav\", buffer))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "async def transcribe_chunk(filename, audio_buffer):\n",
    "    return await client.audio.transcriptions.create(\n",
    "        file=(filename, audio_buffer.read()),\n",
    "        model=\"distil-whisper-large-v3-en\",\n",
    "        response_format=\"verbose_json\",\n",
    "        timestamp_granularities=[\"word\"]\n",
    "    )\n",
    "\n",
    "\n",
    "async def transcribe_audio(audio_path, client=client):\n",
    "    \"\"\"Transcribe an audio file without saving the chunks to disk\"\"\"\n",
    "    chunks = split_audio_in_memory(audio_path)\n",
    "    tasks = [transcribe_chunk(name, buffer) for name, buffer in chunks]\n",
    "    all_transcripts = await asyncio.gather(*tasks)\n",
    "\n",
    "    transcript_parts = []\n",
    "    all_words = []\n",
    "    total_duration = 0.0\n",
    "\n",
    "    for chunk in all_transcripts:\n",
    "        transcript_parts.append(chunk.text)\n",
    "        all_words.extend(getattr(chunk, \"words\", []))\n",
    "        total_duration += chunk.duration\n",
    "\n",
    "    transcript = \"\".join(transcript_parts)\n",
    "    \n",
    "    return Transcription(text=transcript, words=all_words, duration=total_duration)\n",
    "\n",
    "\n",
    "transcript = await transcribe_audio(\"samples/confident.wav\")\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1898093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for calculating syllables speaking rate\n",
    "def get_word_syllable_count(word):\n",
    "    word = word.lower().strip(\".,?!;:\")\n",
    "    if word in cmu_dict:\n",
    "        return len([p for p in cmu_dict[word][0] if p[-1].isdigit()])\n",
    "    return max(1, len(re.findall(r'[aeiouy]+', word)))\n",
    "\n",
    "\n",
    "def estimate_syllable_rate(transcript, duration_sec):\n",
    "    words = transcript.split()\n",
    "    total_syllables = sum(get_word_syllable_count(word) for word in words)\n",
    "    return total_syllables / duration_sec if duration_sec > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958711f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pitch statistics, Jitter, Shimmer, and HNR ratio through Parselmouth\n",
    "@monitor_resources\n",
    "def extract_parselmouth_features(data, sr):\n",
    "    snd = parselmouth.Sound(values=data, sampling_frequency=sr)\n",
    "\n",
    "    pitch_obj = snd.to_pitch()\n",
    "    pitch_mean = call(pitch_obj, \"Get mean\", 0, 0, \"Hertz\")\n",
    "    pitch_std = call(pitch_obj, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "\n",
    "    point_process = call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "    jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    shimmer = call([snd, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "    harmonicity = call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "\n",
    "    return {\n",
    "        \"pitch_mean\": pitch_mean,\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"pitch_var\": pitch_std**2,\n",
    "        \"jitter_local\": jitter,\n",
    "        \"shimmer_local\": shimmer,\n",
    "        \"hnr\": hnr\n",
    "    }\n",
    "\n",
    "async def async_extract_parselmouth_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_parselmouth_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6055eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RMS Energy, ZCR, MFCC and Deltas using librosa\n",
    "@monitor_resources\n",
    "def extract_librosa_features(data, sr):\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(data))\n",
    "    \n",
    "    rms = librosa.feature.rms(y=data)[0]\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    rms_var = np.var(rms)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_mean = np.mean(mfcc)\n",
    "    delta_mean = np.mean(delta)\n",
    "\n",
    "    return {\n",
    "        \"zcr\": zcr,\n",
    "        \"rms_mean\": rms_mean,\n",
    "        \"rms_std\": rms_std,\n",
    "        \"rms_var\": rms_var,\n",
    "        \"mfcc\": mfcc.mean(axis=1),\n",
    "        \"delta_mfcc\": delta.mean(axis=1),\n",
    "        \"mfcc_mean\": mfcc_mean,\n",
    "        \"delta_mean\": delta_mean\n",
    "    }\n",
    "    \n",
    "async def async_extract_librosa_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_librosa_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_wave(data, sr):\n",
    "    return {\n",
    "        **extract_librosa_features(data, sr),\n",
    "        **extract_parselmouth_features(data, sr)\n",
    "    }\n",
    "    \n",
    "async def async_extract_features_from_wave(data, sr, executor):\n",
    "    # Start both tasks concurrently\n",
    "    librosa_task = asyncio.create_task(async_extract_librosa_features(data, sr, executor))\n",
    "    parselmouth_task = asyncio.create_task(async_extract_parselmouth_features(data, sr, executor))\n",
    "\n",
    "    # Wait for both\n",
    "    librosa_feats, parselmouth_feats = await asyncio.gather(librosa_task, parselmouth_task)\n",
    "\n",
    "    return {**librosa_feats, **parselmouth_feats}\n",
    "\n",
    "\n",
    "@monitor_resources\n",
    "async def extract_features(audio_path, baseline_duration: float = 0.0, fluency_model=fluency_model):\n",
    "    # -------------- Load the audio file --------------\n",
    "    data, sr = librosa.load(audio_path)\n",
    "    assert len(data) != 0, \"Your audio file appears to contain no content. Please input a valid file\"\n",
    "    \n",
    "    \n",
    "    # -------------- Get transcription and check minimum duration --------------\n",
    "    transcription_json = await transcribe_audio(audio_path)\n",
    "    duration_sec = transcription_json.duration # type: ignore\n",
    "    baseline_duration = baseline_duration or max(10.0, duration_sec * 0.05)\n",
    "\n",
    "    assert duration_sec != 0, \"File duration appears to be 0 after transcription?\"\n",
    "    \n",
    "    \n",
    "    # -------------- Get features of baseline and full wave --------------\n",
    "    baseline_data = data[:min(len(data), int(sr * baseline_duration))]\n",
    "    baseline_feats = extract_features_from_wave(baseline_data, sr)\n",
    "    full_feats = extract_features_from_wave(data, sr)\n",
    "\n",
    "\n",
    "    # -------------- Get fluency ratings --------------\n",
    "    features = ['zcr', 'pitch_mean', 'pitch_std', 'rms_mean', 'rms_std', 'rms_var', 'mfcc_mean', 'delta_mean']\n",
    "    rating_map = ['Low', 'Medium', 'High']\n",
    "        \n",
    "    baseline_fluency_features = np.array([baseline_feats[key] for key in baseline_feats if key in features])\n",
    "    full_fluency_features = np.array([full_feats[key] for key in full_feats if key in features])\n",
    "\n",
    "    res = fluency_model.predict(np.vstack((baseline_fluency_features, full_fluency_features)))\n",
    "    baseline_fluency = rating_map[res[0].argmax()]\n",
    "    full_fluency = rating_map[res[1].argmax()]\n",
    "\n",
    "    relative_feats = {}\n",
    "    for key in full_feats:\n",
    "        if key not in ['mfcc', 'delta_mfcc']:\n",
    "            base = baseline_feats.get(key, 0.0)\n",
    "            full = full_feats[key]\n",
    "            relative_feats[f'{key}_delta'] = full - base\n",
    "    \n",
    "    \n",
    "    # -------------- Get speaking rates --------------\n",
    "    # Assuming the transcript has come by now\n",
    "\n",
    "    # Baseline speaking rate\n",
    "    baseline_transcript = [word_segment['word'] for word_segment in transcription_json.words if word_segment['start'] <= baseline_duration]  # type: ignore\n",
    "    baseline_word_count = len(baseline_transcript)\n",
    "    baseline_transcript = \" \".join(baseline_transcript)\n",
    "    baseline_speaking_rate = baseline_word_count / baseline_duration\n",
    "    baseline_syllables_rate = estimate_syllable_rate(baseline_transcript, baseline_duration)\n",
    "\n",
    "    # Full data speaking rate\n",
    "    transcript = transcription_json.text\n",
    "    word_count = len(transcript.split())\n",
    "    speaking_rate = word_count / duration_sec\n",
    "    syllables_rate = estimate_syllable_rate(transcript, duration_sec)\n",
    "        \n",
    "    \n",
    "    # -------------- Pause detection --------------\n",
    "    intervals = librosa.effects.split(data, top_db=30)\n",
    "    pauses = [(intervals[i][0] - intervals[i - 1][1]) / sr\n",
    "              for i in range(1, len(intervals))\n",
    "              if (intervals[i][0] - intervals[i - 1][1]) / sr > 1.0]\n",
    "    \n",
    "    long_pause_count = len(pauses)\n",
    "    long_pause_total = sum(pauses)\n",
    "\n",
    "    return {\n",
    "        \"transcript\": transcript,\n",
    "        \"duration\": duration_sec,\n",
    "        \"baseline_duration\": baseline_duration,\n",
    "        \"speaking_rate\": speaking_rate,\n",
    "        \"syllables_rate\": syllables_rate,\n",
    "        \"baseline_speaking_rate\": baseline_speaking_rate,\n",
    "        \"baseline_syllables_rate\": baseline_syllables_rate,\n",
    "        \"long_pause_count\": long_pause_count,\n",
    "        \"long_pause_duration\": long_pause_total,\n",
    "        \"fluency_rating\": full_fluency,\n",
    "        \"baseline_fluency_rating\": baseline_fluency,\n",
    "        **full_feats,\n",
    "        **{f'baseline_{k}': v for k, v in baseline_feats.items()},\n",
    "        **relative_feats,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05c6d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 14.20%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 1.69 sec\n",
      "Memory Usage: 14.68 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.35 sec\n",
      "Memory Usage: 4.38 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 1.27 sec\n",
      "Memory Usage: 3.27 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 5.06 sec\n",
      "Memory Usage: -5.28 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "features = await extract_features('samples/tim-urban.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d332011f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Reviewer.pxs. So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know, you get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light, and I'd bump it up in the middle months. And then at the end, I would kick it up into high gear, just like a little staircase. How hard can it be to just walk up the stairs? No big deal, right? But then the funniest thing happened. Those first few months, they came and went, and I couldn't quite do stuff. So we had an awesome new revised plan. And then ... But then those middle months actually went by, and I didn't really write words. And so we were here. And then two months turned into one month, which turned into two weeks. And one day I woke up, with three days until the deadline, still not having written a word, and so I did the only thing I could. I wrote 90 pages over 72 hours, pulling not one but two all-nighters. Humans are not supposed to pull two all-nighters. Sprinted across campus, dove in slow motion, and got it in just at the deadline. I thought that was the end of everything. But a week later, I get a call. It's the school. And they say, is this Tim Urban? And I say, yeah. And they say, we need to talk about your thesis. And I say, OK. And they say, it's the best one we've ever seen. That did not happen. It was a very, very bad thesis. I just wanted to enjoy that one moment when all of you thought, this guy is amazing! No, no, it was very, very bad. Anyway, today I'm a writer, blogger, guy. I write the blog Wait But Why. And a couple of years ago, I decided to write about procrastination. My behaviors always perplexed the non-procrastinators around me, and I wanted to explain to the non-procrastinators of the world what goes on in the heads of procrastinators and why we are the way we are. Now, I had a hypothesis that the brains of procrastinators were actually different than the brains of other people. And to test this, I found an MRI lab that actually let me scan both my brain and the brain of a proven non-procrastinator so I could compare them. And I actually brought them here to show you today, and I want you to take a look carefully to see if you can notice a difference. And I know that if you're not a trained brain expert, it's not that obvious, but just take a look, OK? So here's the brain of a non-procrastinator. Now, here's my brain. There is a difference. Both brains have a rational decision-maker in them, but the procrastinator's brain also has an instant gratification monkey. Now, what does this mean for the procrastinator? It means everything's fine until this happens. So the rational decision-maker will make the rational decision to do something productive, but the monkey doesn't like that plan. So he actually takes the wheel and he says, actually, let's read the entire Wikipedia page of the Nancy Kerrigan-Tanya Harding scandal, because I just remembered that that happened. Then we're going to go over to the fridge, we're going to see if there's anything new in there since 10 minutes ago. After that, we're going to go on a YouTube spiral that starts with videos of Richard Feynman talking about magnets and ends much, much later with us watching interviews with Justin Bieber's mom. All of that's going to take a while, so we're not going to really have room on the schedule for any work today. Sorry. Now, what is going on here? The instant gratification monkey does not seem like a guy you want behind the wheel. He lives entirely in the present moment, he has no memory of the past, no knowledge of the future, He cares about two things, easy and fun. Now, in the animal world, that works fine. If you're a dog, and you spend your whole life doing nothing other than easy and fun things, you're a huge success. And to the monkey, humans are just another animal species. He has to keep well-slept, well-fed and propagating into the next generation, which in tribal times might have worked OK. But if you haven't noticed, now we're not in tribal times. We're in an advanced civilization, and the monkey does not know what that is. Which is why we have another guy in our brain, the rational decision-maker, who gives us the ability to do things no other animal can do. We can visualize the future, we can see the big picture, We can make long-term plans, and he wants to take all of that into account, and he wants to just have us do whatever makes sense to be doing right now. Now, sometimes it makes sense to be doing things that are easy and fun, like when you're having dinner or going to bed or enjoying well-earned leisure time. That's why there's an overlap. Sometimes they agree. But other times, it makes much more sense to be doing things that are harder and less pleasant for the sake of the big picture, and that's when we have a conflict. And for the procrastinator, that conflict tends to end a certain way every time, leaving him spending a lot of time in this orange zone, an easy and fun place that's entirely out of the make-sense circle. I call it the dark playground. Now, the dark playground is a place that all of you procrastinators out there know very well. It's where leisure activities happen at times at times when leisure activities are not supposed to be happening. The fun you have in the dark playground isn't actually fun because it's completely unearned, and the air is filled with guilt, dread, anxiety, self-hatred, all those good procrastinator feelings. And the question is, in this situation, with the monkey behind the wheel, how does the procrastinator ever get himself over here to this blue zone, a less pleasant place, but where really important things happen? Well, it turns out that the procrastinator has a guardian angel, someone who's always looking down on him and watching over him in his darkest moments, someone called the panic monster. Now, the panic monster is dormant most of the time, but he suddenly wakes up any time a deadline gets too close or there's danger of public embarrassment, a career disaster or some other scary consequence. And importantly, he's the only thing that the monkey is terrified of. Now, he became very relevant in my life pretty recently because the people of TED reached out to me about six months ago and invited me to do a TED Talk. Now, of course, I said, yes, it's always been a dream of mine to have done a TED Talk in the past. But, But in the middle of all this excitement, the rational decision-maker seemed to have something else in his mind. He was saying, are we clear on what we just accepted? Do we get what's going to be now happening one day in the future? We need to sit down and work on this right now. And the monkey said, totally agree, but also let's just open Google Earth and let's zoom into the bottom of India, like 200 feet above the ground, and we're going to scroll up two and a half hours so we can get a better feel for India. So that's what we did that day. As six months turned into four, and then two, and then one, the people of TED decided to release the speakers. And I opened up the website, and there was my face staring right back at me, and guess who woke up? So the panic monster starts losing his mind, and a few seconds later, the whole system's in mayhem. And the monkey, who, remember, he's terrified of the panic monster, And finally, finally, the rational decision-maker can take the wheel and I can start working on the talk. Now, the panic monster explains all kinds of pretty insane, procrastinated behavior, like how someone like me could spend two weeks unable to start the opening sentence of a paper and then miraculously find the unbelievable work ethic to stay up all night and write eight pages. And this entire situation with the three characters, three characters, this is the procrastinator's system. It's not pretty, but in the end, it works. And this is what I decided to write about on the blog just a couple years ago. Now, when I did, I was amazed by the response. Literally thousands of emails came in from all different kinds of people, from all over the world, doing all different kinds of things. These were people who were nurses and bankers and painters and engineers and lots and lots of PhD students. And they were all writing, saying the same thing. I have this problem, too. But what struck me was the contrast between the light tone of the post and the heaviness of these emails. These people were writing with intense frustration about what procrastination had done for them, about what this monkey had done to them. And I thought about this, and I said, if the procrastinator system works, then what's going on? Why are all these people in such a dark place? Well, it turns out that there's two kinds of procrastination. Everything I've talked about today, the examples I've given, they all have deadlines. And when there's deadlines, the effects of procrastination are contained to the short term because the panic monster gets involved. There's a second kind of procrastination that happens in situations when there is no deadline. So if you want to have a career where you want to be a self-starter, something in the arts, something entrepreneurial, there's no deadlines on those things at first, because nothing's happening at first, not until you've gone out and done the hard work to get some momentum, to get things going. There's also all kinds of important things outside of your career that don't involve any deadlines, like seeing your family or exercising and taking care of your health, working on your relationship or getting out of a relationship that isn't working. Now, if the procrastinator's only mechanism of doing these hard things is the panic monster, that's a problem, because in all of these non-deadline situations, the panic monster doesn't show up, he has nothing to wake up for, they're not contained, they just extend outward forever. And it's this long-term kind of procrastination that's much less visible and much less talked about than the funnier short-term deadline-based kind. It's usually suffered quietly and privately, and it can be the source of a huge amount of long-term unhappiness and regrets. And I thought, you know, that's why these people are emailing, and that's why they're in such a bad place. It's not that they're cramming for some project. It's that long-term procrastination has made them feel like a spectator, at times, in their own lives. The frustration was not that they couldn't achieve their dreams, it's that they weren't even able to start chasing them. So I read these emails and I had a little bit of an epiphany that I don't think non-procrastinators exist. That's right, I think all of you are procrastinators. Now, you might not all be a mess, like some of us. And some of you may have a healthy relationship with deadlines. But remember, the monkey's sneakiest trick is when the deadlines aren't there. Now, I want to show you one last thing. I call this a life calendar. That's one box for every week of a 90-year life. That's not that many boxes, especially since we've already used a bunch of those. So I think we need to all take a long, hard look at that calendar. We need to think about what we're really procrastinating on, because everyone is procrastinating on something in life. We need to stay aware of the instant gratification monkey. That's a job for all of us. And because there's not that many boxes on there, it's a job that should probably start today. Well, maybe not today, but... You know, sometime soon. Thank you. Thank you.\",\n",
       " 'duration': 843.7500000000001,\n",
       " 'baseline_duration': 42.18833560090703,\n",
       " 'speaking_rate': 2.639407407407407,\n",
       " 'syllables_rate': 3.6989629629629626,\n",
       " 'baseline_speaking_rate': 30.790501248692827,\n",
       " 'baseline_syllables_rate': 42.83174423124553,\n",
       " 'long_pause_count': 28,\n",
       " 'long_pause_duration': 39.96154195011337,\n",
       " 'fluency_rating': 'Low',\n",
       " 'baseline_fluency_rating': 'Low',\n",
       " 'zcr': 0.12431602464380555,\n",
       " 'rms_mean': 0.070083246,\n",
       " 'rms_std': 0.061486296,\n",
       " 'rms_var': 0.0037805648,\n",
       " 'mfcc': array([-232.46727  ,  101.34686  ,  -19.867247 ,   15.719451 ,\n",
       "          -4.8433776,   -9.056608 ,  -11.200629 ,   -8.6175165,\n",
       "         -10.230278 ,   -1.2496375,   -5.5351243,   -2.8022056,\n",
       "          -4.6042967], dtype=float32),\n",
       " 'delta_mfcc': array([-1.4429400e-04,  2.3761578e-04,  3.8400068e-04, -1.1415667e-04,\n",
       "         6.7685498e-05,  1.4521193e-04,  1.3248599e-04,  1.1030027e-04,\n",
       "         1.4142298e-04,  1.9693849e-04,  1.6654511e-04,  1.4718014e-04,\n",
       "        -7.9398051e-05], dtype=float32),\n",
       " 'mfcc_mean': -14.877528,\n",
       " 'delta_mean': 0.000107041196,\n",
       " 'pitch_mean': 208.4127135401901,\n",
       " 'pitch_std': 79.55233694275556,\n",
       " 'pitch_var': 6328.574313053711,\n",
       " 'jitter_local': 0.027873774485195162,\n",
       " 'shimmer_local': 0.12354313818068587,\n",
       " 'hnr': 8.696793580183748,\n",
       " 'baseline_zcr': 0.11371094824917446,\n",
       " 'baseline_rms_mean': 0.06305937,\n",
       " 'baseline_rms_std': 0.058996927,\n",
       " 'baseline_rms_var': 0.0034806374,\n",
       " 'baseline_mfcc': array([-255.28789  ,  105.465546 ,  -15.627713 ,   17.496836 ,\n",
       "           1.7893533,   -4.4778514,   -5.8012633,   -6.3778076,\n",
       "          -4.190897 ,    3.2504325,   -2.3401265,    0.7608236,\n",
       "          -1.9429184], dtype=float32),\n",
       " 'baseline_delta_mfcc': array([ 0.23301336,  0.04249881, -0.01336857,  0.02402507,  0.00256586,\n",
       "        -0.01195078, -0.01759782, -0.01087944, -0.00485254,  0.00321395,\n",
       "        -0.00905112, -0.00292875, -0.00775515], dtype=float32),\n",
       " 'baseline_mfcc_mean': -12.86796,\n",
       " 'baseline_delta_mean': 0.017456377,\n",
       " 'baseline_pitch_mean': 202.56118723150064,\n",
       " 'baseline_pitch_std': 111.33053868942093,\n",
       " 'baseline_pitch_var': 12394.48884487665,\n",
       " 'baseline_jitter_local': 0.030714485586106302,\n",
       " 'baseline_shimmer_local': 0.1309953102233376,\n",
       " 'baseline_hnr': 6.344837854100443,\n",
       " 'zcr_delta': 0.010605076394631083,\n",
       " 'zcr_ratio': 1.0932634593055386,\n",
       " 'rms_mean_delta': 0.0070238784,\n",
       " 'rms_mean_ratio': 1.1113852,\n",
       " 'rms_std_delta': 0.0024893694,\n",
       " 'rms_std_ratio': 1.0421948,\n",
       " 'rms_var_delta': 0.0002999273,\n",
       " 'rms_var_ratio': 1.0861702,\n",
       " 'mfcc_mean_delta': -2.0095682,\n",
       " 'mfcc_mean_ratio': 1.1561683,\n",
       " 'delta_mean_delta': -0.017349336,\n",
       " 'delta_mean_ratio': 0.006131925,\n",
       " 'pitch_mean_delta': 5.851526308689472,\n",
       " 'pitch_mean_ratio': 1.0288876975331012,\n",
       " 'pitch_std_delta': -31.778201746665374,\n",
       " 'pitch_std_ratio': 0.7145598851783418,\n",
       " 'pitch_var_delta': -6065.914531822939,\n",
       " 'pitch_var_ratio': 0.5105958295060851,\n",
       " 'jitter_local_delta': -0.00284071110091114,\n",
       " 'jitter_local_ratio': 0.9075123334575352,\n",
       " 'shimmer_local_delta': -0.007452172042651736,\n",
       " 'shimmer_local_ratio': 0.9431111539035534,\n",
       " 'hnr_delta': 2.351955726083305,\n",
       " 'hnr_ratio': 1.3706880743316898}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc49174",
   "metadata": {},
   "source": [
    "# Send to GPT for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(audio_features, posture_features = None):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is a detailed snapshot of the speaker’s delivery — both baseline and full-clip — along with their changes. Use this to deliver personalized, context-aware feedback.\n",
    "\n",
    "## NOTE:\n",
    "- The **first {int(audio_features['baseline_duration'])} seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Interpret *changes* from baseline as signs of adaptation or stress — not necessarily flaws.\n",
    "- **Avoid quoting any raw values** in your response. Use intuitive, narrative insights only.\n",
    "- An 86% accurate ML model was used to rate the fluency of the speech, and that rating has also been provided to you.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📝 TRANSCRIPT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "<transcript>\n",
    "{audio_features['transcript']}\n",
    "</transcript>\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📏 BASELINE METRICS (First {int(audio_features['baseline_duration'])} seconds)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['baseline_fluency_rating']}\n",
    "- Words/sec: {audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['baseline_pitch_mean']:.2f} / {audio_features['baseline_pitch_std']:.2f} / {audio_features['baseline_pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['baseline_jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['baseline_shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['baseline_hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['baseline_rms_mean']:.2f} / {audio_features['baseline_rms_std']:.2f} / {audio_features['baseline_rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['baseline_zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['baseline_mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['baseline_delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📊 FULL CLIP METRICS\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['fluency_rating']}\n",
    "- Words/sec: {audio_features['speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['syllables_rate']:.2f}\n",
    "- Long pauses (>1s): {audio_features['long_pause_count']}\n",
    "- Total pause duration: {audio_features['long_pause_duration']:.2f} sec\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['pitch_mean']:.2f} / {audio_features['pitch_std']:.2f} / {audio_features['pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['rms_mean']:.2f} / {audio_features['rms_std']:.2f} / {audio_features['rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 RELATIVE CHANGES FROM BASELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Tempo & Fluency\n",
    "- Speaking rate ratio: {audio_features['speaking_rate'] / audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllable rate ratio: {audio_features['syllables_rate'] / audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Modulation\n",
    "- Pitch std delta: {audio_features['pitch_std_delta']:+.2f}\n",
    "- Jitter delta: {audio_features['jitter_local_delta']:+.3f}\n",
    "- Shimmer delta: {audio_features['shimmer_local_delta']:+.3f}\n",
    "- HNR delta: {audio_features['hnr_delta']:+.2f}\n",
    "\n",
    "## Energy\n",
    "- RMS mean delta: {audio_features['rms_mean_delta']:+.2f}\n",
    "- RMS std delta: {audio_features['rms_std_delta']:+.2f}\n",
    "- ZCR delta: {audio_features['zcr_delta']:+.3f}\n",
    "\n",
    "## Timbre\n",
    "- MFCC mean delta: {audio_features['mfcc_mean_delta']:+.2f}\n",
    "- Delta MFCC mean delta: {audio_features['delta_mean_delta']:+.6f}\n",
    "\n",
    "🧠 **Interpretation Tips** (for internal use only):\n",
    "- A **negative pitch_std_delta** might suggest monotony or nervousness; a positive value implies expressive modulation.\n",
    "- **Decreased RMS or HNR** may imply loss of vocal energy or confidence.\n",
    "- **Increased jitter/shimmer** may reflect stress or instability.\n",
    "- A **low syllable rate ratio** suggests slowing down relative to their natural pace, which may imply hesitation or deliberate pacing.\n",
    "- **ZCR changes** may reflect articulation style or clarity.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🧭 INSTRUCTIONS FOR FEEDBACK GENERATION\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Using the data above, write a highly personalized and supportive **narrative-style voice coaching paragraph**. Do not cite any specific numerical values. Your tone should be professional, encouraging, and practical.\n",
    "\n",
    "Structure your feedback in **three sections**:\n",
    "\n",
    "1. ✅ **What the speaker did well** — Highlight strengths or improvements in vocal control, energy, fluency, or confidence.\n",
    "2. 🛠️ **What they can improve** — Tactfully mention areas that deviated from their baseline and might affect clarity or delivery.\n",
    "3. 📊 **Confidence & fluency rating** — Conclude with your overall impression of their vocal confidence and fluency (e.g., low, moderate, high), based on relative metrics.\n",
    "\n",
    "DO NOT compare to average speakers. DO NOT be generic. Focus only on deviations from this speaker's own baseline and the emotional/functional impact of those changes.\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import AudioOnlyFeedback, FeedbackSchema\n",
    "\n",
    "def get_prompt_with_schema(audio_features, posture_features = None, schema = None):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is a detailed snapshot of the speaker’s delivery — both baseline and full-clip — along with their changes. Use this to deliver personalized, context-aware feedback.\n",
    "\n",
    "## NOTE:\n",
    "- The **first {int(audio_features['baseline_duration'])} seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Interpret *changes* from baseline as signs of adaptation or stress — not necessarily flaws.\n",
    "- **Avoid quoting any raw values** in your response. Use intuitive, narrative insights only.\n",
    "- An 86% accurate ML model was used to rate the fluency of the speech, and that rating has also been provided to you.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📝 TRANSCRIPT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "<transcript>\n",
    "{audio_features['transcript']}\n",
    "</transcript>\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📏 BASELINE METRICS (First {int(audio_features['baseline_duration'])} seconds)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['baseline_fluency_rating']}\n",
    "- Words/sec: {audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['baseline_pitch_mean']:.2f} / {audio_features['baseline_pitch_std']:.2f} / {audio_features['baseline_pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['baseline_jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['baseline_shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['baseline_hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['baseline_rms_mean']:.2f} / {audio_features['baseline_rms_std']:.2f} / {audio_features['baseline_rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['baseline_zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['baseline_mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['baseline_delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📊 FULL CLIP METRICS\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['fluency_rating']}\n",
    "- Words/sec: {audio_features['speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['syllables_rate']:.2f}\n",
    "- Long pauses (>1s): {audio_features['long_pause_count']}\n",
    "- Total pause duration: {audio_features['long_pause_duration']:.2f} sec\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['pitch_mean']:.2f} / {audio_features['pitch_std']:.2f} / {audio_features['pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['rms_mean']:.2f} / {audio_features['rms_std']:.2f} / {audio_features['rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 RELATIVE CHANGES FROM BASELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Tempo & Fluency\n",
    "- Speaking rate ratio: {audio_features['speaking_rate'] / audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllable rate ratio: {audio_features['syllables_rate'] / audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Modulation\n",
    "- Pitch std delta: {audio_features['pitch_std_delta']:+.2f}\n",
    "- Jitter delta: {audio_features['jitter_local_delta']:+.3f}\n",
    "- Shimmer delta: {audio_features['shimmer_local_delta']:+.3f}\n",
    "- HNR delta: {audio_features['hnr_delta']:+.2f}\n",
    "\n",
    "## Energy\n",
    "- RMS mean delta: {audio_features['rms_mean_delta']:+.2f}\n",
    "- RMS std delta: {audio_features['rms_std_delta']:+.2f}\n",
    "- ZCR delta: {audio_features['zcr_delta']:+.3f}\n",
    "\n",
    "## Timbre\n",
    "- MFCC mean delta: {audio_features['mfcc_mean_delta']:+.2f}\n",
    "- Delta MFCC mean delta: {audio_features['delta_mean_delta']:+.6f}\n",
    "\n",
    "🧠 **Interpretation Tips** (for internal use only):\n",
    "- A **negative pitch_std_delta** might suggest monotony or nervousness; a positive value implies expressive modulation.\n",
    "- **Decreased RMS or HNR** may imply loss of vocal energy or confidence.\n",
    "- **Increased jitter/shimmer** may reflect stress or instability.\n",
    "- A **low syllable rate ratio** suggests slowing down relative to their natural pace, which may imply hesitation or deliberate pacing.\n",
    "- **ZCR changes** may reflect articulation style or clarity.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🧭 INSTRUCTIONS FOR FEEDBACK GENERATION\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Using the data above, write a highly personalized and supportive **narrative-style voice coaching paragraph**. Do not cite any specific numerical values. Your tone should be professional, encouraging, and practical.\n",
    "\n",
    "Structure your feedback in **three sections**:\n",
    "\n",
    "1. ✅ **What the speaker did well** — Highlight strengths or improvements in vocal control, energy, fluency, or confidence.\n",
    "2. 🛠️ **What they can improve** — Tactfully mention areas that deviated from their baseline and might affect clarity or delivery.\n",
    "3. 📊 **Confidence & fluency rating** — Conclude with your overall impression of their vocal confidence and fluency (e.g., low, moderate, high), based on relative metrics.\n",
    "\n",
    "DO NOT compare to average speakers. DO NOT be generic. Focus only on deviations from this speaker's own baseline and the emotional/functional impact of those changes.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_feedback(audio_features, posture_features = None, response_schema = None, llm_model : str = \"llama-3.3-70b-versatile\"):\n",
    "    prompt = get_prompt(audio_features)\n",
    "\n",
    "    client = Groq()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_completion_tokens=32768,\n",
    "        top_p=1,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211bdbf",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e99f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_tokens(features): return len(encoder.encode(get_prompt(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0365e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.04 sec\n",
      "Memory Usage: 1.54 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.17 sec\n",
      "Memory Usage: 1.82 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.06 sec\n",
      "Memory Usage: 0.59 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.28 sec\n",
      "Memory Usage: 0.77 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1438"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An unconfident speech\n",
    "path = \"samples/unconfident.wav\"\n",
    "features = await extract_features(path)\n",
    "feedback = generate_feedback(features)\n",
    "\n",
    "get_n_tokens(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e792308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As you began speaking, it was clear that you took a moment to settle into your pace, and once you did, you showed a notable increase in your speaking rate, which suggests that you were able to find a rhythm that worked for you. Your voice modulation also became slightly more consistent, which is a great sign of adapting to the speaking environment. Additionally, your articulation showed some positive shifts, indicating an effort to enunciate clearly, which is commendable, especially given your self-described nervousness about public speaking.\n",
       "\n",
       "However, there were moments where your pitch became slightly more monotone, and your vocal energy, while consistent, could benefit from a bit more variation to add emphasis and keep the listener engaged. It's also worth noting that you had a brief pause, which, while not uncommon, might be an area to work on for smoother transitions between thoughts. Furthermore, your syllable rate, while increased, still reflects a careful and perhaps slightly hesitant pace, suggesting that you might be focusing intently on your words, which is understandable but could be balanced with a more natural flow.\n",
       "\n",
       "Overall, considering your fluency and confidence, I would say that you demonstrated a moderate level of vocal confidence and fluency. Your ability to adapt and find a comfortable pace is a significant strength, and with some practice on varying your pitch and energy, as well as working on smoother transitions, you could see noticeable improvements in your delivery. Remember, the key is not to compare yourself to others but to focus on your own growth and comfort with speaking, and it's clear that you have a good foundation to build upon."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a8ec9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.03 sec\n",
      "Memory Usage: 0.03 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.10 sec\n",
      "Memory Usage: 0.03 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.04 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.11 sec\n",
      "Memory Usage: 0.96 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A confident speech\n",
    "path = \"samples/confident.wav\"\n",
    "features = await extract_features(path)\n",
    "feedback = generate_feedback(features)\n",
    "\n",
    "get_n_tokens(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41201a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Hi, my name is Adkarsh Malaya. I'm a student and right now what I'm trying to do is I'm trying to get a model and I'm trying to use it to transcribe some filler words. I am very scared. I don't know what will happen and I really really hope this works.\",\n",
       " 'duration': 15.72,\n",
       " 'baseline_duration': 10.0,\n",
       " 'speaking_rate': 3.3078880407124682,\n",
       " 'syllables_rate': 4.134860050890585,\n",
       " 'baseline_speaking_rate': 3.3,\n",
       " 'baseline_syllables_rate': 4.1,\n",
       " 'long_pause_count': 0,\n",
       " 'long_pause_duration': 0,\n",
       " 'fluency_rating': 'Low',\n",
       " 'baseline_fluency_rating': 'Low',\n",
       " 'zcr': 0.08035611403023599,\n",
       " 'rms_mean': 0.0131712835,\n",
       " 'rms_std': 0.010488089,\n",
       " 'rms_var': 0.00011000002,\n",
       " 'mfcc': array([-467.24243   ,  134.46071   ,  -29.663124  ,   35.92738   ,\n",
       "           7.6379876 ,    9.713081  ,    9.715767  ,    2.3445828 ,\n",
       "         -13.212228  ,    0.7006626 ,   -0.62327105,  -11.099308  ,\n",
       "          -2.377678  ], dtype=float32),\n",
       " 'delta_mfcc': array([ 0.08744686,  0.09395339,  0.03829468,  0.00731569,  0.00382623,\n",
       "         0.00140616, -0.005586  , -0.00485631,  0.00310424,  0.00509462,\n",
       "        -0.00254051, -0.0089882 , -0.00698517], dtype=float32),\n",
       " 'mfcc_mean': -24.901373,\n",
       " 'delta_mean': 0.01626812,\n",
       " 'pitch_mean': 113.54379006181584,\n",
       " 'pitch_std': 31.072712613124782,\n",
       " 'pitch_var': 965.5134691378439,\n",
       " 'jitter_local': 0.02618617590765349,\n",
       " 'shimmer_local': 0.11475101152329432,\n",
       " 'hnr': 11.071314081880908,\n",
       " 'baseline_zcr': 0.07430711644431555,\n",
       " 'baseline_rms_mean': 0.013894684,\n",
       " 'baseline_rms_std': 0.01011108,\n",
       " 'baseline_rms_var': 0.000102233935,\n",
       " 'baseline_mfcc': array([-4.5408627e+02,  1.3814586e+02, -3.3332783e+01,  3.7128952e+01,\n",
       "         6.1656003e+00,  9.3982143e+00,  1.0864515e+01,  2.5044122e+00,\n",
       "        -1.4483069e+01,  7.4870068e-01, -5.4402858e-02, -1.1127012e+01,\n",
       "        -3.4484932e+00], dtype=float32),\n",
       " 'baseline_delta_mfcc': array([ 0.52117467,  0.1673277 ,  0.05842987,  0.19814922,  0.00387959,\n",
       "         0.0409784 ,  0.02039228,  0.01867066, -0.0131035 ,  0.00443509,\n",
       "         0.01996688,  0.00516739,  0.01024148], dtype=float32),\n",
       " 'baseline_mfcc_mean': -23.967363,\n",
       " 'baseline_delta_mean': 0.08120845,\n",
       " 'baseline_pitch_mean': 116.59827642848579,\n",
       " 'baseline_pitch_std': 30.8047851252861,\n",
       " 'baseline_pitch_var': 948.9347866150476,\n",
       " 'baseline_jitter_local': 0.0270972503045527,\n",
       " 'baseline_shimmer_local': 0.1268208564345333,\n",
       " 'baseline_hnr': 10.524540410055721,\n",
       " 'zcr_delta': 0.006048997585920438,\n",
       " 'zcr_ratio': 1.0814053602854237,\n",
       " 'rms_mean_delta': -0.00072340015,\n",
       " 'rms_mean_ratio': 0.9479369,\n",
       " 'rms_std_delta': 0.00037700962,\n",
       " 'rms_std_ratio': 1.0372868,\n",
       " 'rms_var_delta': 7.766088e-06,\n",
       " 'rms_var_ratio': 1.0759639,\n",
       " 'mfcc_mean_delta': -0.93400955,\n",
       " 'mfcc_mean_ratio': 1.0389701,\n",
       " 'delta_mean_delta': -0.06494033,\n",
       " 'delta_mean_ratio': 0.20032544,\n",
       " 'pitch_mean_delta': -3.0544863666699484,\n",
       " 'pitch_mean_ratio': 0.9738033317452734,\n",
       " 'pitch_std_delta': 0.2679274878386835,\n",
       " 'pitch_std_ratio': 1.0086975931417472,\n",
       " 'pitch_var_delta': 16.57868252279627,\n",
       " 'pitch_var_ratio': 1.0174708344099537,\n",
       " 'jitter_local_delta': -0.0009110743968992092,\n",
       " 'jitter_local_ratio': 0.9663776070760163,\n",
       " 'shimmer_local_delta': -0.012069844911238983,\n",
       " 'shimmer_local_ratio': 0.904827602883524,\n",
       " 'hnr_delta': 0.5467736718251874,\n",
       " 'hnr_ratio': 1.0519522611460326}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc7ec295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As you began your speech, it was clear that you had a good foundation to build upon, with a consistent pace that allowed your words to flow smoothly. Your voice modulation showed some expressive qualities, which is a great strength to leverage in your delivery. Notably, your ability to maintain a relatively steady energy level throughout your speech is commendable, as it suggests a good level of comfort with your material. \n",
       "\n",
       "However, there were moments where your pitch variation became slightly more pronounced, which may indicate a touch of nervousness or an attempt to add emphasis to certain points. Additionally, your articulation and timbre underwent some subtle shifts, potentially affecting the clarity of your message. It's also worth exploring how you can harness your natural syllable rate to enhance the overall flow of your speech, as there were instances where your pace was very slightly quicker than your baseline, which might be an adaptation to convey your ideas more urgently.\n",
       "\n",
       "Overall, your vocal confidence and fluency came across as moderate, considering the slight deviations from your baseline in pitch modulation and articulation. The fluency rating from the model also suggests that there's room for improvement in terms of smoothness and natural flow. Nonetheless, your speech demonstrated a clear and sincere attempt to communicate your thoughts, and with practice and attention to these areas, you have the potential to develop a more expressive and engaging delivery style."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40662727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 14.30%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.09 sec\n",
      "Memory Usage: 1.18 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.42 sec\n",
      "Memory Usage: -11.55 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 2.23 sec\n",
      "Memory Usage: 2.18 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 7.44 sec\n",
      "Memory Usage: 1.32 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "tim_urban_path = \"samples/tim-urban.wav\"\n",
    "tim_urban_features = await extract_features(tim_urban_path)\n",
    "tim_urban_feedback = generate_feedback(tim_urban_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ab0de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4172"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_tokens(tim_urban_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ddedf701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Reviewer.pxs. So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know, you get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light, and I'd bump it up in the middle months. And then at the end, I would kick it up into high gear, just like a little staircase. How hard can it be to just walk up the stairs? No big deal, right? But then the funniest thing happened. Those first few months, they came and went, and I couldn't quite do stuff. So we had an awesome new revised plan. And then ... But then those middle months actually went by, and I didn't really write words. And so we were here. And then two months turned into one month, which turned into two weeks. And one day I woke up, with three days until the deadline, still not having written a word, and so I did the only thing I could. I wrote 90 pages over 72 hours, pulling not one but two all-nighters. Humans are not supposed to pull two all-nighters. Sprinted across campus, dove in slow motion, and got it in just at the deadline. I thought that was the end of everything. But a week later, I get a call. It's the school. And they say, is this Tim Urban? And I say, yeah. And they say, we need to talk about your thesis. And I say, OK. And they say, it's the best one we've ever seen. That did not happen. It was a very, very bad thesis. I just wanted to enjoy that one moment when all of you thought, this guy is amazing! No, no, it was very, very bad. Anyway, today I'm a writer, blogger, guy. I write the blog Wait But Why. And a couple of years ago, I decided to write about procrastination. My behaviors always perplexed the non-procrastinators around me, and I wanted to explain to the non-procrastinators of the world what goes on in the heads of procrastinators and why we are the way we are. Now, I had a hypothesis that the brains of procrastinators were actually different than the brains of other people. And to test this, I found an MRI lab that actually let me scan both my brain and the brain of a proven non-procrastinator so I could compare them. And I actually brought them here to show you today, and I want you to take a look carefully to see if you can notice a difference. And I know that if you're not a trained brain expert, it's not that obvious, but just take a look, OK? So here's the brain of a non-procrastinator. Now, here's my brain. There is a difference. Both brains have a rational decision-maker in them, But the procrastinator's brain also has an instant gratification monkey. Now, what does this mean for the procrastinator? Well, it means everything's fine until this happens. So the rational decision-maker will make the rational decision to do something productive, but the monkey doesn't like that plan. So he actually takes the wheel and he says, actually, let's read the entire Wikipedia page of the Nancy Kerrigan-Tanya Harding scandal, because I just remembered that that happened. Then we're going to go over to the fridge, we're going to see if there's anything new in there since 10 minutes ago. After that, we're going to go on a YouTube spiral that starts with videos of Richard Feynman talking about magnets and ends much, much later with us watching interviews with Justin Bieber's mom. All of that's going to take a while, so we're not going to really have room on the schedule for any work today. Sorry. Now, what is going on here? The instant gratification monkey does not seem like a guy you want behind the wheel. He lives entirely in the present moment, he has no memory of the past, no knowledge of the future, He cares about two things, easy and fun. Now, in the animal world, that works fine. If you're a dog, and you spend your whole life doing nothing other than easy and fun things, you're a huge success. And to the monkey, humans are just another animal species. He has to keep well-slept, well-fed and propagating into the next generation, which in tribal times might have worked OK. But if you haven't noticed, now we're not in tribal times. We're in an advanced civilization, and the monkey does not know what that is. Which is why we have another guy in our brain, the rational decision-maker, who gives us the ability to do things no other animal can do. We can visualize the future, we can see the big picture, We can make long-term plans, and he wants to take all of that into account, and he wants to just have us do whatever makes sense to be doing right now. Now, sometimes it makes sense to be doing things that are easy and fun, like when you're having dinner or going to bed or enjoying well-earned leisure time. That's why there's an overlap. Sometimes they agree. But other times, it makes much more sense to be doing things that are harder and less pleasant for the sake of the big picture, and that's when we have a conflict. And for the procrastinator, that conflict tends to end a certain way every time, leaving him spending a lot of time in this orange zone, an easy and fun place that's entirely out of the make-sense circle. I call it the dark playground. Now, the dark playground is a place that all of you procrastinators out there know very well. It's where leisure activities happen at times at times when leisure activities are not supposed to be happening. The fun you have in the dark playground isn't actually fun because it's completely unearned, and the air is filled with guilt, dread, anxiety, self-hatred, all those good procrastinator feelings. And the question is, in this situation, with the monkey behind the wheel, how does the procrastinator ever get himself over here to this blue zone, a less pleasant place, but where really important things happen? Well, it turns out that the procrastinator has a guardian angel, someone who's always looking down on him and watching over him in his darkest moments, someone called the panic monster. I'm not sure. Now, the panic monster is dormant most of the time, but he suddenly wakes up any time a deadline gets too close or there's danger of public embarrassment, a career disaster or some other scary consequence. And importantly, he's the only thing that the monkey is terrified of. Now, he became very relevant in my life pretty recently because the people of TED reached out to me about six months ago and invited me to do a TED Talk. Now, of course, I said, yes, it's always been a dream of mine to have done a TED Talk in the past. But, But in the middle of all this excitement, the rational decision-maker seemed to have something else in his mind. He was saying, are we clear on what we just accepted? Do we get what's going to be now happening one day in the future? We need to sit down and work on this right now. And the monkey said, totally agree, but also let's just open Google Earth and let's zoom into the bottom of India, like 200 feet above the ground, and we're going to scroll up two and a half hours so we can get a better feel for India. So that's what we did that day. As six months turned into four, and then two, and then one, the people of TED decided to release the speakers. And I opened up the website, and there was my face staring right back at me, and guess who woke up? So the panic monster starts losing his mind, and a few seconds later, the whole system's in mayhem. And the monkey, who, remember, he's terrified of the panic monster, And finally, finally, the rational decision-maker can take the wheel and I can start working on the talk. Now, the panic monster explains all kinds of pretty insane, procrastinated behavior, like how someone like me could spend two weeks unable to start the opening sentence of a paper and then miraculously find the unbelievable work ethic to stay up all night and write eight pages. And this entire situation with the three characters, three characters, this is the procrastinator's system. It's not pretty, but in the end, it works. And this is what I decided to write about on the blog just a couple years ago. Now, when I did, I was amazed by the response. Literally thousands of emails came in from all different kinds of people, from all over the world, doing all different kinds of things. These were people who were nurses and bankers and painters and engineers and lots and lots of PhD students. And they were all writing, saying the same thing. I have this problem, too. But what struck me was the contrast between the light tone of the post and the heaviness of these emails. These people were writing with intense frustration about what procrastination had done for them, about what this monkey had done to them. And I thought about this, and I said, if the procrastinator system works, then what's going on? Why are all these people in such a dark place? Well, it turns out that there's two kinds of procrastination. Everything I've talked about today, the examples I've given, they all have deadlines. And when there's deadlines, the effects of procrastination are contained to the short term because the panic monster gets involved. There's a second kind of procrastination that happens in situations when there is no deadline. So if you want to have a career where you want to be a self-starter, something in the arts, something entrepreneurial, there's no deadlines on those things at first, because nothing's happening at first, not until you've gone out and done the hard work to get some momentum, to get things going. There's also all kinds of important things outside of your career that don't involve any deadlines, like seeing your family or exercising and taking care of your health, working on your relationship or getting out of a relationship that isn't working. Now, if the procrastinator's only mechanism of doing these hard things is the panic monster, that's a problem, because in all of these non-deadline situations, the panic monster doesn't show up, he has nothing to wake up for, they're not contained, they just extend outward forever. And it's this long-term kind of procrastination that's much less visible and much less talked about than the funnier short-term deadline-based kind. It's usually suffered quietly and privately, and it can be the source of a huge amount of long-term unhappiness and regrets. And I thought, you know, that's why these people are emailing, and that's why they're in such a bad place. It's not that they're cramming for some project. It's that long-term procrastination has made them feel like a spectator, at times, in their own lives. The frustration was not that they couldn't achieve their dreams, it's that they weren't even able to start chasing them. So I read these emails and I had a little bit of an epiphany that I don't think non-procrastinators exist. That's right, I think all of you are procrastinators. Now, you might not all be a mess, like some of us. And some of you may have a healthy relationship with deadlines. But remember, the monkey's sneakiest trick is when the deadlines aren't there. Now, I want to show you one last thing. I call this a life calendar. That's one box for every week of a 90-year life. That's not that many boxes, especially since we've already used a bunch of those. So I think we need to all take a long, hard look at that calendar. We need to think about what we're really procrastinating on, because everyone is procrastinating on something in life. We need to stay aware of the instant gratification monkey. That's a job for all of us. And because there's not that many boxes on there, it's a job that should probably start today. Well, maybe not today, but... You know, sometime soon. Thank you. Thank you.\",\n",
       " 'duration': 843.7500000000001,\n",
       " 'baseline_duration': 42.18833560090703,\n",
       " 'speaking_rate': 2.644148148148148,\n",
       " 'syllables_rate': 3.7037037037037033,\n",
       " 'baseline_speaking_rate': 30.88531418556332,\n",
       " 'baseline_syllables_rate': 42.92655716811602,\n",
       " 'long_pause_count': 28,\n",
       " 'long_pause_duration': 39.96154195011337,\n",
       " 'fluency_rating': 'Low',\n",
       " 'baseline_fluency_rating': 'Low',\n",
       " 'zcr': 0.12431602464380555,\n",
       " 'rms_mean': 0.070083246,\n",
       " 'rms_std': 0.061486296,\n",
       " 'rms_var': 0.0037805648,\n",
       " 'mfcc': array([-232.46727  ,  101.34686  ,  -19.867247 ,   15.719451 ,\n",
       "          -4.8433776,   -9.056608 ,  -11.200629 ,   -8.6175165,\n",
       "         -10.230278 ,   -1.2496375,   -5.5351243,   -2.8022056,\n",
       "          -4.6042967], dtype=float32),\n",
       " 'delta_mfcc': array([-1.4429400e-04,  2.3761578e-04,  3.8400068e-04, -1.1415667e-04,\n",
       "         6.7685498e-05,  1.4521193e-04,  1.3248599e-04,  1.1030027e-04,\n",
       "         1.4142298e-04,  1.9693849e-04,  1.6654511e-04,  1.4718014e-04,\n",
       "        -7.9398051e-05], dtype=float32),\n",
       " 'mfcc_mean': -14.877528,\n",
       " 'delta_mean': 0.000107041196,\n",
       " 'pitch_mean': 208.4127135401901,\n",
       " 'pitch_std': 79.55233694275556,\n",
       " 'pitch_var': 6328.574313053711,\n",
       " 'jitter_local': 0.027873774485195162,\n",
       " 'shimmer_local': 0.12354313818068587,\n",
       " 'hnr': 8.696793580183748,\n",
       " 'baseline_zcr': 0.11371094824917446,\n",
       " 'baseline_rms_mean': 0.06305937,\n",
       " 'baseline_rms_std': 0.058996927,\n",
       " 'baseline_rms_var': 0.0034806374,\n",
       " 'baseline_mfcc': array([-255.28789  ,  105.465546 ,  -15.627713 ,   17.496836 ,\n",
       "           1.7893533,   -4.4778514,   -5.8012633,   -6.3778076,\n",
       "          -4.190897 ,    3.2504325,   -2.3401265,    0.7608236,\n",
       "          -1.9429184], dtype=float32),\n",
       " 'baseline_delta_mfcc': array([ 0.23301336,  0.04249881, -0.01336857,  0.02402507,  0.00256586,\n",
       "        -0.01195078, -0.01759782, -0.01087944, -0.00485254,  0.00321395,\n",
       "        -0.00905112, -0.00292875, -0.00775515], dtype=float32),\n",
       " 'baseline_mfcc_mean': -12.86796,\n",
       " 'baseline_delta_mean': 0.017456377,\n",
       " 'baseline_pitch_mean': 202.56118723150064,\n",
       " 'baseline_pitch_std': 111.33053868942093,\n",
       " 'baseline_pitch_var': 12394.48884487665,\n",
       " 'baseline_jitter_local': 0.030714485586106302,\n",
       " 'baseline_shimmer_local': 0.1309953102233376,\n",
       " 'baseline_hnr': 6.344837854100443,\n",
       " 'zcr_delta': 0.010605076394631083,\n",
       " 'zcr_ratio': 1.0932634593055386,\n",
       " 'rms_mean_delta': 0.0070238784,\n",
       " 'rms_mean_ratio': 1.1113852,\n",
       " 'rms_std_delta': 0.0024893694,\n",
       " 'rms_std_ratio': 1.0421948,\n",
       " 'rms_var_delta': 0.0002999273,\n",
       " 'rms_var_ratio': 1.0861702,\n",
       " 'mfcc_mean_delta': -2.0095682,\n",
       " 'mfcc_mean_ratio': 1.1561683,\n",
       " 'delta_mean_delta': -0.017349336,\n",
       " 'delta_mean_ratio': 0.006131925,\n",
       " 'pitch_mean_delta': 5.851526308689472,\n",
       " 'pitch_mean_ratio': 1.0288876975331012,\n",
       " 'pitch_std_delta': -31.778201746665374,\n",
       " 'pitch_std_ratio': 0.7145598851783418,\n",
       " 'pitch_var_delta': -6065.914531822939,\n",
       " 'pitch_var_ratio': 0.5105958295060851,\n",
       " 'jitter_local_delta': -0.00284071110091114,\n",
       " 'jitter_local_ratio': 0.9075123334575352,\n",
       " 'shimmer_local_delta': -0.007452172042651736,\n",
       " 'shimmer_local_ratio': 0.9431111539035534,\n",
       " 'hnr_delta': 2.351955726083305,\n",
       " 'hnr_ratio': 1.3706880743316898}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tim_urban_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65042b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As I listened to your speech, I was struck by your ability to maintain a sense of humor and lightheartedness, even when discussing a topic as personal as procrastination. Your vocal control showed moments of strength, particularly in your ability to convey a sense of irony and self-deprecation, which helped to engage your audience. Your energy levels remained relatively consistent, which is commendable given the length of your speech. \n",
       "\n",
       "However, there were moments where your pace slowed down significantly, and your vocal modulation became less varied. This could be an indication of hesitation or a desire to emphasize certain points, but it also led to a sense of monotony in some sections. Additionally, your articulation and clarity were affected by a decrease in your syllable rate, which may have made it slightly more challenging for your audience to follow your train of thought. It's possible that you were deliberately pacing yourself to ensure your message was conveyed effectively, but being aware of these changes can help you strike a better balance between emphasis and flow.\n",
       "\n",
       "Overall, I would rate your vocal confidence and fluency as moderate. While you demonstrated a good sense of humor and audience awareness, there were moments where your delivery was affected by changes in your tempo and modulation. With some attention to these areas, you can work on maintaining a more consistent pace and varied tone, which will help to enhance your overall confidence and fluency. The fact that you were able to engage your audience and convey a meaningful message despite these challenges is a testament to your strengths as a speaker, and with some refinement, you can continue to grow and improve in your public speaking abilities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(tim_urban_feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b98539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60330ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
