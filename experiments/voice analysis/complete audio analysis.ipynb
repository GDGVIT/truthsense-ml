{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b5d9a",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25780097",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas librosa groq load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk tiktoken parselmouth pydub psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfa84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import asyncio\n",
    "import librosa\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "from pydub import AudioSegment\n",
    "from nltk.corpus import cmudict\n",
    "from parselmouth.praat import call\n",
    "from groq import Groq, AsyncClient\n",
    "from groq.types.audio import Transcription\n",
    "\n",
    "# Load environment file\n",
    "from load_dotenv import load_dotenv\n",
    "print(load_dotenv('../../.env.local'))\n",
    "\n",
    "assert os.environ.get('GROQ_API_KEY'), \"Groq API key not found in .env file, please set the key before starting this notebook\"\n",
    "\n",
    "# Global variables\n",
    "client = AsyncClient()\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "fluency_model = joblib.load('../fluency/weights/xgboost_model.pkl')\n",
    "\n",
    "try:\n",
    "    cmu_dict = cmudict.dict()\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('cmudict')\n",
    "    cmu_dict = cmudict.dict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f222a",
   "metadata": {},
   "source": [
    "## Monitor CPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import functools\n",
    "\n",
    "def monitor_resources(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process = psutil.Process(os.getpid())\n",
    "\n",
    "        # Get memory and CPU before\n",
    "        mem_before = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "        cpu_before = process.cpu_percent(interval=None)\n",
    "\n",
    "        # Start time and CPU\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get memory and CPU after\n",
    "        mem_after = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "        cpu_after = process.cpu_percent(interval=0.1)\n",
    "\n",
    "        # Get number of CPUs used\n",
    "        cpu_affinity = process.cpu_affinity()\n",
    "        \n",
    "        print(f\"Function: {func.__name__}\")\n",
    "        print(f\"Execution Time: {end_time - start_time:.2f} sec\")\n",
    "        print(f\"Memory Usage: {mem_after - mem_before:.2f} MB\")\n",
    "        print(f\"CPU Usage: {cpu_after:.2f}%\")\n",
    "        print(f\"CPU Cores Used: {cpu_affinity}\")\n",
    "\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def limit_to_one_core(core_id=0):\n",
    "    \"\"\"\n",
    "    Set process to run only on one CPU core (default: core 0).\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            process = psutil.Process(os.getpid())\n",
    "\n",
    "            # Store the current affinity to restore later\n",
    "            original_affinity = process.cpu_affinity()\n",
    "            \n",
    "            try:\n",
    "                # Set affinity to a single core\n",
    "                process.cpu_affinity([core_id])\n",
    "                print(f\"Running {func.__name__} on CPU core {core_id}\")\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                # Restore original affinity\n",
    "                process.cpu_affinity(original_affinity)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# # Limit NumPy, OpenBLAS etc to use only one CPU core\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c16201",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Features extracted:\n",
    "* ZCR\n",
    "* Pitch\n",
    "* Jitter\n",
    "* Shimmer\n",
    "* Harmonic-to-Noise ratio\n",
    "* RMS\n",
    "* MFCC\n",
    "* DeltaMFCC\n",
    "* SpeakingRate\n",
    "* PauseCount\n",
    "* PauseDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b64c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async Transcription\n",
    "def split_audio_in_memory(audio_path, max_mb=24):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    bytes_per_ms = (audio.frame_rate * audio.frame_width * audio.channels) / 1000\n",
    "    max_bytes = max_mb * 1024 * 1024\n",
    "    chunk_duration_ms = int(max_bytes / bytes_per_ms)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(audio), chunk_duration_ms):\n",
    "        chunk = audio[i:i+chunk_duration_ms]\n",
    "        buffer = io.BytesIO()\n",
    "        chunk.export(buffer, format=\"wav\")\n",
    "        buffer.seek(0)\n",
    "        chunks.append((f\"chunk_{i//chunk_duration_ms}.wav\", buffer))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "async def transcribe_chunk(filename, audio_buffer):\n",
    "    return await client.audio.transcriptions.create(\n",
    "        file=(filename, audio_buffer.read()),\n",
    "        model=\"distil-whisper-large-v3-en\",\n",
    "        response_format=\"verbose_json\",\n",
    "        timestamp_granularities=[\"word\"]\n",
    "    )\n",
    "\n",
    "\n",
    "async def transcribe_audio(audio_path, client=client):\n",
    "    \"\"\"Transcribe an audio file without saving the chunks to disk\"\"\"\n",
    "    chunks = split_audio_in_memory(audio_path)\n",
    "    tasks = [transcribe_chunk(name, buffer) for name, buffer in chunks]\n",
    "    all_transcripts = await asyncio.gather(*tasks)\n",
    "\n",
    "    transcript_parts = []\n",
    "    all_words = []\n",
    "    total_duration = 0.0\n",
    "\n",
    "    for chunk in all_transcripts:\n",
    "        transcript_parts.append(chunk.text)\n",
    "        all_words.extend(getattr(chunk, \"words\", []))\n",
    "        total_duration += chunk.duration\n",
    "\n",
    "    transcript = \"\".join(transcript_parts)\n",
    "    \n",
    "    return Transcription(text=transcript, words=all_words, duration=total_duration)\n",
    "\n",
    "\n",
    "transcript = await transcribe_audio(\"../../samples/confident.wav\")\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1898093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for calculating syllables speaking rate\n",
    "def get_word_syllable_count(word):\n",
    "    word = word.lower().strip(\".,?!;:\")\n",
    "    if word in cmu_dict:\n",
    "        return len([p for p in cmu_dict[word][0] if p[-1].isdigit()])\n",
    "    return max(1, len(re.findall(r'[aeiouy]+', word)))\n",
    "\n",
    "\n",
    "def estimate_syllable_rate(transcript, duration_sec):\n",
    "    words = transcript.split()\n",
    "    total_syllables = sum(get_word_syllable_count(word) for word in words)\n",
    "    return total_syllables / duration_sec if duration_sec > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958711f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pitch statistics, Jitter, Shimmer, and HNR ratio through Parselmouth\n",
    "@monitor_resources\n",
    "def extract_parselmouth_features(data, sr):\n",
    "    snd = parselmouth.Sound(values=data, sampling_frequency=sr)\n",
    "\n",
    "    pitch_obj = snd.to_pitch()\n",
    "    pitch_mean = call(pitch_obj, \"Get mean\", 0, 0, \"Hertz\")\n",
    "    pitch_std = call(pitch_obj, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "\n",
    "    point_process = call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "    jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    shimmer = call([snd, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "    harmonicity = call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "\n",
    "    return {\n",
    "        \"pitch_mean\": pitch_mean,\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"pitch_var\": pitch_std**2,\n",
    "        \"jitter_local\": jitter,\n",
    "        \"shimmer_local\": shimmer,\n",
    "        \"hnr\": hnr\n",
    "    }\n",
    "\n",
    "async def async_extract_parselmouth_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_parselmouth_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RMS Energy, ZCR, MFCC and Deltas using librosa\n",
    "@monitor_resources\n",
    "def extract_librosa_features(data, sr):\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(data))\n",
    "    \n",
    "    rms = librosa.feature.rms(y=data)[0]\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    rms_var = np.var(rms)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_mean = np.mean(mfcc)\n",
    "    delta_mean = np.mean(delta)\n",
    "\n",
    "    return {\n",
    "        \"zcr\": zcr,\n",
    "        \"rms_mean\": rms_mean,\n",
    "        \"rms_std\": rms_std,\n",
    "        \"rms_var\": rms_var,\n",
    "        \"mfcc\": mfcc.mean(axis=1),\n",
    "        \"delta_mfcc\": delta.mean(axis=1),\n",
    "        \"mfcc_mean\": mfcc_mean,\n",
    "        \"delta_mean\": delta_mean\n",
    "    }\n",
    "    \n",
    "async def async_extract_librosa_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_librosa_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_wave(data, sr):\n",
    "    return {\n",
    "        **extract_librosa_features(data, sr),\n",
    "        **extract_parselmouth_features(data, sr)\n",
    "    }\n",
    "    \n",
    "async def async_extract_features_from_wave(data, sr, executor):\n",
    "    # Start both tasks concurrently\n",
    "    librosa_task = asyncio.create_task(async_extract_librosa_features(data, sr, executor))\n",
    "    parselmouth_task = asyncio.create_task(async_extract_parselmouth_features(data, sr, executor))\n",
    "\n",
    "    # Wait for both\n",
    "    librosa_feats, parselmouth_feats = await asyncio.gather(librosa_task, parselmouth_task)\n",
    "\n",
    "    return {**librosa_feats, **parselmouth_feats}\n",
    "\n",
    "\n",
    "@monitor_resources\n",
    "async def extract_features(audio_path, baseline_duration: float = 0.0, fluency_model=fluency_model):\n",
    "    # -------------- Load the audio file --------------\n",
    "    data, sr = librosa.load(audio_path)\n",
    "    assert len(data) != 0, \"Your audio file appears to contain no content. Please input a valid file\"\n",
    "    \n",
    "    \n",
    "    # -------------- Get transcription and check minimum duration --------------\n",
    "    transcription_json = await transcribe_audio(audio_path)\n",
    "    duration_sec = transcription_json.duration # type: ignore\n",
    "    baseline_duration = baseline_duration or max(10.0, duration_sec * 0.05)\n",
    "\n",
    "    assert duration_sec != 0, \"File duration appears to be 0 after transcription?\"\n",
    "    \n",
    "    \n",
    "    # -------------- Get features of baseline and full wave --------------\n",
    "    baseline_data = data[:min(len(data), int(sr * baseline_duration))]\n",
    "    baseline_feats = extract_features_from_wave(baseline_data, sr)\n",
    "    full_feats = extract_features_from_wave(data, sr)\n",
    "\n",
    "\n",
    "    # -------------- Get fluency ratings --------------\n",
    "    features = ['zcr', 'pitch_mean', 'pitch_std', 'rms_mean', 'rms_std', 'rms_var', 'mfcc_mean', 'delta_mean']\n",
    "    rating_map = ['Low', 'Medium', 'High']\n",
    "        \n",
    "    baseline_fluency_features = np.array([baseline_feats[key] for key in baseline_feats if key in features])\n",
    "    full_fluency_features = np.array([full_feats[key] for key in full_feats if key in features])\n",
    "\n",
    "    res = fluency_model.predict(np.vstack((baseline_fluency_features, full_fluency_features)))\n",
    "    baseline_fluency = rating_map[res[0].argmax()]\n",
    "    full_fluency = rating_map[res[1].argmax()]\n",
    "\n",
    "    relative_feats = {}\n",
    "    for key in full_feats:\n",
    "        if key not in ['mfcc', 'delta_mfcc']:\n",
    "            base = baseline_feats.get(key, 0.0)\n",
    "            full = full_feats[key]\n",
    "            relative_feats[f'{key}_delta'] = full - base\n",
    "    \n",
    "    \n",
    "    # -------------- Get speaking rates --------------\n",
    "    # Assuming the transcript has come by now\n",
    "\n",
    "    # Baseline speaking rate\n",
    "    baseline_transcript = [word_segment['word'] for word_segment in transcription_json.words if word_segment['start'] <= baseline_duration]  # type: ignore\n",
    "    baseline_word_count = len(baseline_transcript)\n",
    "    baseline_transcript = \" \".join(baseline_transcript)\n",
    "    baseline_speaking_rate = baseline_word_count / baseline_duration\n",
    "    baseline_syllables_rate = estimate_syllable_rate(baseline_transcript, baseline_duration)\n",
    "\n",
    "    # Full data speaking rate\n",
    "    transcript = transcription_json.text\n",
    "    word_count = len(transcript.split())\n",
    "    speaking_rate = word_count / duration_sec\n",
    "    syllables_rate = estimate_syllable_rate(transcript, duration_sec)\n",
    "        \n",
    "    \n",
    "    # -------------- Pause detection --------------\n",
    "    intervals = librosa.effects.split(data, top_db=30)\n",
    "    pauses = [(intervals[i][0] - intervals[i - 1][1]) / sr\n",
    "              for i in range(1, len(intervals))\n",
    "              if (intervals[i][0] - intervals[i - 1][1]) / sr > 1.0]\n",
    "    \n",
    "    long_pause_count = len(pauses)\n",
    "    long_pause_total = sum(pauses)\n",
    "\n",
    "    return {\n",
    "        \"transcript\": transcript,\n",
    "        \"duration\": duration_sec,\n",
    "        \"baseline_duration\": baseline_duration,\n",
    "        \"speaking_rate\": speaking_rate,\n",
    "        \"syllables_rate\": syllables_rate,\n",
    "        \"baseline_speaking_rate\": baseline_speaking_rate,\n",
    "        \"baseline_syllables_rate\": baseline_syllables_rate,\n",
    "        \"long_pause_count\": long_pause_count,\n",
    "        \"long_pause_duration\": long_pause_total,\n",
    "        \"fluency_rating\": full_fluency,\n",
    "        \"baseline_fluency_rating\": baseline_fluency,\n",
    "        **full_feats,\n",
    "        **{f'baseline_{k}': v for k, v in baseline_feats.items()},\n",
    "        **relative_feats,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = await extract_features('../../samples/tim-urban.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc49174",
   "metadata": {},
   "source": [
    "# Send to GPT for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(audio_features, posture_features = None):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is a detailed snapshot of the speaker’s delivery — both baseline and full-clip — along with their changes. Use this to deliver personalized, context-aware feedback.\n",
    "\n",
    "## NOTE:\n",
    "- The **first {int(audio_features['baseline_duration'])} seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Interpret *changes* from baseline as signs of adaptation or stress — not necessarily flaws.\n",
    "- **Avoid quoting any raw values** in your response. Use intuitive, narrative insights only.\n",
    "- An 86% accurate ML model was used to rate the fluency of the speech, and that rating has also been provided to you.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📝 TRANSCRIPT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "<transcript>\n",
    "{audio_features['transcript']}\n",
    "</transcript>\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📏 BASELINE METRICS (First {int(audio_features['baseline_duration'])} seconds)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['baseline_fluency_rating']}\n",
    "- Words/sec: {audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['baseline_pitch_mean']:.2f} / {audio_features['baseline_pitch_std']:.2f} / {audio_features['baseline_pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['baseline_jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['baseline_shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['baseline_hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['baseline_rms_mean']:.2f} / {audio_features['baseline_rms_std']:.2f} / {audio_features['baseline_rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['baseline_zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['baseline_mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['baseline_delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📊 FULL CLIP METRICS\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['fluency_rating']}\n",
    "- Words/sec: {audio_features['speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['syllables_rate']:.2f}\n",
    "- Long pauses (>1s): {audio_features['long_pause_count']}\n",
    "- Total pause duration: {audio_features['long_pause_duration']:.2f} sec\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['pitch_mean']:.2f} / {audio_features['pitch_std']:.2f} / {audio_features['pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['rms_mean']:.2f} / {audio_features['rms_std']:.2f} / {audio_features['rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 RELATIVE CHANGES FROM BASELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Tempo & Fluency\n",
    "- Speaking rate ratio: {audio_features['speaking_rate'] / audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllable rate ratio: {audio_features['syllables_rate'] / audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Modulation\n",
    "- Pitch std delta: {audio_features['pitch_std_delta']:+.2f}\n",
    "- Jitter delta: {audio_features['jitter_local_delta']:+.3f}\n",
    "- Shimmer delta: {audio_features['shimmer_local_delta']:+.3f}\n",
    "- HNR delta: {audio_features['hnr_delta']:+.2f}\n",
    "\n",
    "## Energy\n",
    "- RMS mean delta: {audio_features['rms_mean_delta']:+.2f}\n",
    "- RMS std delta: {audio_features['rms_std_delta']:+.2f}\n",
    "- ZCR delta: {audio_features['zcr_delta']:+.3f}\n",
    "\n",
    "## Timbre\n",
    "- MFCC mean delta: {audio_features['mfcc_mean_delta']:+.2f}\n",
    "- Delta MFCC mean delta: {audio_features['delta_mean_delta']:+.6f}\n",
    "\n",
    "🧠 **Interpretation Tips** (for internal use only):\n",
    "- A **negative pitch_std_delta** might suggest monotony or nervousness; a positive value implies expressive modulation.\n",
    "- **Decreased RMS or HNR** may imply loss of vocal energy or confidence.\n",
    "- **Increased jitter/shimmer** may reflect stress or instability.\n",
    "- A **low syllable rate ratio** suggests slowing down relative to their natural pace, which may imply hesitation or deliberate pacing.\n",
    "- **ZCR changes** may reflect articulation style or clarity.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🧭 INSTRUCTIONS FOR FEEDBACK GENERATION\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Using the data above, write a highly personalized and supportive **narrative-style voice coaching paragraph**. Do not cite any specific numerical values. Your tone should be professional, encouraging, and practical.\n",
    "\n",
    "Structure your feedback in **three sections**:\n",
    "\n",
    "1. ✅ **What the speaker did well** — Highlight strengths or improvements in vocal control, energy, fluency, or confidence.\n",
    "2. 🛠️ **What they can improve** — Tactfully mention areas that deviated from their baseline and might affect clarity or delivery.\n",
    "3. 📊 **Confidence & fluency rating** — Conclude with your overall impression of their vocal confidence and fluency (e.g., low, moderate, high), based on relative metrics.\n",
    "\n",
    "DO NOT compare to average speakers. DO NOT be generic. Focus only on deviations from this speaker's own baseline and the emotional/functional impact of those changes.\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import AudioOnlyFeedback, FeedbackSchema\n",
    "\n",
    "def get_prompt_with_schema(audio_features, posture_features = None, schema = None):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is a detailed snapshot of the speaker’s delivery — both baseline and full-clip — along with their changes. Use this to deliver personalized, context-aware feedback.\n",
    "\n",
    "## NOTE:\n",
    "- The **first {int(audio_features['baseline_duration'])} seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Interpret *changes* from baseline as signs of adaptation or stress — not necessarily flaws.\n",
    "- **Avoid quoting any raw values** in your response. Use intuitive, narrative insights only.\n",
    "- An 86% accurate ML model was used to rate the fluency of the speech, and that rating has also been provided to you.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📝 TRANSCRIPT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "<transcript>\n",
    "{audio_features['transcript']}\n",
    "</transcript>\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📏 BASELINE METRICS (First {int(audio_features['baseline_duration'])} seconds)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['baseline_fluency_rating']}\n",
    "- Words/sec: {audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['baseline_pitch_mean']:.2f} / {audio_features['baseline_pitch_std']:.2f} / {audio_features['baseline_pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['baseline_jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['baseline_shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['baseline_hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['baseline_rms_mean']:.2f} / {audio_features['baseline_rms_std']:.2f} / {audio_features['baseline_rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['baseline_zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['baseline_mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['baseline_delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📊 FULL CLIP METRICS\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {audio_features['fluency_rating']}\n",
    "- Words/sec: {audio_features['speaking_rate']:.2f}\n",
    "- Syllables/sec: {audio_features['syllables_rate']:.2f}\n",
    "- Long pauses (>1s): {audio_features['long_pause_count']}\n",
    "- Total pause duration: {audio_features['long_pause_duration']:.2f} sec\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {audio_features['pitch_mean']:.2f} / {audio_features['pitch_std']:.2f} / {audio_features['pitch_var']:.2f}\n",
    "- Jitter (local): {audio_features['jitter_local']:.3f}\n",
    "- Shimmer (local): {audio_features['shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {audio_features['hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {audio_features['rms_mean']:.2f} / {audio_features['rms_std']:.2f} / {audio_features['rms_var']:.2f}\n",
    "- Zero Crossing Rate: {audio_features['zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {audio_features['mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {audio_features['delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 RELATIVE CHANGES FROM BASELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Tempo & Fluency\n",
    "- Speaking rate ratio: {audio_features['speaking_rate'] / audio_features['baseline_speaking_rate']:.2f}\n",
    "- Syllable rate ratio: {audio_features['syllables_rate'] / audio_features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Modulation\n",
    "- Pitch std delta: {audio_features['pitch_std_delta']:+.2f}\n",
    "- Jitter delta: {audio_features['jitter_local_delta']:+.3f}\n",
    "- Shimmer delta: {audio_features['shimmer_local_delta']:+.3f}\n",
    "- HNR delta: {audio_features['hnr_delta']:+.2f}\n",
    "\n",
    "## Energy\n",
    "- RMS mean delta: {audio_features['rms_mean_delta']:+.2f}\n",
    "- RMS std delta: {audio_features['rms_std_delta']:+.2f}\n",
    "- ZCR delta: {audio_features['zcr_delta']:+.3f}\n",
    "\n",
    "## Timbre\n",
    "- MFCC mean delta: {audio_features['mfcc_mean_delta']:+.2f}\n",
    "- Delta MFCC mean delta: {audio_features['delta_mean_delta']:+.6f}\n",
    "\n",
    "🧠 **Interpretation Tips** (for internal use only):\n",
    "- A **negative pitch_std_delta** might suggest monotony or nervousness; a positive value implies expressive modulation.\n",
    "- **Decreased RMS or HNR** may imply loss of vocal energy or confidence.\n",
    "- **Increased jitter/shimmer** may reflect stress or instability.\n",
    "- A **low syllable rate ratio** suggests slowing down relative to their natural pace, which may imply hesitation or deliberate pacing.\n",
    "- **ZCR changes** may reflect articulation style or clarity.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🧭 INSTRUCTIONS FOR FEEDBACK GENERATION\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Using the data above, write a highly personalized and supportive **narrative-style voice coaching paragraph**. Do not cite any specific numerical values. Your tone should be professional, encouraging, and practical.\n",
    "\n",
    "Structure your feedback in **three sections**:\n",
    "\n",
    "1. ✅ **What the speaker did well** — Highlight strengths or improvements in vocal control, energy, fluency, or confidence.\n",
    "2. 🛠️ **What they can improve** — Tactfully mention areas that deviated from their baseline and might affect clarity or delivery.\n",
    "3. 📊 **Confidence & fluency rating** — Conclude with your overall impression of their vocal confidence and fluency (e.g., low, moderate, high), based on relative metrics.\n",
    "\n",
    "DO NOT compare to average speakers. DO NOT be generic. Focus only on deviations from this speaker's own baseline and the emotional/functional impact of those changes.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_feedback(audio_features, posture_features = None, response_schema = None, llm_model : str = \"llama-3.3-70b-versatile\"):\n",
    "    prompt = get_prompt(audio_features)\n",
    "\n",
    "    client = Groq()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_completion_tokens=32768,\n",
    "        top_p=1,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211bdbf",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e99f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_tokens(features): return len(encoder.encode(get_prompt(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An unconfident speech\n",
    "path = \"../../samples/unconfident.wav\"\n",
    "features = await extract_features(path)\n",
    "feedback = generate_feedback(features)\n",
    "\n",
    "get_n_tokens(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e792308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ec9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confident speech\n",
    "path = \"../../samples/confident.wav\"\n",
    "features = await extract_features(path)\n",
    "feedback = generate_feedback(features)\n",
    "\n",
    "get_n_tokens(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41201a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ec295",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40662727",
   "metadata": {},
   "outputs": [],
   "source": [
    "tim_urban_path = \"../../samples/tim-urban.wav\"\n",
    "tim_urban_features = await extract_features(tim_urban_path)\n",
    "tim_urban_feedback = generate_feedback(tim_urban_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_tokens(tim_urban_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedf701",
   "metadata": {},
   "outputs": [],
   "source": [
    "tim_urban_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65042b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(tim_urban_feedback.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
