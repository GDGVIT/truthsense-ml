{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b5d9a",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25780097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: torch in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: groq in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: load_dotenv in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (4.14.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from load_dotenv) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from standard-aifc->librosa) (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas librosa torch groq load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771b07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tiktoken in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pytubefix in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (9.2.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken pytubefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91bfa84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import librosa\n",
    "from groq import Groq\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Load environment file\n",
    "from load_dotenv import load_dotenv\n",
    "print(load_dotenv('.env.local'))\n",
    "\n",
    "assert os.environ.get('GROQ_API_KEY'), \"Groq API key not found in .env file, please set the key before starting this notebook\"\n",
    "\n",
    "# Global variables\n",
    "client = Groq()\n",
    "\n",
    "try:\n",
    "    cmu_dict = cmudict.dict()\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('cmudict')\n",
    "    cmu_dict = cmudict.dict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c16201",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Features extracted:\n",
    "\t•\tZCR\n",
    "\t•\tPitch\n",
    "\t•\tRMS\n",
    "\t•\tMFCC\n",
    "\t•\tDeltaMFCC\n",
    "\t•\tSpeakingRate\n",
    "\t•\tPauseCount\n",
    "\t•\tPauseDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "958711f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, client=client):\n",
    "    with open(audio_path, \"rb\") as file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "        file=(audio_path, file.read()),\n",
    "        model=\"distil-whisper-large-v3-en\",\n",
    "        response_format=\"verbose_json\",\n",
    "        timestamp_granularities=['word', 'segment']\n",
    "        )\n",
    "\n",
    "    return transcription\n",
    "\n",
    "\n",
    "def get_word_syllable_count(word):\n",
    "    word = word.lower().strip(\".,?!;:\")\n",
    "    if word in cmu_dict:\n",
    "        return len([p for p in cmu_dict[word][0] if p[-1].isdigit()])\n",
    "    return max(1, len(re.findall(r'[aeiouy]+', word)))\n",
    "\n",
    "\n",
    "def estimate_syllable_rate(transcript, duration_sec):\n",
    "    words = transcript.split()\n",
    "    total_syllables = sum(get_word_syllable_count(word) for word in words)\n",
    "    return total_syllables / duration_sec if duration_sec > 0 else 0\n",
    "\n",
    "\n",
    "def extract_features_from_wave(data, sr, prefix: str = \"\"):\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(data))\n",
    "\n",
    "    pitch = librosa.yin(data, fmin=librosa.note_to_hz(\"C2\"), fmax=librosa.note_to_hz(\"C7\"), sr=sr)\n",
    "    pitch = np.nan_to_num(pitch)\n",
    "    pitch_mean = np.mean(pitch)\n",
    "    pitch_std = np.std(pitch)\n",
    "    pitch_var = np.var(pitch)\n",
    "\n",
    "    rms = librosa.feature.rms(y=data)[0]\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    rms_var = np.var(rms)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_mean = np.mean(mfcc)\n",
    "    delta_mean = np.mean(delta)\n",
    "\n",
    "    return {\n",
    "        \"zcr\": zcr,\n",
    "        \"pitch_mean\": pitch_mean,\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"pitch_var\": pitch_var,\n",
    "        \"rms_mean\": rms_mean,\n",
    "        \"rms_std\": rms_std,\n",
    "        \"rms_var\": rms_var,\n",
    "        \"mfcc_mean\": mfcc_mean,\n",
    "        \"delta_mean\": delta_mean\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_features(audio_path, baseline_duration=10.0):\n",
    "    data, sr = librosa.load(audio_path)\n",
    "\n",
    "    assert len(data) != 0, \"Your audio file appears to contain no content. Please input a valid file\"\n",
    "    assert baseline_duration != 0, \"Baseline cannot be 0!\"\n",
    "    assert baseline_duration < len(data) / sr, \"Baseline cannot be greater than the length of the audio file input\"\n",
    "    \n",
    "    # Baseline from first few seconds\n",
    "    baseline_data = data[:min(len(data), int(sr * baseline_duration))]\n",
    "    baseline_feats = extract_features_from_wave(baseline_data, sr)\n",
    "    full_feats = extract_features_from_wave(data, sr)\n",
    "\n",
    "    relative_feats = {}\n",
    "    for key in full_feats:\n",
    "        base = baseline_feats.get(key, 0.0)\n",
    "        full = full_feats[key]\n",
    "        relative_feats[f'{key}_delta'] = full - base\n",
    "        relative_feats[f'{key}_ratio'] = full / base if base != 0 else 0\n",
    "\n",
    "    # Transcription and Speaking Rates\n",
    "    transcription_json = transcribe_audio(audio_path)\n",
    "    duration_sec = transcription_json.duration # type: ignore\n",
    "    baseline_duration = max(10.0, duration_sec * 0.05)\n",
    "    print(baseline_duration)\n",
    "\n",
    "    assert duration_sec != 0, \"File duration appears to be 0 after transcription?\"\n",
    "    \n",
    "    # Full data speaking rate\n",
    "    transcript = transcription_json.text\n",
    "    word_count = len(transcript.split())\n",
    "    speaking_rate = word_count / duration_sec\n",
    "    syllables_rate = estimate_syllable_rate(transcript, duration_sec)\n",
    "    \n",
    "    # Baseline speaking rate\n",
    "    baseline_transcript = [word_segment['word'] for word_segment in transcription_json.words if word_segment['start'] <= baseline_duration]  # type: ignore\n",
    "    baseline_word_count = len(baseline_transcript)\n",
    "    baseline_transcript = \" \".join(baseline_transcript)\n",
    "    baseline_speaking_rate = baseline_word_count / baseline_duration\n",
    "    baseline_syllables_rate = estimate_syllable_rate(baseline_transcript, baseline_duration)\n",
    "    \n",
    "    # Pause detection\n",
    "    intervals = librosa.effects.split(data, top_db=30)\n",
    "    pauses = [(intervals[i][0] - intervals[i - 1][1]) / sr\n",
    "              for i in range(1, len(intervals))\n",
    "              if (intervals[i][0] - intervals[i - 1][1]) / sr > 1.0]\n",
    "    \n",
    "    long_pause_count = len(pauses)\n",
    "    long_pause_total = sum(pauses)\n",
    "\n",
    "    return {\n",
    "        \"transcript\": transcript,\n",
    "        \"duration\": duration_sec,\n",
    "        \"baseline_duration\": baseline_duration,\n",
    "        \"speaking_rate\": speaking_rate,\n",
    "        \"syllables_rate\": syllables_rate,\n",
    "        \"baseline_speaking_rate\": baseline_speaking_rate,\n",
    "        \"baseline_syllables_rate\": baseline_syllables_rate,\n",
    "        \"long_pause_count\": long_pause_count,\n",
    "        \"long_pause_duration\": long_pause_total,\n",
    "        **full_feats,\n",
    "        **{f'baseline_{k}': v for k, v in baseline_feats.items()},\n",
    "        **relative_feats,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05c6d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\AppData\\Local\\Temp\\ipykernel_28780\\1006254953.py:59: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, sr = librosa.load(audio_path)\n",
      "c:\\Users\\Utkarsh\\OneDrive\\Documents\\GitHub\\truthsense-ml\\venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "features = extract_features('samples/unconfident.m4a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc49174",
   "metadata": {},
   "source": [
    "# Send to GPT for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751d8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(features):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is an in-depth description of the speech signal, including baseline characteristics, absolute values, and relative shifts.\n",
    "\n",
    "## NOTE:\n",
    "- The **first 10 seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Your feedback should interpret the *changes* from baseline — not just absolute values — as indicators of intentional modulation or stress, not necessarily flaws.\n",
    "\n",
    "## TRANSCRIPT\n",
    "<transcript> \n",
    "{features['transcript']} \n",
    "</transcript>\n",
    "\n",
    "## BASELINE METRICS\n",
    "- Speaking rate: {features['baseline_speaking_rate']:.2f} words/sec\n",
    "- Speaking rate: {features['baseline_syllables_rate']:.2f} syllables/sec\n",
    "- Pitch (Mean; Standard deviation; Variation): {features['baseline_pitch_mean']:.2f}; {features['baseline_pitch_std']:.2f}; {features['baseline_pitch_var']:.2f}\n",
    "- RMS Energy (Mean; Standard deviation; Variation): {features['baseline_rms_mean']:.2f}; {features['baseline_rms_std']:.2f}; {features['baseline_rms_var']:.2f}\n",
    "- ZCR: {features['baseline_zcr']:.2f}\n",
    "- MFCC and Delta MFCC Mean: {features['baseline_mfcc_mean']:.2f}; {features['baseline_delta_mean']:.2f}\n",
    "\n",
    "## RAW METRICS (FOR THE WHOLE SPEECH)\n",
    "- Speaking Rate: {features['speaking_rate']:.2f} words/sec\n",
    "- Speaking rate: {features['baseline_syllables_rate']:.2f} syllables/sec\n",
    "- Long Pauses: {features['long_pause_count']} (>1s)\n",
    "- Total Long Pause Duration: {features['long_pause_duration']:.2f} sec\n",
    "- Pitch (Mean; Standard deviation; Variation): {features['pitch_mean']:.2f}; {features['pitch_std']:.2f}; {features['pitch_var']:.2f}\n",
    "- RMS Energy (Mean; Standard deviation; Variation): {features['rms_mean']:.2f}; {features['rms_std']:.2f}; {features['rms_var']:.2f}\n",
    "- ZCR: {features['zcr']:.2f}\n",
    "- MFCC and Delta MFCC Mean: {features['mfcc_mean']:.2f}; {features['delta_mean']:.2f}\n",
    "\n",
    "## RELATIVE CHANGES FROM BASELINE\n",
    "- Pitch variation change (std): {features['pitch_std_delta']:+.2f}\n",
    "- RMS Energy mean change: {features['rms_mean_delta']:+.2f}\n",
    "- Speaking rate ratio: {features['speaking_rate'] / features['baseline_speaking_rate']}\n",
    "- Interpretation Tip:\n",
    "    - A Pitch variation change > 0 may suggest more modulation than usual; < 0 may suggest flattening.\n",
    "    - RMS mean delta > 0 = more vocal energy than the beginning few seconds.\n",
    "    - Speaking rate ratio < 1 = speaker slowed down as compared to the start of their speech.\n",
    "    NOTE: This tip should not be used as an absolute, a speaking rate slowing could mean anxiety as well, infer that from the script\n",
    "\n",
    "## INSTRUCTION\n",
    "\n",
    "Now, based on this input, write a narrative-style feedback giving clear, constructive, and context-aware feedback. \n",
    "\n",
    "DO NOT judge the speaker based on universal norms; instead, use their own baseline as reference to detect signs of:\n",
    "- Increased or decreased vocal control,\n",
    "- Confidence shifts,\n",
    "- Monotony vs. modulation,\n",
    "- Hesitation or fluency issues.\n",
    "\n",
    "You are a closed source model. So you are expected not to reference any specific acoustic features and their values in your feedback.\n",
    "\n",
    "Split your feedback in 3 parts: What they did correctly, what they could improve on, and rate their confidence and fluency levels based on the relative metrics.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_feedback(features):\n",
    "    prompt = get_prompt(features)\n",
    "\n",
    "    client = Groq()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_completion_tokens=32768,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31321419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tiktoken in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7928aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "787"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "len(encoder.encode(get_prompt(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211bdbf",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0365e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\AppData\\Local\\Temp\\ipykernel_28780\\1006254953.py:59: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, sr = librosa.load(audio_path)\n",
      "c:\\Users\\Utkarsh\\OneDrive\\Documents\\GitHub\\truthsense-ml\\venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# An unconfident speech\n",
    "path = \"samples/unconfident.m4a\"\n",
    "features = extract_features(path)\n",
    "feedback = generate_feedback(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e792308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Part 1: Strengths\n",
       "\n",
       "The speaker demonstrates a notable ability to convey their emotions and thoughts, despite admitting to a fear of public speaking. Their speech is filled with personal touches, such as expressing their fear, which shows a level of vulnerability and authenticity. This ability to connect with the audience on a personal level is a significant strength. Additionally, the speaker's attempts to articulate their goals, such as trying to find a model to recognize filler words, indicate a clear direction and purpose in their speech. These aspects suggest that the speaker has a good foundation in terms of content and personal connection, which are crucial elements of effective public speaking.\n",
       "\n",
       "## Part 2: Areas for Improvement\n",
       "\n",
       "There are areas where the speaker could enhance their delivery to improve the overall impact of their speech. One noticeable aspect is the speaker's tendency to hesitate, as evidenced by the frequent use of filler words. This could be an indication of nervousness or a lack of confidence in their speaking abilities. Furthermore, the speaker's vocal modulation could be more varied, as there are moments where the speech sounds somewhat flat or monotonous. Working on varying the tone and pitch could help keep the audience engaged and interested in the message being conveyed. It's also worth noting that the speaker's pace could be more consistent, as there are instances where the speech slows down, potentially due to anxiety or a lack of fluency.\n",
       "\n",
       "## Part 3: Confidence and Fluency Assessment\n",
       "\n",
       "Based on the relative changes from the speaker's baseline, it appears that there are shifts in confidence and fluency throughout the speech. The speaker's confidence level seems to waver, particularly when discussing their fear of public speaking, which is a sensitive topic for them. This emotional vulnerability, while commendable, also reveals a degree of nervousness that affects their fluency. The speaker's fluency level is somewhat impacted by their hesitation and the use of filler words, which disrupts the smooth flow of their speech. On a scale of 1 to 10, with 1 being the lowest and 10 being the highest, I would rate the speaker's confidence level at around 6, as they do show moments of clarity and purpose but are also clearly held back by their fear. Their fluency level would be around 5, as the frequent filler words and hesitations detract from the overall smoothness of their delivery. With practice and focusing on building confidence, the speaker has the potential to significantly improve both their confidence and fluency levels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8ec9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\AppData\\Local\\Temp\\ipykernel_28780\\1006254953.py:59: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, sr = librosa.load(audio_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# A confident speech\n",
    "path = \"samples/confident.m4a\"\n",
    "features = extract_features(path)\n",
    "feedback = generate_feedback(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41201a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Hi, my name is Adkarsh Malaya. I'm a student and right now what I'm trying to do is I'm trying to get a model and I'm trying to use it to transcribe some filler words. I am very scared. I don't know what will happen and I really really hope this works.\",\n",
       " 'duration': 15.72,\n",
       " 'baseline_duration': 10.0,\n",
       " 'speaking_rate': 3.3078880407124682,\n",
       " 'syllables_rate': 4.134860050890585,\n",
       " 'baseline_speaking_rate': 3.3,\n",
       " 'baseline_syllables_rate': 4.1,\n",
       " 'long_pause_count': 0,\n",
       " 'long_pause_duration': 0,\n",
       " 'zcr': np.float64(0.08035611403023599),\n",
       " 'pitch_mean': np.float64(287.1815062705668),\n",
       " 'pitch_std': np.float64(466.33901796330355),\n",
       " 'pitch_var': np.float64(217472.07967497836),\n",
       " 'rms_mean': np.float32(0.0131712835),\n",
       " 'rms_std': np.float32(0.010488089),\n",
       " 'rms_var': np.float32(0.00011000002),\n",
       " 'mfcc_mean': np.float32(-24.901373),\n",
       " 'delta_mean': np.float32(0.016268123),\n",
       " 'baseline_zcr': np.float64(0.07430711644431555),\n",
       " 'baseline_pitch_mean': np.float64(272.55173088127896),\n",
       " 'baseline_pitch_std': np.float64(445.4344329724631),\n",
       " 'baseline_pitch_var': np.float64(198411.83407749975),\n",
       " 'baseline_rms_mean': np.float32(0.013894684),\n",
       " 'baseline_rms_std': np.float32(0.01011108),\n",
       " 'baseline_rms_var': np.float32(0.000102233935),\n",
       " 'baseline_mfcc_mean': np.float32(-23.967363),\n",
       " 'baseline_delta_mean': np.float32(0.08120846),\n",
       " 'zcr_delta': np.float64(0.006048997585920438),\n",
       " 'zcr_ratio': np.float64(1.0814053602854237),\n",
       " 'pitch_mean_delta': np.float64(14.629775389287829),\n",
       " 'pitch_mean_ratio': np.float64(1.0536770591842635),\n",
       " 'pitch_std_delta': np.float64(20.90458499084042),\n",
       " 'pitch_std_ratio': np.float64(1.0469307791302536),\n",
       " 'pitch_var_delta': np.float64(19060.245597478613),\n",
       " 'pitch_var_ratio': np.float64(1.09606405629028),\n",
       " 'rms_mean_delta': np.float32(-0.00072340015),\n",
       " 'rms_mean_ratio': np.float32(0.9479369),\n",
       " 'rms_std_delta': np.float32(0.00037700962),\n",
       " 'rms_std_ratio': np.float32(1.0372868),\n",
       " 'rms_var_delta': np.float32(7.766088e-06),\n",
       " 'rms_var_ratio': np.float32(1.0759639),\n",
       " 'mfcc_mean_delta': np.float32(-0.93400955),\n",
       " 'mfcc_mean_ratio': np.float32(1.0389701),\n",
       " 'delta_mean_delta': np.float32(-0.06494033),\n",
       " 'delta_mean_ratio': np.float32(0.20032547)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc7ec295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Part 1: Strengths\n",
       "The speaker demonstrates a strong ability to maintain a consistent pace throughout their speech, which suggests a good level of comfort with the topic they are discussing. This consistency is a notable strength, as it helps to engage the listener and convey the message more effectively. Additionally, the speaker's ability to articulate their thoughts and express their feelings in a clear manner is commendable. The initial introduction is smooth, and the speaker's personal baseline characteristics are well-established, providing a solid foundation for the rest of the speech.\n",
       "\n",
       "## Part 2: Areas for Improvement\n",
       "There are moments where the speaker's vocal modulation increases, which may indicate a heightened emotional state or a slight loss of control over their voice. This is particularly noticeable when the speaker expresses uncertainty or fear, such as when mentioning being scared or hoping something works. While this emotional expression can be engaging, it also slightly detracts from the overall fluency of the speech. Furthermore, the speaker could benefit from practicing ways to manage their vocal energy when discussing sensitive or uncertain topics, as this would help to maintain a more even tone throughout the speech.\n",
       "\n",
       "## Part 3: Confidence and Fluency Assessment\n",
       "Based on the relative changes from the speaker's baseline, it appears that their confidence levels are somewhat affected by the content of their speech. When discussing personal fears or uncertainties, the speaker's vocal characteristics shift in a way that suggests a decrease in confidence. However, this shift is not drastic and is likely a natural response to the topic at hand. In terms of fluency, the speaker performs well, with no significant hesitations or long pauses that would disrupt the flow of the speech. Overall, I would rate the speaker's confidence level as moderately affected by the topic, but still within a manageable range. Their fluency level is good, with only minor adjustments needed to achieve a smoother, more controlled delivery. On a scale of 1 to 10, with 10 being the highest, I would rate their confidence a 7 and their fluency an 8. With practice and experience, the speaker has the potential to develop even greater control over their voice and message, leading to more effective and engaging communications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f54ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the audio from Tim Urban's TED talk and using that to assess the model\n",
    "from pytubefix import YouTube\n",
    "\n",
    "if not os.path.exists(\"samples/tim-urban.m4a\"):\n",
    "    yt = YouTube('https://www.youtube.com/watch?v=arj7oStGLkU')\n",
    "    yt.streams.get_audio_only().download('samples', 'tim-urban.m4a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40662727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utkarsh\\AppData\\Local\\Temp\\ipykernel_28780\\1006254953.py:59: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  data, sr = librosa.load(audio_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.1872517\n"
     ]
    }
   ],
   "source": [
    "tim_urban_path = \"samples/tim-urban.m4a\"\n",
    "tim_urban_features = extract_features(tim_urban_path)\n",
    "tim_urban_feedback = generate_feedback(tim_urban_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fc3740e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3478"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.encode(get_prompt(tim_urban_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddedf701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Reviewer.pxs. So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know, you get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light, and I'd bump it up in the middle months, and then at the end, I would kick it up into high gear, just like a little staircase. How hard can it be to just walk up the stairs? No big deal, right? But then the funniest thing happened. Those first few months, They came and went, and I couldn't quite do stuff. So we had an awesome new revised plan. And then ... But then those middle months actually went by, and I didn't really write words. And so we were here. And then two months turned into one month, which turned into two weeks. And one day I woke up with three days until the deadline, still not having written a word. And so I did the only thing I could. I wrote 90 pages over 72 hours, pulling not one but two all-nighters. Humans are not supposed to pull two all-nighters. Sprinted across campus, dove in slow motion, and got it in just at the deadline. I thought that was the end of everything. But a week later, I get a call. It's the school. And they say, is this Tim Urban? And I say, yeah. And they say, we need to talk about your thesis. And I say, OK. And they say, it's the best one we've ever seen. That did not happen. It was a very, very bad thesis. I just wanted to enjoy that one moment when all of you thought, this guy is amazing. No, no, it was very, very bad. Anyway, today I'm a writer, blogger, guy. I write the blog Wait But Why. And a couple of years ago, I decided to write about procrastination. My behaviors always perplexed the non-procrastinators around me, and I wanted to explain to the non-procrastinators of the world what goes on in the heads of procrastinators and why we are the way we are. Now, I had a hypothesis that the brains of procrastinators were actually different than the brains of other people. And to test this, I found an MRI lab that actually let me scan both my brain and the brain of a proven non so I could compare them And I actually brought them here to show you today and I want you to take a look carefully to see if you can notice a difference And I know that if you not a trained brain expert it's not that obvious, but just take a look, okay? So here's the brain of a non-procrastinator. Now, here's my brain. There is a difference. Both brains have a rational decision maker in them, but the procrastinator's brain also has an instant gratification monkey. Now, what does this mean for the procrastinator? Well, it means everything's fine until this happens. So the rational decision maker will make the rational decision to do something productive, but the monkey doesn't like that plan. So he actually takes the wheel and he says, actually, let's read the entire Wikipedia page of the Nancy Kerrigan-Tanya Harding scandal, because I just remembered that that happened. Then ... Then we're going to go over to the fridge, we're going to see if there's anything new in there since 10 minutes ago. After that, we're going to go on a YouTube spiral that starts with videos of Richard Feynman talking about magnets and ends much, much later with us watching interviews with Justin Bieber's mom. All of that's going to take a while, so we're not going to really have room on the schedule for any work today. Sorry. Now, what is going on here? The instant gratification monkey does not seem like a guy you want behind the wheel. He lives entirely in the present moment, he has no memory of the past, no knowledge of the future, and he only cares about two things, easy and fun. Now, in the animal world, that works fine. If you're a dog, and you spend your whole life doing nothing other than easy and fun things, you're a huge success. And to the monkey, humans are just another animal species. It has to keep well-slept, well-fed and propagating into the next generation. Which in tribal times might have worked OK. But if you haven't noticed, now we're not in tribal times. We're in an advanced civilization, and the monkey does not know what that is. Which is why we have another guy in our brain, the rational decision-maker, who gives us the ability to do things no other animal can do. We can visualize the future, we can see the big picture, we can make long-term plans. And he wants to take all of that into account, and he wants to just have us do whatever makes sense to be doing right now. Now, sometimes it makes sense to be doing things that are easy and fun, like when you're having dinner, or going to bed, enjoying well-earned leisure time. That's why there's an overlap. Sometimes they agree. But other times, it makes much more sense to be doing things that are harder and less pleasant for the sake of the big picture, and that's when we have a conflict. And for the procrastinator, that conflict tends to end a certain way every time, leaving him spending a lot of time in this orange zone, an easy and fun place that's entirely out of the make-sense circle. I call it the dark playground. Now, the dark playground is a place that all of you procrastinators out there know very well. It's where leisure activities happen at times when leisure activities are not supposed to be happening. The fun you have in the dark playground isn actually fun because it completely unearned and the air is filled with guilt dread anxiety self all those good procrastinator feelings And the question is in this situation with the monkey behind the wheel how does the procrastinator ever get himself over here to this blue zone, a less pleasant place, but where really important things happen? Well, it turns out that the procrastinator has a guardian angel, someone who's always looking down on him watching over him in his darkest moments, someone called the Panic Monster. Now, the Panic Monster is dormant most of the time, but he suddenly wakes up. Any time a deadline gets too close or there's danger of public embarrassment, a career disaster or some other scary consequence, and importantly, he's the only thing that the monkey is terrified of. Now, he became very relevant in my life pretty recently because the people of TED reached out to me about six months ago and invited me to do a TED Talk. Now, of course, I said, yes, it's always been a dream of mine to have done a TED Talk in the past. But in the middle of all this excitement, the rational decision-maker seemed to have something else in his mind. He was saying, are we clear on what we just accepted? Do we get what's going to be now happening one day in the future? We need to sit down and work on this right now. And the monkey said, totally agree, but also let's just open Google Earth and let's zoom into the bottom of India, like 200 feet above the ground, and we're going to scroll up for two and a half hours until we get to the top of the country so we can get a better feel for India. So that's what we did that day. As six months turned into four, and then two, and then one, the people of TED decided to release the speakers. And I opened up the website, and there was my face staring right back at me, and guess who woke up? So the panic monster starts losing his mind, and a few seconds later, the whole system's in mayhem. And the monkey, who, remember, he's terrified of the panic monster. Boom, he's up the tree. And finally, finally, the rational decision-maker can take the wheel, and I can start working on the talk. Now, the panic monster explains all kinds of pretty insane, procrastinated behavior, like how someone like me could spend two weeks unable to start the opening sentence of a paper and then miraculously find the unbelievable work ethic to stay up all night and write eight pages. And this entire situation with the three characters, this is the procrastinator's system. It's not pretty, but in the end, it works. And this is what I decided to write about on the blog just a couple years ago. Now, when I did, I was amazed by the response. Literally thousands of emails came in from all different kinds of people, from all over the world, doing all different kinds of things. These were people who were nurses and bankers and painters and engineers and lots and lots of PhD students. And they were all writing saying the same thing I have this problem too But what struck me was the contrast between the light tone of the post and the heaviness of these emails These people were writing with intense frustration about what procrastination had done to their lives, about what this monkey had done to them. And I thought about this, and I said, if the procrastinator system works, then what's going on? Why are all these people in such a dark place? It turns out that there's two kinds of procrastination. Everything I've talked about today, the examples I've given, they all have deadlines. And when there's deadlines, the effects of procrastination are contained to the short term because the panic monster gets involved. But there's a second kind of procrastination that happens in situations when there is no deadline. So if you want to have a career where you want to be a self-starter, something in the arts, something entrepreneurial, there's no deadlines on those things at first, because nothing's happening at first, until you've gone out and done the hard work to get some momentum, to get things going. There's also all kinds of important things outside of your career that don't involve any deadlines, like seeing your family or exercising and taking care of your health, working on your relationship or getting out of a relationship that isn't working. Now, if the procrastinator's only mechanism of doing these hard things is the panic monster, that's a problem, because in all of these non-deadline situations, the panic monster doesn't show up, he has nothing to wake up for, The effects of procrastination, they're not contained, they just extend outward forever. And it's this long-term kind of procrastination that's much less visible and much less talked about than the funnier short-term deadline-based kind. It's usually suffered quietly and privately, and it can be the source of a huge amount of long-term unhappiness and regrets. And I thought, you know, that's why these people are emailing, and that's why they're in such a bad place. It's not that they're cramming for some project. It's that long-term procrastination has made them feel like a spectator, at times, in their own lives. The frustration was not that they couldn't achieve their dreams, it's that they weren't even able to start chasing them. So I read these emails and I had a little bit of an epiphany that I don't think non-procrastinators exist. That's right, I think all of you are procrastinators. Now, you might not all be a mess, like some of us. And some of you may have a healthy relationship with deadlines. But remember, the monkey's sneakiest trick is when the deadlines aren't there. Now, I want to show you one last thing. I call this a life calendar. That's one box for every week of a 90-year life. That's not that many boxes, especially since we've already used a bunch of those. So I think we need to all take a long, hard look at that calendar. And we need to think about what we're really procrastinating on, because everyone is procrastinating on something in life. We need to stay aware of the instant gratification monkey. That's a job for all of us. And because there's not that many boxes on there, it's a job that should probably start today. Well, maybe not today, but ... You know, sometime soon. Thank you.\",\n",
       " 'duration': 843.7450339999999,\n",
       " 'baseline_duration': 42.1872517,\n",
       " 'speaking_rate': 2.661941593128239,\n",
       " 'syllables_rate': 3.7167626162289213,\n",
       " 'baseline_speaking_rate': 2.2281612622800933,\n",
       " 'baseline_syllables_rate': 2.7733496562422437,\n",
       " 'long_pause_count': 28,\n",
       " 'long_pause_duration': np.float64(39.96154195011337),\n",
       " 'zcr': np.float64(0.12431602464380555),\n",
       " 'pitch_mean': np.float64(297.7152699555939),\n",
       " 'pitch_std': np.float64(337.9058853964825),\n",
       " 'pitch_var': np.float64(114180.38738558079),\n",
       " 'rms_mean': np.float32(0.070083246),\n",
       " 'rms_std': np.float32(0.061486296),\n",
       " 'rms_var': np.float32(0.0037805648),\n",
       " 'mfcc_mean': np.float32(-14.877528),\n",
       " 'delta_mean': np.float32(0.00010704117),\n",
       " 'baseline_zcr': np.float64(0.0985455789588167),\n",
       " 'baseline_pitch_mean': np.float64(344.77829722568737),\n",
       " 'baseline_pitch_std': np.float64(568.1480977328476),\n",
       " 'baseline_pitch_var': np.float64(322792.26095745334),\n",
       " 'baseline_rms_mean': np.float32(0.06299822),\n",
       " 'baseline_rms_std': np.float32(0.048998803),\n",
       " 'baseline_rms_var': np.float32(0.0024008828),\n",
       " 'baseline_mfcc_mean': np.float32(-3.0564888),\n",
       " 'baseline_delta_mean': np.float32(0.0722437),\n",
       " 'zcr_delta': np.float64(0.025770445684988846),\n",
       " 'zcr_ratio': np.float64(1.261507882517577),\n",
       " 'pitch_mean_delta': np.float64(-47.06302727009347),\n",
       " 'pitch_mean_ratio': np.float64(0.8634977095461243),\n",
       " 'pitch_std_delta': np.float64(-230.24221233636513),\n",
       " 'pitch_std_ratio': np.float64(0.594749655494528),\n",
       " 'pitch_var_delta': np.float64(-208611.87357187254),\n",
       " 'pitch_var_ratio': np.float64(0.35372715271085975),\n",
       " 'rms_mean_delta': np.float32(0.0070850253),\n",
       " 'rms_mean_ratio': np.float32(1.112464),\n",
       " 'rms_std_delta': np.float32(0.012487493),\n",
       " 'rms_std_ratio': np.float32(1.254853),\n",
       " 'rms_var_delta': np.float32(0.001379682),\n",
       " 'rms_var_ratio': np.float32(1.5746561),\n",
       " 'mfcc_mean_delta': np.float32(-11.821039),\n",
       " 'mfcc_mean_ratio': np.float32(4.8675227),\n",
       " 'delta_mean_delta': np.float32(-0.072136655),\n",
       " 'delta_mean_ratio': np.float32(0.0014816679)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tim_urban_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65042b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Part 1: Strengths**\n",
       "The speaker demonstrates a good level of vocal control, particularly in terms of modulation. There are noticeable variations in their tone, which helps to convey enthusiasm and emphasis on certain points. This is evident in the way they share personal anecdotes and explain complex concepts, such as the \"instant gratification monkey\" and the \"panic monster.\" The speaker's ability to modulate their voice adds depth and engagement to their narrative, making it more enjoyable to listen to. Additionally, their speaking rate is well-balanced, allowing them to convey their ideas clearly without rushing or dragging.\n",
       "\n",
       "**Part 2: Areas for Improvement**\n",
       "One area where the speaker could improve is in maintaining a consistent level of vocal energy. At times, their voice seems to lack the energy and emphasis present at the beginning of the speech. This could be due to the speaker settling into a comfortable rhythm, but it also results in some sections feeling slightly flat. Furthermore, the speaker's use of long pauses, while sometimes effective for dramatic effect, can also disrupt the flow of their narrative. It would be beneficial for the speaker to practice varying their pause lengths and using them more strategically to enhance their message.\n",
       "\n",
       "**Part 3: Confidence and Fluency Assessment**\n",
       "Based on the relative metrics, I would rate the speaker's confidence level as moderate to high. Their ability to share personal stories and explain complex ideas suggests a good level of self-assurance. However, the decrease in vocal energy and the use of long pauses may indicate some hesitation or uncertainty. In terms of fluency, the speaker generally demonstrates a good level of articulation and coherence. Their speaking rate is well-balanced, and they are able to convey their ideas clearly. However, there are moments where their fluency is disrupted by pauses or a slight decrease in vocal energy. Overall, I would give the speaker a confidence rating of 7.5/10 and a fluency rating of 8/10. With some practice and attention to maintaining consistent vocal energy and strategic pause use, the speaker has the potential to deliver even more engaging and effective presentations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(tim_urban_feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b98539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
