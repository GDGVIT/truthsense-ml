Phase 1 Evaluation Scope

The current version of TruthSense (Phase 1) is designed to offer lightweight, practical feedback on speech fluency and delivery using simple acoustic features and GPT-based interpretation. It provides meaningful insights on elements such as speaking rate, pause frequency, tone flatness, and overall vocal energy. This enables it to act as a first-level communication coach, capable of pointing out issues like monotone delivery, rushed speech, or overuse of long pauses.

However, it does not yet perform deep emotional analysis or per-second tracking of nervousness or confidence, as those require time-resolved modeling or emotion-classifying neural networks. Despite these limitations, Phase 1 serves as a valuable and efficient MVP, offering immediately actionable feedback while laying the groundwork for more advanced evaluation in future phases.



TruthSense Phase 1: Voice Feedback System

Overview

TruthSense Phase 1 is a lightweight voice-only feedback tool that analyzes spoken audio recordings to evaluate the clarity, fluency, and delivery quality of the user’s speech. It does this using heuristically extracted acoustic features and leverages GPT to provide feedback. No deep learning model training is involved at this stage, making the system fast, interpretable, and easy to deploy.

⸻

Workflow Summary
	1.	Audio Upload
The user uploads a .wav or .mp3 file of their speech, interview answer, or presentation.
	2.	Speech Transcription
The audio is transcribed using a speech-to-text system (e.g., OpenAI Whisper via API or Groq).
	3.	Acoustic Feature Extraction
The system extracts multiple speech-related features, including pitch dynamics, energy variation, long pauses, and speaking rate. These are summary statistics (e.g., mean, standard deviation) derived from the audio signal.
	4.	Feature Aggregation
All extracted metrics are combined with the transcript to form a structured input.
	5.	LLM Feedback Generation
A prompt is created and sent to GPT, asking it to analyze the transcript and acoustic delivery features to provide human-readable feedback on clarity, fluency, confidence, and delivery strength.
	6.	Output Delivery
The user receives a paragraph of constructive, narrative-style feedback evaluating their speech delivery and tone.

⸻

Features Extracted

The following features are computed from the raw audio signal:
	•	Zero-Crossing Rate (ZCR) – Indicator of noisiness and articulation
	•	Pitch Mean and Standard Deviation – Prosody and voice modulation
	•	Root Mean Square (RMS) Energy – Speaking strength and volume control
	•	Mean
	•	Standard deviation
	•	Variance
	•	MFCC Mean – Core vocal tract features for timbre and clarity
	•	Delta MFCC Mean – Short-term changes in speech articulation
	•	Speaking Rate – Words per second, indicating fluency
	•	Long Pause Detection
	•	Count of silences longer than 1 second
	•	Total duration of long pauses

⸻

Output

The system returns:
	•	A textual transcript of the speech
	•	A summary dictionary of metrics
	•	A narrative feedback paragraph generated by GPT

This feedback explains whether the speaker’s delivery was confident, clear, fluent, or hesitant — and what can be improved.

⸻
