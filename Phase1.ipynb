{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b5d9a",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb65140",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install compatible versions\n",
    "!pip install numpy==1.23.5 librosa==0.10.0.post2 transformers openai soundfile --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfa84c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import openai\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07983b90",
   "metadata": {},
   "source": [
    "# Transcribe using Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53bbba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# HuggingFace Whisper transcription\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")\n",
    "\n",
    "def transcribe_audio(path):\n",
    "    return transcriber(path)[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c16201",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Features extracted:\n",
    "\t•\tZCR\n",
    "\t•\tPitch\n",
    "\t•\tRMS\n",
    "\t•\tMFCC\n",
    "\t•\tDeltaMFCC\n",
    "\t•\tSpeakingRate\n",
    "\t•\tPauseCount\n",
    "\t•\tPauseDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b40339b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(audio_path, transcribe_fn):\n",
    "    data, sr = librosa.load(audio_path)\n",
    "\n",
    "    # 1. Zero-Crossing Rate (ZCR)\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(data))\n",
    "\n",
    "    # 2. Pitch: Mean and Std Deviation\n",
    "    pitch = librosa.yin(data, fmin=librosa.note_to_hz(\"C2\"),\n",
    "                        fmax=librosa.note_to_hz(\"C7\"), sr=sr)\n",
    "    pitch = np.nan_to_num(pitch, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    pitch_mean = np.mean(pitch)\n",
    "    pitch_std = np.std(pitch)\n",
    "\n",
    "    # 3. Energy: RMS Mean, Std, Variance\n",
    "    rms = librosa.feature.rms(y=data)[0]\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    rms_var = np.var(rms)\n",
    "\n",
    "    # 4. MFCC and Delta MFCC Mean\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_mean = np.mean(mfcc)\n",
    "    delta_mean = np.mean(delta)\n",
    "\n",
    "    # 5. Transcription\n",
    "    transcript = transcribe_fn(audio_path)  # Whisper or API call\n",
    "\n",
    "    # 6. Speaking Rate: Words per second\n",
    "    duration_sec = librosa.get_duration(y=data, sr=sr)\n",
    "    word_count = len(transcript.split())\n",
    "    speaking_rate = word_count / duration_sec if duration_sec > 0 else 0\n",
    "\n",
    "    # 7. Long Pauses (>1s)\n",
    "    intervals = librosa.effects.split(data, top_db=30)\n",
    "    pauses = []\n",
    "    for i in range(1, len(intervals)):\n",
    "        prev_end = intervals[i-1][1]\n",
    "        cur_start = intervals[i][0]\n",
    "        silence_duration = (cur_start - prev_end) / sr\n",
    "        if silence_duration > 1.0:\n",
    "            pauses.append(silence_duration)\n",
    "    long_pause_count = len(pauses)\n",
    "    long_pause_total = sum(pauses)\n",
    "\n",
    "    return {\n",
    "        \"transcript\": transcript,\n",
    "        \"zcr\": zcr,\n",
    "        \"pitch_mean\": pitch_mean,\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"rms_mean\": rms_mean,\n",
    "        \"rms_std\": rms_std,\n",
    "        \"rms_var\": rms_var,\n",
    "        \"speaking_rate\": speaking_rate,\n",
    "        \"long_pause_count\": long_pause_count,\n",
    "        \"long_pause_duration\": long_pause_total,\n",
    "        \"mfcc_mean\": mfcc_mean,\n",
    "        \"delta_mean\": delta_mean\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc49174",
   "metadata": {},
   "source": [
    "# Send to GPT for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49443371",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load .env variables into environment\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d8f04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_feedback(features):\n",
    "    prompt = f\"\"\"\n",
    "You're a communication coach. Analyze the following features of a speaker:\n",
    "\n",
    "- Transcript: {features['transcript']}\n",
    "- ZCR: {features['zcr']}\n",
    "- Mean pitch: {features['pitch_mean']}\n",
    "- Std pitch: {features['pitch_std']}\n",
    "- RMS (mean/std/var): {features['rms_mean']}, {features['rms_std']}, {features['rms_var']}\n",
    "- Speaking rate: {features['speaking_rate']} words/sec\n",
    "- Long pauses: {features['long_pause_count']} pauses totaling {features['long_pause_duration']} sec\n",
    "- MFCC mean: {features['mfcc_mean']}\n",
    "- Delta MFCC mean: {features['delta_mean']}\n",
    "\n",
    "Based on this data, provide feedback on the user's fluency, confidence, and delivery.\n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211bdbf",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365e68b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "path = \"/path/to/audio.wav\"\n",
    "features = extract_features(path)\n",
    "feedback = generate_feedback(features)\n",
    "print(\"Generated Feedback:\\n\", feedback)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
