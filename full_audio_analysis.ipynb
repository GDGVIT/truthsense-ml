{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b5d9a",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25780097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: librosa in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: torch in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: groq in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: load_dotenv in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (4.14.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from load_dotenv) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from standard-aifc->librosa) (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas librosa torch groq load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "771b07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tiktoken in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pytubefix in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (9.2.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utkarsh\\onedrive\\documents\\github\\truthsense-ml\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken pytubefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91bfa84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import asyncio\n",
    "import librosa\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "from pydub import AudioSegment\n",
    "from nltk.corpus import cmudict\n",
    "from parselmouth.praat import call\n",
    "from groq import Groq, AsyncClient\n",
    "from groq.types.audio import Transcription\n",
    "\n",
    "# Load environment file\n",
    "from load_dotenv import load_dotenv\n",
    "print(load_dotenv('.env.local'))\n",
    "\n",
    "assert os.environ.get('GROQ_API_KEY'), \"Groq API key not found in .env file, please set the key before starting this notebook\"\n",
    "\n",
    "# Global variables\n",
    "client = AsyncClient()\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "fluency_model = joblib.load('fluency/models/weights/xgboost_model.pkl')\n",
    "\n",
    "try:\n",
    "    cmu_dict = cmudict.dict()\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('cmudict')\n",
    "    cmu_dict = cmudict.dict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071f222a",
   "metadata": {},
   "source": [
    "## Monitor CPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5d9afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import functools\n",
    "\n",
    "def monitor_resources(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process = psutil.Process(os.getpid())\n",
    "\n",
    "        # Get memory and CPU before\n",
    "        mem_before = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "        cpu_before = process.cpu_percent(interval=None)\n",
    "\n",
    "        # Start time and CPU\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Get memory and CPU after\n",
    "        mem_after = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "        cpu_after = process.cpu_percent(interval=0.1)\n",
    "\n",
    "        # Get number of CPUs used\n",
    "        cpu_affinity = process.cpu_affinity()\n",
    "        \n",
    "        print(f\"Function: {func.__name__}\")\n",
    "        print(f\"Execution Time: {end_time - start_time:.2f} sec\")\n",
    "        print(f\"Memory Usage: {mem_after - mem_before:.2f} MB\")\n",
    "        print(f\"CPU Usage: {cpu_after:.2f}%\")\n",
    "        print(f\"CPU Cores Used: {cpu_affinity}\")\n",
    "\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def limit_to_one_core(core_id=0):\n",
    "    \"\"\"\n",
    "    Set process to run only on one CPU core (default: core 0).\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            process = psutil.Process(os.getpid())\n",
    "\n",
    "            # Store the current affinity to restore later\n",
    "            original_affinity = process.cpu_affinity()\n",
    "            \n",
    "            try:\n",
    "                # Set affinity to a single core\n",
    "                process.cpu_affinity([core_id])\n",
    "                print(f\"Running {func.__name__} on CPU core {core_id}\")\n",
    "                return func(*args, **kwargs)\n",
    "            finally:\n",
    "                # Restore original affinity\n",
    "                process.cpu_affinity(original_affinity)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# # Limit NumPy, OpenBLAS etc to use only one CPU core\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c16201",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Features extracted:\n",
    "* ZCR\n",
    "* Pitch\n",
    "* Jitter\n",
    "* Shimmer\n",
    "* Harmonic-to-Noise ratio\n",
    "* RMS\n",
    "* MFCC\n",
    "* DeltaMFCC\n",
    "* SpeakingRate\n",
    "* PauseCount\n",
    "* PauseDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4b64c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transcription(text=\" Hi, my name is Adkarsh Malaya. I'm a student and right now what I'm trying to do is I'm trying to get a model and I'm trying to use it to transcribe some filler words. I am very scared. I don't know what will happen and I really really hope this works.\", words=[{'word': 'Hi,', 'start': 0.78, 'end': 1.18}, {'word': 'my', 'start': 1.18, 'end': 1.52}, {'word': 'name', 'start': 1.52, 'end': 1.66}, {'word': 'is', 'start': 1.66, 'end': 1.82}, {'word': 'Adkarsh', 'start': 1.82, 'end': 2.04}, {'word': 'Malaya.', 'start': 2.04, 'end': 2.52}, {'word': \"I'm\", 'start': 2.52, 'end': 3.12}, {'word': 'a', 'start': 3.12, 'end': 3.26}, {'word': 'student', 'start': 3.26, 'end': 3.76}, {'word': 'and', 'start': 3.76, 'end': 4.22}, {'word': 'right', 'start': 4.22, 'end': 4.62}, {'word': 'now', 'start': 4.62, 'end': 4.84}, {'word': 'what', 'start': 4.84, 'end': 5.1}, {'word': \"I'm\", 'start': 5.1, 'end': 5.26}, {'word': 'trying', 'start': 5.26, 'end': 5.38}, {'word': 'to', 'start': 5.38, 'end': 5.5}, {'word': 'do', 'start': 5.5, 'end': 5.64}, {'word': 'is', 'start': 5.64, 'end': 5.88}, {'word': \"I'm\", 'start': 5.88, 'end': 6.02}, {'word': 'trying', 'start': 6.02, 'end': 6.2}, {'word': 'to', 'start': 6.2, 'end': 6.34}, {'word': 'get', 'start': 6.34, 'end': 6.54}, {'word': 'a', 'start': 6.54, 'end': 6.84}, {'word': 'model', 'start': 6.84, 'end': 7.08}, {'word': 'and', 'start': 7.08, 'end': 7.7}, {'word': \"I'm\", 'start': 7.7, 'end': 7.88}, {'word': 'trying', 'start': 7.88, 'end': 8.02}, {'word': 'to', 'start': 8.02, 'end': 8.18}, {'word': 'use', 'start': 8.18, 'end': 8.76}, {'word': 'it', 'start': 8.76, 'end': 8.98}, {'word': 'to', 'start': 8.98, 'end': 9.18}, {'word': 'transcribe', 'start': 9.18, 'end': 9.82}, {'word': 'some', 'start': 9.82, 'end': 10.06}, {'word': 'filler', 'start': 10.06, 'end': 10.34}, {'word': 'words.', 'start': 10.34, 'end': 10.68}, {'word': 'I', 'start': 10.68, 'end': 11.46}, {'word': 'am', 'start': 11.46, 'end': 11.56}, {'word': 'very', 'start': 11.56, 'end': 11.76}, {'word': 'scared.', 'start': 11.76, 'end': 12.22}, {'word': 'I', 'start': 12.22, 'end': 12.54}, {'word': \"don't\", 'start': 12.54, 'end': 12.7}, {'word': 'know', 'start': 12.7, 'end': 12.78}, {'word': 'what', 'start': 12.78, 'end': 12.92}, {'word': 'will', 'start': 12.92, 'end': 13.04}, {'word': 'happen', 'start': 13.04, 'end': 13.22}, {'word': 'and', 'start': 13.22, 'end': 13.96}, {'word': 'I', 'start': 13.96, 'end': 14.16}, {'word': 'really', 'start': 14.16, 'end': 14.42}, {'word': 'really', 'start': 14.42, 'end': 14.58}, {'word': 'hope', 'start': 14.58, 'end': 14.78}, {'word': 'this', 'start': 14.78, 'end': 14.96}, {'word': 'works.', 'start': 14.96, 'end': 15.26}], duration=15.72)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async Transcription\n",
    "def split_audio_in_memory(audio_path, max_mb=24):\n",
    "    audio = AudioSegment.from_wav(audio_path)\n",
    "    bytes_per_ms = (audio.frame_rate * audio.frame_width * audio.channels) / 1000\n",
    "    max_bytes = max_mb * 1024 * 1024\n",
    "    chunk_duration_ms = int(max_bytes / bytes_per_ms)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(audio), chunk_duration_ms):\n",
    "        chunk = audio[i:i+chunk_duration_ms]\n",
    "        buffer = io.BytesIO()\n",
    "        chunk.export(buffer, format=\"wav\")\n",
    "        buffer.seek(0)\n",
    "        chunks.append((f\"chunk_{i//chunk_duration_ms}.wav\", buffer))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "async def transcribe_chunk(filename, audio_buffer):\n",
    "    return await client.audio.transcriptions.create(\n",
    "        file=(filename, audio_buffer.read()),\n",
    "        model=\"distil-whisper-large-v3-en\",\n",
    "        response_format=\"verbose_json\",\n",
    "        timestamp_granularities=[\"word\"]\n",
    "    )\n",
    "\n",
    "\n",
    "async def transcribe_audio(audio_path, client=client):\n",
    "    \"\"\"Transcribe an audio file without saving the chunks to disk\"\"\"\n",
    "    chunks = split_audio_in_memory(audio_path)\n",
    "    tasks = [transcribe_chunk(name, buffer) for name, buffer in chunks]\n",
    "    all_transcripts = await asyncio.gather(*tasks)\n",
    "\n",
    "    transcript_parts = []\n",
    "    all_words = []\n",
    "    total_duration = 0.0\n",
    "\n",
    "    for chunk in all_transcripts:\n",
    "        transcript_parts.append(chunk.text)\n",
    "        all_words.extend(getattr(chunk, \"words\", []))\n",
    "        total_duration += chunk.duration\n",
    "\n",
    "    transcript = \"\".join(transcript_parts)\n",
    "    \n",
    "    return Transcription(text=transcript, words=all_words, duration=total_duration)\n",
    "\n",
    "\n",
    "transcript = await transcribe_audio(\"samples/confident.wav\")\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1898093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for calculating syllables speaking rate\n",
    "def get_word_syllable_count(word):\n",
    "    word = word.lower().strip(\".,?!;:\")\n",
    "    if word in cmu_dict:\n",
    "        return len([p for p in cmu_dict[word][0] if p[-1].isdigit()])\n",
    "    return max(1, len(re.findall(r'[aeiouy]+', word)))\n",
    "\n",
    "\n",
    "def estimate_syllable_rate(transcript, duration_sec):\n",
    "    words = transcript.split()\n",
    "    total_syllables = sum(get_word_syllable_count(word) for word in words)\n",
    "    return total_syllables / duration_sec if duration_sec > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "958711f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pitch statistics, Jitter, Shimmer, and HNR ratio through Parselmouth\n",
    "@monitor_resources\n",
    "def extract_parselmouth_features(data, sr):\n",
    "    snd = parselmouth.Sound(values=data, sampling_frequency=sr)\n",
    "\n",
    "    pitch_obj = snd.to_pitch()\n",
    "    pitch_mean = call(pitch_obj, \"Get mean\", 0, 0, \"Hertz\")\n",
    "    pitch_std = call(pitch_obj, \"Get standard deviation\", 0, 0, \"Hertz\")\n",
    "\n",
    "    point_process = call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
    "    jitter = call(point_process, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    shimmer = call([snd, point_process], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "\n",
    "    harmonicity = call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "\n",
    "    return {\n",
    "        \"pitch_mean\": pitch_mean,\n",
    "        \"pitch_std\": pitch_std,\n",
    "        \"pitch_var\": pitch_std**2,\n",
    "        \"jitter_local\": jitter,\n",
    "        \"shimmer_local\": shimmer,\n",
    "        \"hnr\": hnr\n",
    "    }\n",
    "\n",
    "async def async_extract_parselmouth_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_parselmouth_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6055eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract RMS Energy, ZCR, MFCC and Deltas using librosa\n",
    "@monitor_resources\n",
    "def extract_librosa_features(data, sr):\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(data))\n",
    "    \n",
    "    rms = librosa.feature.rms(y=data)[0]\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    rms_var = np.var(rms)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    mfcc_mean = np.mean(mfcc)\n",
    "    delta_mean = np.mean(delta)\n",
    "\n",
    "    return {\n",
    "        \"zcr\": zcr,\n",
    "        \"rms_mean\": rms_mean,\n",
    "        \"rms_std\": rms_std,\n",
    "        \"rms_var\": rms_var,\n",
    "        \"mfcc\": mfcc.mean(axis=1),\n",
    "        \"delta_mfcc\": delta.mean(axis=1),\n",
    "        \"mfcc_mean\": mfcc_mean,\n",
    "        \"delta_mean\": delta_mean\n",
    "    }\n",
    "    \n",
    "async def async_extract_librosa_features(data, sr, executor):\n",
    "    return await asyncio.get_event_loop().run_in_executor(\n",
    "        executor, extract_librosa_features, data, sr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a970b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_wave(data, sr):\n",
    "    return {\n",
    "        **extract_librosa_features(data, sr),\n",
    "        **extract_parselmouth_features(data, sr)\n",
    "    }\n",
    "    \n",
    "async def async_extract_features_from_wave(data, sr, executor):\n",
    "    # Start both tasks concurrently\n",
    "    librosa_task = asyncio.create_task(async_extract_librosa_features(data, sr, executor))\n",
    "    parselmouth_task = asyncio.create_task(async_extract_parselmouth_features(data, sr, executor))\n",
    "\n",
    "    # Wait for both\n",
    "    librosa_feats, parselmouth_feats = await asyncio.gather(librosa_task, parselmouth_task)\n",
    "\n",
    "    return {**librosa_feats, **parselmouth_feats}\n",
    "\n",
    "\n",
    "@monitor_resources\n",
    "async def extract_features(audio_path, baseline_duration: float = 0.0, fluency_model=fluency_model):\n",
    "    data, sr = librosa.load(audio_path)\n",
    "    assert len(data) != 0, \"Your audio file appears to contain no content. Please input a valid file\"\n",
    "    \n",
    "    duration_sec = librosa.get_duration(y=data, sr=sr)\n",
    "    baseline_duration = baseline_duration or max(10.0, duration_sec * 0.05)\n",
    "    \n",
    "    # Baseline from first few seconds\n",
    "    baseline_data = data[:min(len(data), int(sr * baseline_duration))]\n",
    "    baseline_feats = extract_features_from_wave(baseline_data, sr)\n",
    "    full_feats = extract_features_from_wave(data, sr)\n",
    "\n",
    "    # Get fluency ratings\n",
    "    features = ['zcr', 'pitch_mean', 'pitch_std', 'rms_mean', 'rms_std', 'rms_var', 'mfcc_mean', 'delta_mean']\n",
    "    rating_map = ['Low', 'Medium', 'High']\n",
    "        \n",
    "    baseline_fluency_features = np.array([baseline_feats[key] for key in baseline_feats if key in features])\n",
    "    full_fluency_features = np.array([full_feats[key] for key in full_feats if key in features])\n",
    "\n",
    "    res = fluency_model.predict(np.vstack((baseline_fluency_features, full_fluency_features)))\n",
    "    baseline_fluency = rating_map[res[0].argmax()]\n",
    "    full_fluency = rating_map[res[1].argmax()]\n",
    "\n",
    "    relative_feats = {}\n",
    "    for key in full_feats:\n",
    "        if key not in ['mfcc', 'delta_mfcc']:\n",
    "            base = baseline_feats.get(key, 0.0)\n",
    "            full = full_feats[key]\n",
    "            relative_feats[f'{key}_delta'] = full - base\n",
    "            relative_feats[f'{key}_ratio'] = full / base if base != 0 else 0\n",
    "\n",
    "    # Transcription and Speaking Rates\n",
    "    transcription_json = await transcribe_audio(audio_path)\n",
    "    duration_sec = transcription_json.duration # type: ignore\n",
    "\n",
    "    assert duration_sec != 0, \"File duration appears to be 0 after transcription?\"\n",
    "    \n",
    "    # Full data speaking rate\n",
    "    transcript = transcription_json.text\n",
    "    word_count = len(transcript.split())\n",
    "    speaking_rate = word_count / duration_sec\n",
    "    syllables_rate = estimate_syllable_rate(transcript, duration_sec)\n",
    "    \n",
    "    # Baseline speaking rate\n",
    "    baseline_transcript = [word_segment['word'] for word_segment in transcription_json.words if word_segment['start'] <= baseline_duration]  # type: ignore\n",
    "    baseline_word_count = len(baseline_transcript)\n",
    "    baseline_transcript = \" \".join(baseline_transcript)\n",
    "    baseline_speaking_rate = baseline_word_count / baseline_duration\n",
    "    baseline_syllables_rate = estimate_syllable_rate(baseline_transcript, baseline_duration)\n",
    "    \n",
    "    # Pause detection\n",
    "    intervals = librosa.effects.split(data, top_db=30)\n",
    "    pauses = [(intervals[i][0] - intervals[i - 1][1]) / sr\n",
    "              for i in range(1, len(intervals))\n",
    "              if (intervals[i][0] - intervals[i - 1][1]) / sr > 1.0]\n",
    "    \n",
    "    long_pause_count = len(pauses)\n",
    "    long_pause_total = sum(pauses)\n",
    "\n",
    "    return {\n",
    "        \"transcript\": transcript,\n",
    "        \"duration\": duration_sec,\n",
    "        \"baseline_duration\": baseline_duration,\n",
    "        \"speaking_rate\": speaking_rate,\n",
    "        \"syllables_rate\": syllables_rate,\n",
    "        \"baseline_speaking_rate\": baseline_speaking_rate,\n",
    "        \"baseline_syllables_rate\": baseline_syllables_rate,\n",
    "        \"long_pause_count\": long_pause_count,\n",
    "        \"long_pause_duration\": long_pause_total,\n",
    "        \"fluency_rating\": full_fluency,\n",
    "        \"baseline_fluency_rating\": baseline_fluency,\n",
    "        **full_feats,\n",
    "        **{f'baseline_{k}': v for k, v in baseline_feats.items()},\n",
    "        **relative_feats,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d05c6d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.07 sec\n",
      "Memory Usage: 0.91 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.28 sec\n",
      "Memory Usage: -2.73 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 1.24 sec\n",
      "Memory Usage: 0.95 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 4.71 sec\n",
      "Memory Usage: 2.18 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "features = await extract_features('samples/tim-urban.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d332011f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Reviewer.pxs. So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know, you get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light, and I'd bump it up in the middle months. And then at the end, I would kick it up into high gear, just like a little staircase. How hard can it be to just walk up the stairs? No big deal, right? But then the funniest thing happened. Those first few months, they came and went, and I couldn't quite do stuff. So we had an awesome new revised plan. And then ... But then those middle months actually went by, and I didn't really write words. And so we were here. And then two months turned into one month, which turned into two weeks. And one day I woke up, with three days until the deadline, still not having written a word, and so I did the only thing I could. I wrote 90 pages over 72 hours, pulling not one but two all-nighters. Humans are not supposed to pull two all-nighters. Sprinted across campus, dove in slow motion, and got it in just at the deadline. I thought that was the end of everything. But a week later, I get a call. It's the school. And they say, is this Tim Urban? And I say, yeah. And they say, we need to talk about your thesis. And I say, OK. And they say, it's the best one we've ever seen. That did not happen. It was a very, very bad thesis. I just wanted to enjoy that one moment when all of you thought, this guy is amazing! No, no, it was very, very bad. Anyway, today I'm a writer, blogger, guy. I write the blog Wait But Why. And a couple of years ago, I decided to write about procrastination. My behaviors always perplexed the non-procrastinators around me, and I wanted to explain to the non-procrastinators of the world what goes on in the heads of procrastinators and why we are the way we are. Now, I had a hypothesis that the brains of procrastinators were actually different than the brains of other people. And to test this, I found an MRI lab that actually let me scan both my brain and the brain of a proven non-procrastinator so I could compare them. And I actually brought them here to show you today, and I want you to take a look carefully to see if you can notice a difference. And I know that if you're not a trained brain expert, it's not that obvious, but just take a look, OK? So here's the brain of a non-procrastinator. Now, here's my brain. There is a difference. Both brains have a rational decision-maker in them, but the procrastinator's brain also has an instant gratification monkey. Now, what does this mean for the procrastinator? It means everything's fine until this happens. So the rational decision-maker will make the rational decision to do something productive, but the monkey doesn't like that plan. So he actually takes the wheel and he says, actually, let's read the entire Wikipedia page of the Nancy Kerrigan-Tanya Harding scandal, because I just remembered that that happened. Then we're going to go over to the fridge, we're going to see if there's anything new in there since 10 minutes ago. After that, we're going to go on a YouTube spiral that starts with videos of Richard Feynman talking about magnets and ends much, much later with us watching interviews with Justin Bieber's mom. All of that's going to take a while, so we're not going to really have room on the schedule for any work today. Sorry. Now, what is going on here? The instant gratification monkey does not seem like a guy you want behind the wheel. He lives entirely in the present moment, he has no memory of the past, no knowledge of the future, He cares about two things, easy and fun. Now, in the animal world, that works fine. If you're a dog, and you spend your whole life doing nothing other than easy and fun things, you're a huge success. And to the monkey, humans are just another animal species. He has to keep well-slept, well-fed and propagating into the next generation, which in tribal times might have worked OK. But if you haven't noticed, now we're not in tribal times. We're in an advanced civilization, and the monkey does not know what that is. Which is why we have another guy in our brain, the rational decision-maker, who gives us the ability to do things no other animal can do. We can visualize the future, we can see the big picture, We can make long-term plans, and he wants to take all of that into account, and he wants to just have us do whatever makes sense to be doing right now. Now, sometimes it makes sense to be doing things that are easy and fun, like when you're having dinner or going to bed or enjoying well-earned leisure time. That's why there's an overlap. Sometimes they agree. But other times, it makes much more sense to be doing things that are harder and less pleasant for the sake of the big picture, and that's when we have a conflict. And for the procrastinator, that conflict tends to end a certain way every time, leaving him spending a lot of time in this orange zone, an easy and fun place that's entirely out of the make-sense circle. I call it the dark playground. Now, the dark playground is a place that all of you procrastinators out there know very well. It's where leisure activities happen at times at times when leisure activities are not supposed to be happening. The fun you have in the dark playground isn't actually fun because it's completely unearned, and the air is filled with guilt, dread, anxiety, self-hatred, all those good procrastinator feelings. And the question is, in this situation, with the monkey behind the wheel, how does the procrastinator ever get himself over here to this blue zone, a less pleasant place, but where really important things happen? Well, it turns out that the procrastinator has a guardian angel, someone who's always looking down on him and watching over him in his darkest moments, someone called the panic monster. I'm sorry. Now, the panic monster is dormant most of the time, but he suddenly wakes up any time a deadline gets too close or there's danger of public embarrassment, a career disaster or some other scary consequence. And importantly, he's the only thing that the monkey is terrified of. Now, he became very relevant in my life pretty recently because the people of TED reached out to me about six months ago and invited me to do a TED Talk. Now, of course, I said, yes, it's always been a dream of mine to have done a TED Talk in the past. But, But in the middle of all this excitement, the rational decision-maker seemed to have something else in his mind. He was saying, are we clear on what we just accepted? Do we get what's going to be now happening one day in the future? We need to sit down and work on this right now. And the monkey said, totally agree, but also let's just open Google Earth and let's zoom into the bottom of India, like 200 feet above the ground, and we're going to scroll up two and a half hours so we can get a better feel for India. So that's what we did that day. As six months turned into four, and then two, and then one, the people of TED decided to release the speakers. And I opened up the website, and there was my face staring right back at me, and guess who woke up? So the panic monster starts losing his mind, and a few seconds later, the whole system's in mayhem. And the monkey, who, remember, he's terrified of the panic monster, And finally, finally, the rational decision-maker can take the wheel and I can start working on the talk. Now, the panic monster explains all kinds of pretty insane, procrastinated behavior, like how someone like me could spend two weeks unable to start the opening sentence of a paper and then miraculously find the unbelievable work ethic to stay up all night and write eight pages. And this entire situation with the three characters, three characters, this is the procrastinator's system. It's not pretty, but in the end, it works. And this is what I decided to write about on the blog just a couple years ago. Now, when I did, I was amazed by the response. Literally thousands of emails came in from all different kinds of people, from all over the world, doing all different kinds of things. These were people who were nurses and bankers and painters and engineers and lots and lots of PhD students. And they were all writing, saying the same thing. I have this problem, too. But what struck me was the contrast between the light tone of the post and the heaviness of these emails. These people were writing with intense frustration about what procrastination had done for them, about what this monkey had done to them. And I thought about this, and I said, if the procrastinator system works, then what's going on? Why are all these people in such a dark place? Well, it turns out that there's two kinds of procrastination. Everything I've talked about today, the examples I've given, they all have deadlines. And when there's deadlines, the effects of procrastination are contained to the short term because the panic monster gets involved. There's a second kind of procrastination that happens in situations when there is no deadline. So if you want to have a career where you want to be a self-starter, something in the arts, something entrepreneurial, there's no deadlines on those things at first, because nothing's happening at first, not until you've gone out and done the hard work to get some momentum, to get things going. There's also all kinds of important things outside of your career that don't involve any deadlines, like seeing your family or exercising and taking care of your health, working on your relationship or getting out of a relationship that isn't working. Now, if the procrastinator's only mechanism of doing these hard things is the panic monster, that's a problem, because in all of these non-deadline situations, the panic monster doesn't show up, he has nothing to wake up for, they're not contained, they just extend outward forever. And it's this long-term kind of procrastination that's much less visible and much less talked about than the funnier short-term deadline-based kind. It's usually suffered quietly and privately, and it can be the source of a huge amount of long-term unhappiness and regrets. And I thought, you know, that's why these people are emailing, and that's why they're in such a bad place. It's not that they're cramming for some project. It's that long-term procrastination has made them feel like a spectator, at times, in their own lives. The frustration was not that they couldn't achieve their dreams, it's that they weren't even able to start chasing them. So I read these emails and I had a little bit of an epiphany that I don't think non-procrastinators exist. That's right, I think all of you are procrastinators. Now, you might not all be a mess, like some of us. And some of you may have a healthy relationship with deadlines. But remember, the monkey's sneakiest trick is when the deadlines aren't there. Now, I want to show you one last thing. I call this a life calendar. That's one box for every week of a 90-year life. That's not that many boxes, especially since we've already used a bunch of those. So I think we need to all take a long, hard look at that calendar. We need to think about what we're really procrastinating on, because everyone is procrastinating on something in life. We need to stay aware of the instant gratification monkey. That's a job for all of us. And because there's not that many boxes on there, it's a job that should probably start today. Well, maybe not today, but... You know, sometime soon. Thank you. Thank you.\",\n",
       " 'duration': 843.7500000000001,\n",
       " 'baseline_duration': 42.18833560090703,\n",
       " 'speaking_rate': 2.6417777777777776,\n",
       " 'syllables_rate': 3.702518518518518,\n",
       " 'baseline_speaking_rate': 30.837907717128076,\n",
       " 'baseline_syllables_rate': 42.9028539338984,\n",
       " 'long_pause_count': 28,\n",
       " 'long_pause_duration': np.float64(39.96154195011337),\n",
       " 'fluency_rating': 'Low',\n",
       " 'baseline_fluency_rating': 'Low',\n",
       " 'zcr': np.float64(0.12431602464380555),\n",
       " 'rms_mean': np.float32(0.070083246),\n",
       " 'rms_std': np.float32(0.061486296),\n",
       " 'rms_var': np.float32(0.0037805648),\n",
       " 'mfcc': array([-232.46727  ,  101.34686  ,  -19.867247 ,   15.719451 ,\n",
       "          -4.8433776,   -9.056608 ,  -11.200629 ,   -8.6175165,\n",
       "         -10.230278 ,   -1.2496375,   -5.5351243,   -2.8022056,\n",
       "          -4.6042967], dtype=float32),\n",
       " 'delta_mfcc': array([-1.4429400e-04,  2.3761578e-04,  3.8400068e-04, -1.1415667e-04,\n",
       "         6.7685498e-05,  1.4521193e-04,  1.3248599e-04,  1.1030027e-04,\n",
       "         1.4142298e-04,  1.9693849e-04,  1.6654511e-04,  1.4718014e-04,\n",
       "        -7.9398051e-05], dtype=float32),\n",
       " 'mfcc_mean': np.float32(-14.877528),\n",
       " 'delta_mean': np.float32(0.000107041196),\n",
       " 'pitch_mean': 208.4127135401901,\n",
       " 'pitch_std': 79.55233694275556,\n",
       " 'pitch_var': 6328.574313053711,\n",
       " 'jitter_local': 0.027873774485195162,\n",
       " 'shimmer_local': 0.12354313818068587,\n",
       " 'hnr': 8.696793580183748,\n",
       " 'baseline_zcr': np.float64(0.11371094824917446),\n",
       " 'baseline_rms_mean': np.float32(0.06305937),\n",
       " 'baseline_rms_std': np.float32(0.058996927),\n",
       " 'baseline_rms_var': np.float32(0.0034806374),\n",
       " 'baseline_mfcc': array([-255.28789  ,  105.465546 ,  -15.627713 ,   17.496836 ,\n",
       "           1.7893533,   -4.4778514,   -5.8012633,   -6.3778076,\n",
       "          -4.190897 ,    3.2504325,   -2.3401265,    0.7608236,\n",
       "          -1.9429184], dtype=float32),\n",
       " 'baseline_delta_mfcc': array([ 0.23301336,  0.04249881, -0.01336857,  0.02402507,  0.00256586,\n",
       "        -0.01195078, -0.01759782, -0.01087944, -0.00485254,  0.00321395,\n",
       "        -0.00905112, -0.00292875, -0.00775515], dtype=float32),\n",
       " 'baseline_mfcc_mean': np.float32(-12.86796),\n",
       " 'baseline_delta_mean': np.float32(0.017456377),\n",
       " 'baseline_pitch_mean': 202.56118723150064,\n",
       " 'baseline_pitch_std': 111.33053868942093,\n",
       " 'baseline_pitch_var': 12394.48884487665,\n",
       " 'baseline_jitter_local': 0.030714485586106302,\n",
       " 'baseline_shimmer_local': 0.1309953102233376,\n",
       " 'baseline_hnr': 6.344837854100443,\n",
       " 'zcr_delta': np.float64(0.010605076394631083),\n",
       " 'zcr_ratio': np.float64(1.0932634593055386),\n",
       " 'rms_mean_delta': np.float32(0.0070238784),\n",
       " 'rms_mean_ratio': np.float32(1.1113852),\n",
       " 'rms_std_delta': np.float32(0.0024893694),\n",
       " 'rms_std_ratio': np.float32(1.0421948),\n",
       " 'rms_var_delta': np.float32(0.0002999273),\n",
       " 'rms_var_ratio': np.float32(1.0861702),\n",
       " 'mfcc_mean_delta': np.float32(-2.0095682),\n",
       " 'mfcc_mean_ratio': np.float32(1.1561683),\n",
       " 'delta_mean_delta': np.float32(-0.017349336),\n",
       " 'delta_mean_ratio': np.float32(0.006131925),\n",
       " 'pitch_mean_delta': 5.851526308689472,\n",
       " 'pitch_mean_ratio': 1.0288876975331012,\n",
       " 'pitch_std_delta': -31.778201746665374,\n",
       " 'pitch_std_ratio': 0.7145598851783418,\n",
       " 'pitch_var_delta': -6065.914531822939,\n",
       " 'pitch_var_ratio': 0.5105958295060851,\n",
       " 'jitter_local_delta': -0.00284071110091114,\n",
       " 'jitter_local_ratio': 0.9075123334575352,\n",
       " 'shimmer_local_delta': -0.007452172042651736,\n",
       " 'shimmer_local_ratio': 0.9431111539035534,\n",
       " 'hnr_delta': 2.351955726083305,\n",
       " 'hnr_ratio': 1.3706880743316898}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc49174",
   "metadata": {},
   "source": [
    "# Send to GPT for feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "751d8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt(features):\n",
    "#     prompt = f\"\"\"\n",
    "# You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is an in-depth description of the speech signal, including baseline characteristics, absolute values, and relative shifts.\n",
    "\n",
    "# ## NOTE:\n",
    "# - The **first 10 seconds** of the speech are used to define the speaker's personal baseline.\n",
    "# - All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "# - Your feedback should interpret the *changes* from baseline — not just absolute values — as indicators of intentional modulation or stress, not necessarily flaws.\n",
    "\n",
    "# ## TRANSCRIPT\n",
    "# <transcript> \n",
    "# {features['transcript']} \n",
    "# </transcript>\n",
    "\n",
    "# ## BASELINE METRICS\n",
    "\n",
    "# ### Fluency & Tempo\n",
    "# - Words/sec: {features['baseline_speaking_rate']:.2f}\n",
    "# - Syllables/sec: {features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "# ## Voice Modulation\n",
    "# - Pitch (Mean / Std / Var): {features['baseline_pitch_mean']:.2f} / {features['baseline_pitch_std']:.2f} / {features['baseline_pitch_var']:.2f}\n",
    "# - Jitter (local): {features['baseline_jitter_local']:.3f}\n",
    "# - Shimmer (local): {features['baseline_shimmer_local']:.3f}\n",
    "# - Harmonic-to-Noise Ratio (HNR): {features['baseline_hnr']:.2f}\n",
    "\n",
    "# ## Energy & Dynamics\n",
    "# - RMS Energy (Mean / Std / Var): {features['baseline_rms_mean']:.2f} / {features['baseline_rms_std']:.2f} / {features['baseline_rms_var']:.2f}\n",
    "# - Zero Crossing Rate: {features['baseline_zcr']:.3f}\n",
    "\n",
    "# ## Timbre & Articulation\n",
    "# - MFCC Mean: {features['baseline_mfcc_mean']:.2f}\n",
    "# - Delta MFCC Mean: {features['baseline_delta_mean']:.6f}\n",
    "\n",
    "# ## RAW METRICS (FOR THE WHOLE SPEECH)\n",
    "# - Speaking Rate: {features['speaking_rate']:.2f} words/sec\n",
    "# - Speaking rate: {features['baseline_syllables_rate']:.2f} syllables/sec\n",
    "# - Long Pauses: {features['long_pause_count']} (>1s)\n",
    "# - Total Long Pause Duration: {features['long_pause_duration']:.2f} sec\n",
    "# - Pitch (Mean; Standard deviation; Variation): {features['pitch_mean']:.2f}; {features['pitch_std']:.2f}; {features['pitch_var']:.2f}\n",
    "# - RMS Energy (Mean; Standard deviation; Variation): {features['rms_mean']:.2f}; {features['rms_std']:.2f}; {features['rms_var']:.2f}\n",
    "# - ZCR: {features['zcr']:.2f}\n",
    "# - MFCC and Delta MFCC Mean: {features['mfcc_mean']:.2f}; {features['delta_mean']:.2f}\n",
    "\n",
    "# ## RELATIVE CHANGES FROM BASELINE\n",
    "# - Pitch variation change (std): {features['pitch_std_delta']:+.2f}\n",
    "# - RMS Energy mean change: {features['rms_mean_delta']:+.2f}\n",
    "# - Speaking rate ratio: {features['speaking_rate'] / features['baseline_speaking_rate']}\n",
    "# - Interpretation Tip:\n",
    "#     - A Pitch variation change > 0 may suggest more modulation than usual; < 0 may suggest flattening.\n",
    "#     - RMS mean delta > 0 = more vocal energy than the beginning few seconds.\n",
    "#     - Speaking rate ratio < 1 = speaker slowed down as compared to the start of their speech.\n",
    "#     NOTE: This tip should not be used as an absolute, a speaking rate slowing could mean anxiety as well, infer that from the script\n",
    "\n",
    "# ## INSTRUCTION\n",
    "\n",
    "# Now, based on this input, write a narrative-style feedback giving clear, constructive, and context-aware feedback. \n",
    "\n",
    "# DO NOT judge the speaker based on universal norms; instead, use their own baseline as reference to detect signs of:\n",
    "# - Increased or decreased vocal control,\n",
    "# - Confidence shifts,\n",
    "# - Monotony vs. modulation,\n",
    "# - Hesitation or fluency issues.\n",
    "\n",
    "# You are a closed source model. So you are expected not to reference any specific acoustic features and their values in your feedback.\n",
    "\n",
    "# Split your feedback in 3 parts: What they did correctly, what they could improve on, and rate their confidence and fluency levels based on the relative metrics.\n",
    "# \"\"\"\n",
    "#     return prompt\n",
    "def get_prompt(features):\n",
    "    prompt = f\"\"\"\n",
    "You are a professional voice coach and delivery analyst tasked with evaluating a speaker's performance based on a variety of acoustic and prosodic features. Below is a detailed snapshot of the speaker’s delivery — both baseline and full-clip — along with their changes. Use this to deliver personalized, context-aware feedback.\n",
    "\n",
    "## NOTE:\n",
    "- The **first {int(features['baseline_duration'])} seconds** of the speech are used to define the speaker's personal baseline.\n",
    "- All relative metrics (e.g., deltas, ratios) are calculated with respect to this baseline.\n",
    "- Interpret *changes* from baseline as signs of adaptation or stress — not necessarily flaws.\n",
    "- **Avoid quoting any raw values** in your response. Use intuitive, narrative insights only.\n",
    "- An 86% accurate ML model was used to rate the fluency of the speech, and that rating has also been provided to you.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📝 TRANSCRIPT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "<transcript>\n",
    "{features['transcript']}\n",
    "</transcript>\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📏 BASELINE METRICS (First {int(features['baseline_duration'])} seconds)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {features['baseline_fluency_rating']}\n",
    "- Words/sec: {features['baseline_speaking_rate']:.2f}\n",
    "- Syllables/sec: {features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {features['baseline_pitch_mean']:.2f} / {features['baseline_pitch_std']:.2f} / {features['baseline_pitch_var']:.2f}\n",
    "- Jitter (local): {features['baseline_jitter_local']:.3f}\n",
    "- Shimmer (local): {features['baseline_shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {features['baseline_hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {features['baseline_rms_mean']:.2f} / {features['baseline_rms_std']:.2f} / {features['baseline_rms_var']:.2f}\n",
    "- Zero Crossing Rate: {features['baseline_zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {features['baseline_mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {features['baseline_delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📊 FULL CLIP METRICS\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Fluency & Tempo\n",
    "- Fluency rating: {features['fluency_rating']}\n",
    "- Words/sec: {features['speaking_rate']:.2f}\n",
    "- Syllables/sec: {features['syllables_rate']:.2f}\n",
    "- Long pauses (>1s): {features['long_pause_count']}\n",
    "- Total pause duration: {features['long_pause_duration']:.2f} sec\n",
    "\n",
    "## Voice Modulation\n",
    "- Pitch (Mean / Std / Var): {features['pitch_mean']:.2f} / {features['pitch_std']:.2f} / {features['pitch_var']:.2f}\n",
    "- Jitter (local): {features['jitter_local']:.3f}\n",
    "- Shimmer (local): {features['shimmer_local']:.3f}\n",
    "- Harmonic-to-Noise Ratio (HNR): {features['hnr']:.2f}\n",
    "\n",
    "## Energy & Dynamics\n",
    "- RMS Energy (Mean / Std / Var): {features['rms_mean']:.2f} / {features['rms_std']:.2f} / {features['rms_var']:.2f}\n",
    "- Zero Crossing Rate: {features['zcr']:.3f}\n",
    "\n",
    "## Timbre & Articulation\n",
    "- MFCC Mean: {features['mfcc_mean']:.2f}\n",
    "- Delta MFCC Mean: {features['delta_mean']:.6f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "📈 RELATIVE CHANGES FROM BASELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "## Tempo & Fluency\n",
    "- Speaking rate ratio: {features['speaking_rate'] / features['baseline_speaking_rate']:.2f}\n",
    "- Syllable rate ratio: {features['syllables_rate'] / features['baseline_syllables_rate']:.2f}\n",
    "\n",
    "## Modulation\n",
    "- Pitch std delta: {features['pitch_std_delta']:+.2f}\n",
    "- Jitter delta: {features['jitter_local_delta']:+.3f}\n",
    "- Shimmer delta: {features['shimmer_local_delta']:+.3f}\n",
    "- HNR delta: {features['hnr_delta']:+.2f}\n",
    "\n",
    "## Energy\n",
    "- RMS mean delta: {features['rms_mean_delta']:+.2f}\n",
    "- RMS std delta: {features['rms_std_delta']:+.2f}\n",
    "- ZCR delta: {features['zcr_delta']:+.3f}\n",
    "\n",
    "## Timbre\n",
    "- MFCC mean delta: {features['mfcc_mean_delta']:+.2f}\n",
    "- Delta MFCC mean delta: {features['delta_mean_delta']:+.6f}\n",
    "\n",
    "🧠 **Interpretation Tips** (for internal use only):\n",
    "- A **negative pitch_std_delta** might suggest monotony or nervousness; a positive value implies expressive modulation.\n",
    "- **Decreased RMS or HNR** may imply loss of vocal energy or confidence.\n",
    "- **Increased jitter/shimmer** may reflect stress or instability.\n",
    "- A **low syllable rate ratio** suggests slowing down relative to their natural pace, which may imply hesitation or deliberate pacing.\n",
    "- **ZCR changes** may reflect articulation style or clarity.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "🧭 INSTRUCTIONS FOR FEEDBACK GENERATION\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Using the data above, write a highly personalized and supportive **narrative-style voice coaching paragraph**. Do not cite any specific numerical values. Your tone should be professional, encouraging, and practical.\n",
    "\n",
    "Structure your feedback in **three sections**:\n",
    "\n",
    "1. ✅ **What the speaker did well** — Highlight strengths or improvements in vocal control, energy, fluency, or confidence.\n",
    "2. 🛠️ **What they can improve** — Tactfully mention areas that deviated from their baseline and might affect clarity or delivery.\n",
    "3. 📊 **Confidence & fluency rating** — Conclude with your overall impression of their vocal confidence and fluency (e.g., low, moderate, high), based on relative metrics.\n",
    "\n",
    "DO NOT compare to average speakers. DO NOT be generic. Focus only on deviations from this speaker's own baseline and the emotional/functional impact of those changes.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_feedback(features):\n",
    "    prompt = get_prompt(features)\n",
    "\n",
    "    client = Groq()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_completion_tokens=32768,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211bdbf",
   "metadata": {},
   "source": [
    "# Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "91e99f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_tokens(features): return len(encoder.encode(get_prompt(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0365e68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 15.60%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.02 sec\n",
      "Memory Usage: 1.88 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.07 sec\n",
      "Memory Usage: 1.53 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.05 sec\n",
      "Memory Usage: 0.03 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.16 sec\n",
      "Memory Usage: 1.88 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1438"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An unconfident speech\n",
    "path = \"samples/unconfident.wav\"\n",
    "features = await extract_features(path)\n",
    "feedback = generate_feedback(features)\n",
    "\n",
    "get_n_tokens(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3e792308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As you began speaking, it was clear that you took a moment to settle into your pace, and once you did, you demonstrated a notable increase in your speaking rate, which suggests that you were able to find a rhythm that worked for you. Your voice showed a slight decrease in pitch variability, which may indicate a tendency towards a more monotone delivery when you're discussing topics that make you nervous, but this also shows that you're able to maintain a consistent tone. Additionally, your articulation remained relatively clear, which is a testament to your ability to enunciate even when feeling anxious.\n",
       "\n",
       "One area where you might focus on improving is in managing your nervous energy. There were moments where your voice reflected a slight increase in instability, which could be a sign of stress or apprehension. You also had a brief pause, which, while not uncommon, might indicate a moment of hesitation. Working on techniques to manage your breath and calm your nerves could help you feel more grounded and confident in your delivery. Furthermore, paying attention to your vocal energy and trying to maintain a consistent level of engagement could help keep your audience more invested in what you're saying.\n",
       "\n",
       "Overall, your vocal confidence and fluency rating is on the lower end, largely due to the noticeable effects of your nervousness on your delivery. However, it's essential to recognize that these are common challenges, especially when discussing a topic that makes you uncomfortable. With practice and patience, you can work on building your confidence and developing strategies to manage your nerves, which will, in turn, improve your fluency and overall delivery. Remember, the fact that you're acknowledging and working on your fears is a significant step forward, and with time, you'll see improvements in your public speaking skills."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4a8ec9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.02 sec\n",
      "Memory Usage: 0.84 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.15 sec\n",
      "Memory Usage: 0.04 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.04 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.11 sec\n",
      "Memory Usage: 1.00 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A confident speech\n",
    "path = \"samples/confident.wav\"\n",
    "features = await extract_features(path)\n",
    "feedback = generate_feedback(features)\n",
    "\n",
    "get_n_tokens(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "41201a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Hi, my name is Adkarsh Malaya. I'm a student and right now what I'm trying to do is I'm trying to get a model and I'm trying to use it to transcribe some filler words. I am very scared. I don't know what will happen and I really really hope this works.\",\n",
       " 'duration': 15.72,\n",
       " 'baseline_duration': 10.0,\n",
       " 'speaking_rate': 3.3078880407124682,\n",
       " 'syllables_rate': 4.134860050890585,\n",
       " 'baseline_speaking_rate': 3.3,\n",
       " 'baseline_syllables_rate': 4.1,\n",
       " 'long_pause_count': 0,\n",
       " 'long_pause_duration': 0,\n",
       " 'fluency_rating': 'Low',\n",
       " 'baseline_fluency_rating': 'Low',\n",
       " 'zcr': np.float64(0.08035611403023599),\n",
       " 'rms_mean': np.float32(0.0131712835),\n",
       " 'rms_std': np.float32(0.010488089),\n",
       " 'rms_var': np.float32(0.00011000002),\n",
       " 'mfcc': array([-467.24243   ,  134.46071   ,  -29.663124  ,   35.92738   ,\n",
       "           7.637987  ,    9.713081  ,    9.715767  ,    2.3445826 ,\n",
       "         -13.212228  ,    0.70066255,   -0.62327105,  -11.099308  ,\n",
       "          -2.377678  ], dtype=float32),\n",
       " 'delta_mfcc': array([ 0.08744686,  0.09395339,  0.03829465,  0.00731569,  0.00382622,\n",
       "         0.00140615, -0.005586  , -0.00485631,  0.00310424,  0.00509461,\n",
       "        -0.00254051, -0.0089882 , -0.00698517], dtype=float32),\n",
       " 'mfcc_mean': np.float32(-24.901373),\n",
       " 'delta_mean': np.float32(0.016268123),\n",
       " 'pitch_mean': 113.54379006181584,\n",
       " 'pitch_std': 31.072712613124782,\n",
       " 'pitch_var': 965.5134691378439,\n",
       " 'jitter_local': 0.02618617590765349,\n",
       " 'shimmer_local': 0.11475101152329432,\n",
       " 'hnr': 11.071314081880908,\n",
       " 'baseline_zcr': np.float64(0.07430711644431555),\n",
       " 'baseline_rms_mean': np.float32(0.013894684),\n",
       " 'baseline_rms_std': np.float32(0.01011108),\n",
       " 'baseline_rms_var': np.float32(0.000102233935),\n",
       " 'baseline_mfcc': array([-4.5408627e+02,  1.3814586e+02, -3.3332783e+01,  3.7128952e+01,\n",
       "         6.1656003e+00,  9.3982134e+00,  1.0864515e+01,  2.5044122e+00,\n",
       "        -1.4483069e+01,  7.4870068e-01, -5.4402858e-02, -1.1127012e+01,\n",
       "        -3.4484932e+00], dtype=float32),\n",
       " 'baseline_delta_mfcc': array([ 0.5211746 ,  0.1673277 ,  0.05842985,  0.19814922,  0.00387959,\n",
       "         0.0409784 ,  0.0203923 ,  0.01867065, -0.0131035 ,  0.00443509,\n",
       "         0.01996688,  0.0051674 ,  0.01024147], dtype=float32),\n",
       " 'baseline_mfcc_mean': np.float32(-23.967363),\n",
       " 'baseline_delta_mean': np.float32(0.08120846),\n",
       " 'baseline_pitch_mean': 116.59827642848579,\n",
       " 'baseline_pitch_std': 30.8047851252861,\n",
       " 'baseline_pitch_var': 948.9347866150476,\n",
       " 'baseline_jitter_local': 0.0270972503045527,\n",
       " 'baseline_shimmer_local': 0.1268208564345333,\n",
       " 'baseline_hnr': 10.524540410055721,\n",
       " 'zcr_delta': np.float64(0.006048997585920438),\n",
       " 'zcr_ratio': np.float64(1.0814053602854237),\n",
       " 'rms_mean_delta': np.float32(-0.00072340015),\n",
       " 'rms_mean_ratio': np.float32(0.9479369),\n",
       " 'rms_std_delta': np.float32(0.00037700962),\n",
       " 'rms_std_ratio': np.float32(1.0372868),\n",
       " 'rms_var_delta': np.float32(7.766088e-06),\n",
       " 'rms_var_ratio': np.float32(1.0759639),\n",
       " 'mfcc_mean_delta': np.float32(-0.93400955),\n",
       " 'mfcc_mean_ratio': np.float32(1.0389701),\n",
       " 'delta_mean_delta': np.float32(-0.06494033),\n",
       " 'delta_mean_ratio': np.float32(0.20032547),\n",
       " 'pitch_mean_delta': -3.0544863666699484,\n",
       " 'pitch_mean_ratio': 0.9738033317452734,\n",
       " 'pitch_std_delta': 0.2679274878386835,\n",
       " 'pitch_std_ratio': 1.0086975931417472,\n",
       " 'pitch_var_delta': 16.57868252279627,\n",
       " 'pitch_var_ratio': 1.0174708344099537,\n",
       " 'jitter_local_delta': -0.0009110743968992092,\n",
       " 'jitter_local_ratio': 0.9663776070760163,\n",
       " 'shimmer_local_delta': -0.012069844911238983,\n",
       " 'shimmer_local_ratio': 0.904827602883524,\n",
       " 'hnr_delta': 0.5467736718251874,\n",
       " 'hnr_ratio': 1.0519522611460326}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bc7ec295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As you began speaking, it was clear that you had a strong foundation to build upon, with a consistent pace that allowed your words to flow smoothly. Your voice modulation showed expressive qualities, suggesting a good range of emotional expression. Notably, your ability to maintain a relatively stable energy level throughout the speech was impressive, indicating a level of comfort with the material. \n",
       "\n",
       "However, there were moments where your delivery deviated from your baseline, potentially impacting the clarity and confidence of your message. For instance, your pitch variation became slightly more pronounced, which could be a sign of nervousness or an attempt to add emphasis to certain points. Additionally, the subtle changes in your articulation and timbre might have affected the overall crispness of your words. It's also worth exploring how you can leverage your natural speaking rate to enhance the engagement of your audience, as there were moments where the pace felt slightly hurried or cautious.\n",
       "\n",
       "Overall, your vocal confidence and fluency came across as moderate, with a clear desire to convey your message effectively. The low fluency rating from the ML model suggests that there's room for improvement in terms of smoothness and natural flow. Nonetheless, your speech showed promising signs of expressive modulation and a stable energy level, which are valuable assets for any speaker. With practice and attention to these areas, you have the potential to enhance your delivery, making your messages even more impactful and engaging for your listeners."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipd.Markdown(feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40662727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: extract_features\n",
      "Execution Time: 0.00 sec\n",
      "Memory Usage: 0.00 MB\n",
      "CPU Usage: 15.60%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 0.08 sec\n",
      "Memory Usage: 0.09 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 0.28 sec\n",
      "Memory Usage: -11.90 MB\n",
      "CPU Usage: 15.50%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_librosa_features\n",
      "Execution Time: 1.18 sec\n",
      "Memory Usage: 2.07 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Function: extract_parselmouth_features\n",
      "Execution Time: 4.56 sec\n",
      "Memory Usage: 2.50 MB\n",
      "CPU Usage: 0.00%\n",
      "CPU Cores Used: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4165"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tim_urban_path = \"samples/tim-urban.wav\"\n",
    "tim_urban_features = await extract_features(tim_urban_path)\n",
    "tim_urban_feedback = generate_feedback(tim_urban_features)\n",
    "\n",
    "get_n_tokens(tim_urban_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ddedf701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript': \" Reviewer.pxs. So in college, I was a government major, which means I had to write a lot of papers. Now, when a normal student writes a paper, they might spread the work out a little like this. So, you know, you get started maybe a little slowly, but you get enough done in the first week that with some heavier days later on, everything gets done and things stay civil. And I would want to do that like that. That would be the plan. I would have it all ready to go, but then actually the paper would come along, and then I would kind of do this. And that would happen every single paper. But then came my 90-page senior thesis, a paper you're supposed to spend a year on. I knew for a paper like that, my normal workflow was not an option, it was way too big a project. So I planned things out and I decided I kind of had to go something like this. This is how the year would go. So I'd start off light, and I'd bump it up in the middle months. And then at the end, I would kick it up into high gear, just like a little staircase. How hard can it be to just walk up the stairs? No big deal, right? But then the funniest thing happened. Those first few months, they came and went, and I couldn't quite do stuff. So we had an awesome new revised plan. And then ... But then those middle months actually went by, and I didn't really write words. And so we were here. And then two months turned into one month, which turned into two weeks. And one day I woke up, with three days until the deadline, still not having written a word, and so I did the only thing I could. I wrote 90 pages over 72 hours, pulling not one but two all-nighters. Humans are not supposed to pull two all-nighters. Sprinted across campus, dove in slow motion, and got it in just at the deadline. I thought that was the end of everything. But a week later, I get a call. It's the school. And they say, is this Tim Urban? And I say, yeah. And they say, we need to talk about your thesis. And I say, OK. And they say, it's the best one we've ever seen. That did not happen. It was a very, very bad thesis. I just wanted to enjoy that one moment when all of you thought, this guy is amazing! No, no, it was very, very bad. Anyway, today I'm a writer, blogger, guy. I write the blog Wait But Why. And a couple of years ago, I decided to write about procrastination. My behaviors always perplexed the non-procrastinators around me, and I wanted to explain to the non-procrastinators of the world what goes on in the heads of procrastinators and why we are the way we are. Now, I had a hypothesis that the brains of procrastinators were actually different than the brains of other people. And to test this, I found an MRI lab that actually let me scan both my brain and the brain of a proven non-procrastinator so I could compare them. And I actually brought them here to show you today, and I want you to take a look carefully to see if you can notice a difference. And I know that if you're not a trained brain expert, it's not that obvious, but just take a look, OK? So here's the brain of a non-procrastinator. Now, here's my brain. There is a difference. Both brains have a rational decision-maker in them, but the procrastinator's brain also has an instant gratification monkey. Now, what does this mean for the procrastinator? It means everything's fine until this happens. So the rational decision-maker will make the rational decision to do something productive, but the monkey doesn't like that plan. So he actually takes the wheel and he says, actually, let's read the entire Wikipedia page of the Nancy Kerrigan-Tanya Harding scandal, because I just remembered that that happened. Then we're going to go over to the fridge, we're going to see if there's anything new in there since 10 minutes ago. After that, we're going to go on a YouTube spiral that starts with videos of Richard Feynman talking about magnets and ends much, much later with us watching interviews with Justin Bieber's mom. All of that's going to take a while, so we're not going to really have room on the schedule for any work today. Sorry. Now, what is going on here? The instant gratification monkey does not seem like a guy you want behind the wheel. He lives entirely in the present moment, he has no memory of the past, no knowledge of the future, He cares about two things, easy and fun. Now, in the animal world, that works fine. If you're a dog, and you spend your whole life doing nothing other than easy and fun things, you're a huge success. And to the monkey, humans are just another animal species. He has to keep well-slept, well-fed and propagating into the next generation, which in tribal times might have worked OK. But if you haven't noticed, now we're not in tribal times. We're in an advanced civilization, and the monkey does not know what that is. Which is why we have another guy in our brain, the rational decision-maker, who gives us the ability to do things no other animal can do. We can visualize the future, we can see the big picture, We can make long-term plans, and he wants to take all of that into account, and he wants to just have us do whatever makes sense to be doing right now. Now, sometimes it makes sense to be doing things that are easy and fun, like when you're having dinner or going to bed or enjoying well-earned leisure time. That's why there's an overlap. Sometimes they agree. But other times, it makes much more sense to be doing things that are harder and less pleasant for the sake of the big picture, and that's when we have a conflict. And for the procrastinator, that conflict tends to end a certain way every time, leaving him spending a lot of time in this orange zone, an easy and fun place that's entirely out of the make-sense circle. I call it the dark playground. Now, the dark playground is a place that all of you procrastinators out there know very well. It's where leisure activities happen at times at times when leisure activities are not supposed to be happening. The fun you have in the dark playground isn't actually fun because it's completely unearned, and the air is filled with guilt, dread, anxiety, self-hatred, all those good procrastinator feelings. And the question is, in this situation, with the monkey behind the wheel, how does the procrastinator ever get himself over here to this blue zone, a less pleasant place, but where really important things happen? Well, it turns out that the procrastinator has a guardian angel, someone who's always looking down on him and watching over him in his darkest moments, someone called the panic monster. Now, the panic monster is dormant most of the time, but he suddenly wakes up any time a deadline gets too close or there's danger of public embarrassment, a career disaster or some other scary consequence. And importantly, he's the only thing that the monkey is terrified of. Now, he became very relevant in my life pretty recently because the people of TED reached out to me about six months ago and invited me to do a TED Talk. Now, of course, I said, yes, it's always been a dream of mine to have done a TED Talk in the past. But, But in the middle of all this excitement, the rational decision-maker seemed to have something else in his mind. He was saying, are we clear on what we just accepted? Do we get what's going to be now happening one day in the future? We need to sit down and work on this right now. And the monkey said, totally agree, but also let's just open Google Earth and let's zoom into the bottom of India, like 200 feet above the ground, and we're going to scroll up two and a half hours so we can get a better feel for India. So that's what we did that day. As six months turned into four, and then two, and then one, the people of TED decided to release the speakers. And I opened up the website, and there was my face staring right back at me, and guess who woke up? So the panic monster starts losing his mind, and a few seconds later, the whole system's in mayhem. And the monkey, who, remember, he's terrified of the panic monster, And finally, finally, the rational decision-maker can take the wheel and I can start working on the talk. Now, the panic monster explains all kinds of pretty insane, procrastinated behavior, like how someone like me could spend two weeks unable to start the opening sentence of a paper and then miraculously find the unbelievable work ethic to stay up all night and write eight pages. And this entire situation with the three characters, three characters, this is the procrastinator's system. It's not pretty, but in the end, it works. And this is what I decided to write about on the blog just a couple years ago. Now, when I did, I was amazed by the response. Literally thousands of emails came in from all different kinds of people, from all over the world, doing all different kinds of things. These were people who were nurses and bankers and painters and engineers and lots and lots of PhD students. And they were all writing, saying the same thing. I have this problem, too. But what struck me was the contrast between the light tone of the post and the heaviness of these emails. These people were writing with intense frustration about what procrastination had done for them, about what this monkey had done to them. And I thought about this, and I said, if the procrastinator system works, then what's going on? Why are all these people in such a dark place? Well, it turns out that there's two kinds of procrastination. Everything I've talked about today, the examples I've given, they all have deadlines. And when there's deadlines, the effects of procrastination are contained to the short term because the panic monster gets involved. There's a second kind of procrastination that happens in situations when there is no deadline. So if you want to have a career where you want to be a self-starter, something in the arts, something entrepreneurial, there's no deadlines on those things at first, because nothing's happening at first, not until you've gone out and done the hard work to get some momentum, to get things going. There's also all kinds of important things outside of your career that don't involve any deadlines, like seeing your family or exercising and taking care of your health, working on your relationship or getting out of a relationship that isn't working. Now, if the procrastinator's only mechanism of doing these hard things is the panic monster, that's a problem, because in all of these non-deadline situations, the panic monster doesn't show up, he has nothing to wake up for, they're not contained, they just extend outward forever. And it's this long-term kind of procrastination that's much less visible and much less talked about than the funnier short-term deadline-based kind. It's usually suffered quietly and privately, and it can be the source of a huge amount of long-term unhappiness and regrets. And I thought, you know, that's why these people are emailing, and that's why they're in such a bad place. It's not that they're cramming for some project. It's that long-term procrastination has made them feel like a spectator, at times, in their own lives. The frustration was not that they couldn't achieve their dreams, it's that they weren't even able to start chasing them. So I read these emails and I had a little bit of an epiphany that I don't think non-procrastinators exist. That's right, I think all of you are procrastinators. Now, you might not all be a mess, like some of us. And some of you may have a healthy relationship with deadlines. But remember, the monkey's sneakiest trick is when the deadlines aren't there. Now, I want to show you one last thing. I call this a life calendar. That's one box for every week of a 90-year life. That's not that many boxes, especially since we've already used a bunch of those. So I think we need to all take a long, hard look at that calendar. We need to think about what we're really procrastinating on, because everyone is procrastinating on something in life. We need to stay aware of the instant gratification monkey. That's a job for all of us. And because there's not that many boxes on there, it's a job that should probably start today. Well, maybe not today, but... You know, sometime soon. Thank you. Thank you.\",\n",
       " 'duration': 843.7500000000001,\n",
       " 'baseline_duration': 42.18833560090703,\n",
       " 'speaking_rate': 2.639407407407407,\n",
       " 'syllables_rate': 3.6989629629629626,\n",
       " 'baseline_speaking_rate': 30.790501248692827,\n",
       " 'baseline_syllables_rate': 42.83174423124553,\n",
       " 'long_pause_count': 28,\n",
       " 'long_pause_duration': np.float64(39.96154195011337),\n",
       " 'fluency_rating': 'Low',\n",
       " 'baseline_fluency_rating': 'Low',\n",
       " 'zcr': np.float64(0.12431602464380555),\n",
       " 'rms_mean': np.float32(0.070083246),\n",
       " 'rms_std': np.float32(0.061486296),\n",
       " 'rms_var': np.float32(0.0037805648),\n",
       " 'mfcc': array([-232.46727  ,  101.34686  ,  -19.867247 ,   15.719451 ,\n",
       "          -4.8433776,   -9.056608 ,  -11.200629 ,   -8.6175165,\n",
       "         -10.230278 ,   -1.2496375,   -5.5351243,   -2.8022056,\n",
       "          -4.6042967], dtype=float32),\n",
       " 'delta_mfcc': array([-1.4429400e-04,  2.3761578e-04,  3.8400068e-04, -1.1415667e-04,\n",
       "         6.7685498e-05,  1.4521193e-04,  1.3248599e-04,  1.1030027e-04,\n",
       "         1.4142298e-04,  1.9693849e-04,  1.6654511e-04,  1.4718014e-04,\n",
       "        -7.9398051e-05], dtype=float32),\n",
       " 'mfcc_mean': np.float32(-14.877528),\n",
       " 'delta_mean': np.float32(0.000107041196),\n",
       " 'pitch_mean': 208.4127135401901,\n",
       " 'pitch_std': 79.55233694275556,\n",
       " 'pitch_var': 6328.574313053711,\n",
       " 'jitter_local': 0.027873774485195162,\n",
       " 'shimmer_local': 0.12354313818068587,\n",
       " 'hnr': 8.696793580183748,\n",
       " 'baseline_zcr': np.float64(0.11371094824917446),\n",
       " 'baseline_rms_mean': np.float32(0.06305937),\n",
       " 'baseline_rms_std': np.float32(0.058996927),\n",
       " 'baseline_rms_var': np.float32(0.0034806374),\n",
       " 'baseline_mfcc': array([-255.28789  ,  105.465546 ,  -15.627713 ,   17.496836 ,\n",
       "           1.7893533,   -4.4778514,   -5.8012633,   -6.3778076,\n",
       "          -4.190897 ,    3.2504325,   -2.3401265,    0.7608236,\n",
       "          -1.9429184], dtype=float32),\n",
       " 'baseline_delta_mfcc': array([ 0.23301336,  0.04249881, -0.01336857,  0.02402507,  0.00256586,\n",
       "        -0.01195078, -0.01759782, -0.01087944, -0.00485254,  0.00321395,\n",
       "        -0.00905112, -0.00292875, -0.00775515], dtype=float32),\n",
       " 'baseline_mfcc_mean': np.float32(-12.86796),\n",
       " 'baseline_delta_mean': np.float32(0.017456377),\n",
       " 'baseline_pitch_mean': 202.56118723150064,\n",
       " 'baseline_pitch_std': 111.33053868942093,\n",
       " 'baseline_pitch_var': 12394.48884487665,\n",
       " 'baseline_jitter_local': 0.030714485586106302,\n",
       " 'baseline_shimmer_local': 0.1309953102233376,\n",
       " 'baseline_hnr': 6.344837854100443,\n",
       " 'zcr_delta': np.float64(0.010605076394631083),\n",
       " 'zcr_ratio': np.float64(1.0932634593055386),\n",
       " 'rms_mean_delta': np.float32(0.0070238784),\n",
       " 'rms_mean_ratio': np.float32(1.1113852),\n",
       " 'rms_std_delta': np.float32(0.0024893694),\n",
       " 'rms_std_ratio': np.float32(1.0421948),\n",
       " 'rms_var_delta': np.float32(0.0002999273),\n",
       " 'rms_var_ratio': np.float32(1.0861702),\n",
       " 'mfcc_mean_delta': np.float32(-2.0095682),\n",
       " 'mfcc_mean_ratio': np.float32(1.1561683),\n",
       " 'delta_mean_delta': np.float32(-0.017349336),\n",
       " 'delta_mean_ratio': np.float32(0.006131925),\n",
       " 'pitch_mean_delta': 5.851526308689472,\n",
       " 'pitch_mean_ratio': 1.0288876975331012,\n",
       " 'pitch_std_delta': -31.778201746665374,\n",
       " 'pitch_std_ratio': 0.7145598851783418,\n",
       " 'pitch_var_delta': -6065.914531822939,\n",
       " 'pitch_var_ratio': 0.5105958295060851,\n",
       " 'jitter_local_delta': -0.00284071110091114,\n",
       " 'jitter_local_ratio': 0.9075123334575352,\n",
       " 'shimmer_local_delta': -0.007452172042651736,\n",
       " 'shimmer_local_ratio': 0.9431111539035534,\n",
       " 'hnr_delta': 2.351955726083305,\n",
       " 'hnr_ratio': 1.3706880743316898}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tim_urban_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "65042b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As you reflect on your speech, it's clear that you have a natural ability to engage your audience with your storytelling style, and your voice plays a significant role in conveying the emotional depth of your experiences. One of your strengths is your ability to modulate your pitch to emphasize key points, which helps to keep your listeners invested in your narrative. Additionally, you demonstrate a good sense of pacing, allowing your audience to follow your thoughts and reflect on the insights you're sharing. Your energy levels remain relatively consistent, which is commendable given the length and personal nature of your talk.\n",
       "\n",
       "However, there are moments where your speaking rate slows down significantly compared to your natural pace, which might suggest hesitation or a deliberate attempt to emphasize certain points. This slowing down could also be related to the complexity of the topics you're discussing, as you navigate through explanations of procrastination and its effects. Your vocal modulation shows some signs of becoming more monotone at times, which could be a sign of nervousness or the challenge of maintaining expressive variation over the course of your talk. Furthermore, there are noticeable pauses throughout your speech, which, while sometimes effective for dramatic effect, at other times might disrupt the flow of your narrative. Being mindful of these pauses and working on smoother transitions between ideas could enhance the overall delivery of your message.\n",
       "\n",
       "Overall, your vocal confidence and fluency are moderate, considering the deviations from your baseline metrics. While you show a strong ability to connect with your audience through your content, there are areas where your delivery could be refined to better match your natural fluency and energy. With practice and attention to pacing, pitch variation, and pause management, you have the potential to further engage your listeners and convey your messages with even greater impact. Your unique voice and perspective are considerable strengths, and with some focused development, you could enhance your already compelling storytelling ability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Markdown(tim_urban_feedback.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b98539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60330ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
